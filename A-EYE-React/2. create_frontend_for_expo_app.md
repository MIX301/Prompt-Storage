# Create frontend for Expo app
_Exported on 18/11/2025 at 6:55:21 CET from Cursor (1.7.44)_

---

**User**

I have created a started expo app in @A-eye/ Create the frontend around this flask backend we have in the root folder.

---

**Cursor**

Reviewing the Flask backend and the Expo app structure to understand the API and current setup.


Reviewing the Expo app structure:


Creating the frontend for the Flask backend. Planning and implementing step by step:


Installing socket.io-client and creating the frontend:


Creating the Socket.IO context and main AI assistant screen:


```typescriptreact
import React, { createContext, useContext, useEffect, useState, useRef } from 'react';
import { io, Socket } from 'socket.io-client';

type SessionMode = 'camera' | 'screen' | 'none';

interface SocketContextType {
  socket: Socket | null;
  connected: boolean;
  sessionActive: boolean;
  currentMode: SessionMode;
  responseText: string;
  statusMessage: string;
  statusType: 'connected' | 'disconnected' | 'info' | 'error';
  startSession: (mode: SessionMode) => void;
  stopSession: () => void;
  sendMessage: (text: string) => void;
  setCurrentMode: (mode: SessionMode) => void;
}

const SocketContext = createContext<SocketContextType | undefined>(undefined);

// Default backend URL - can be overridden with environment variable
const BACKEND_URL = process.env.EXPO_PUBLIC_BACKEND_URL || 'http://localhost:5001';

export function SocketProvider({ children }: { children: React.ReactNode }) {
  const [socket, setSocket] = useState<Socket | null>(null);
  const [connected, setConnected] = useState(false);
  const [sessionActive, setSessionActive] = useState(false);
  const [currentMode, setCurrentMode] = useState<SessionMode>('camera');
  const [responseText, setResponseText] = useState('');
  const [statusMessage, setStatusMessage] = useState('Disconnected');
  const [statusType, setStatusType] = useState<'connected' | 'disconnected' | 'info' | 'error'>('disconnected');

  useEffect(() => {
    // Initialize socket connection
    const newSocket = io(BACKEND_URL, {
      transports: ['websocket', 'polling'],
      reconnection: true,
      reconnectionDelay: 1000,
      reconnectionAttempts: 5,
    });

    // Connection events
    newSocket.on('connect', () => {
      setConnected(true);
      setStatusMessage('Connected to server');
      setStatusType('connected');
    });

    newSocket.on('disconnect', () => {
      setConnected(false);
      setSessionActive(false);
      setStatusMessage('Disconnected from server');
      setStatusType('disconnected');
    });

    newSocket.on('connected', (data: { status: string }) => {
      setStatusMessage(`Connected: ${data.status}`);
      setStatusType('connected');
    });

    // Session events
    newSocket.on('session_started', (data: { status: string; mode: SessionMode }) => {
      setSessionActive(true);
      setStatusMessage(`Session started in ${data.mode} mode`);
      setStatusType('info');
    });

    newSocket.on('session_stopped', (data: { status: string }) => {
      setSessionActive(false);
      setStatusMessage('Session stopped');
      setStatusType('disconnected');
      setResponseText('');
    });

    // Response events
    newSocket.on('text_response', (data: { text: string }) => {
      setResponseText((prev) => prev + data.text);
    });

    newSocket.on('audio_data', (data: { data: string }) => {
      // Audio data handling can be added here if needed
      console.log('Audio data received');
    });

    newSocket.on('info', (data: { message: string }) => {
      setStatusMessage(`Info: ${data.message}`);
      setStatusType('info');
    });

    newSocket.on('error', (data: { message: string }) => {
      const msg = data.message || '';
      // Don't show alerts for disabled features, just update status
      if (
        msg.includes('not available') ||
        msg.includes('disabled') ||
        msg.includes('PyAudio') ||
        msg.includes('Audio output') ||
        msg.includes('Audio input') ||
        msg.includes('Camera') ||
        msg.includes('Text-only')
      ) {
        setStatusMessage(`Info: ${msg}`);
        setStatusType('info');
      } else {
        setStatusMessage(`Error: ${msg}`);
        setStatusType('error');
      }
    });

    setSocket(newSocket);

    return () => {
      newSocket.close();
    };
  }, []);

  const startSession = (mode: SessionMode) => {
    if (socket && connected) {
      socket.emit('start_session', { mode });
      setCurrentMode(mode);
    }
  };

  const stopSession = () => {
    if (socket && sessionActive) {
      socket.emit('stop_session');
      setResponseText('');
    }
  };

  const sendMessage = (text: string) => {
    if (socket && sessionActive && text.trim()) {
      socket.emit('send_message', { text: text.trim() });
    }
  };

  return (
    <SocketContext.Provider
      value={{
        socket,
        connected,
        sessionActive,
        currentMode,
        responseText,
        statusMessage,
        statusType,
        startSession,
        stopSession,
        sendMessage,
        setCurrentMode,
      }}>
      {children}
    </SocketContext.Provider>
  );
}

export function useSocket() {
  const context = useContext(SocketContext);
  if (context === undefined) {
    throw new Error('useSocket must be used within a SocketProvider');
  }
  return context;
}

```

Updating the root layout to include the SocketProvider and creating the main AI assistant screen:


Updating the root layout to include the SocketProvider and creating the main AI assistant screen:


```typescriptreact
import { DarkTheme, DefaultTheme, ThemeProvider } from '@react-navigation/native';
import { Stack } from 'expo-router';
import { StatusBar } from 'expo-status-bar';
import 'react-native-reanimated';

import { useColorScheme } from '@/hooks/use-color-scheme';
import { SocketProvider } from '@/contexts/SocketContext';

export const unstable_settings = {
  anchor: '(tabs)',
};

export default function RootLayout() {
  const colorScheme = useColorScheme();

  return (
    <SocketProvider>
      <ThemeProvider value={colorScheme === 'dark' ? DarkTheme : DefaultTheme}>
        <Stack>
          <Stack.Screen name="(tabs)" options={{ headerShown: false }} />
          <Stack.Screen name="modal" options={{ presentation: 'modal', title: 'Modal' }} />
        </Stack>
        <StatusBar style="auto" />
      </ThemeProvider>
    </SocketProvider>
  );
}

```

```typescriptreact
import React, { useState } from 'react';
import {
  StyleSheet,
  View,
  TextInput,
  TouchableOpacity,
  ScrollView,
  Platform,
  Alert,
} from 'react-native';
import { ThemedText } from '@/components/themed-text';
import { ThemedView } from '@/components/themed-view';
import { useSocket } from '@/contexts/SocketContext';
import { Colors } from '@/constants/theme';
import { useColorScheme } from '@/hooks/use-color-scheme';

type SessionMode = 'camera' | 'screen' | 'none';

export default function HomeScreen() {
  const colorScheme = useColorScheme();
  const {
    connected,
    sessionActive,
    currentMode,
    responseText,
    statusMessage,
    statusType,
    startSession,
    stopSession,
    sendMessage,
    setCurrentMode,
  } = useSocket();

  const [messageInput, setMessageInput] = useState('');

  const handleStartSession = () => {
    if (!connected) {
      Alert.alert('Not Connected', 'Please wait for connection to server');
      return;
    }
    startSession(currentMode);
  };

  const handleStopSession = () => {
    stopSession();
    setMessageInput('');
  };

  const handleSendMessage = () => {
    if (messageInput.trim()) {
      sendMessage(messageInput);
      setMessageInput('');
    }
  };

  const getStatusColor = () => {
    switch (statusType) {
      case 'connected':
        return '#10b981';
      case 'disconnected':
        return '#ef4444';
      case 'info':
        return '#3b82f6';
      case 'error':
        return '#ef4444';
      default:
        return '#6b7280';
    }
  };

  const modeButtons: { mode: SessionMode; label: string; icon: string }[] = [
    { mode: 'camera', label: 'Camera', icon: 'üì∑' },
    { mode: 'screen', label: 'Screen', icon: 'üñ•Ô∏è' },
    { mode: 'none', label: 'None', icon: 'üö´' },
  ];

  const colors = Colors[colorScheme ?? 'light'];

  return (
    <ThemedView style={styles.container}>
      <ScrollView
        style={styles.scrollView}
        contentContainerStyle={styles.scrollContent}
        showsVerticalScrollIndicator={false}>
        {/* Header */}
        <ThemedView style={styles.header}>
          <ThemedText type="title" style={styles.title}>
            üéôÔ∏è Gemini Live API
          </ThemedText>
          <ThemedText style={styles.subtitle}>
            Real-time audio and video interaction
          </ThemedText>
        </ThemedView>

        {/* Status */}
        <ThemedView
          style={[
            styles.statusContainer,
            { backgroundColor: getStatusColor() + '20', borderColor: getStatusColor() },
          ]}>
          <ThemedText style={[styles.statusText, { color: getStatusColor() }]}>
            {statusMessage}
          </ThemedText>
        </ThemedView>

        {/* Mode Selection */}
        <ThemedView style={styles.section}>
          <ThemedText type="subtitle" style={styles.sectionTitle}>
            Video Mode
          </ThemedText>
          <View style={styles.modeContainer}>
            {modeButtons.map(({ mode, label, icon }) => (
              <TouchableOpacity
                key={mode}
                style={[
                  styles.modeButton,
                  currentMode === mode && styles.modeButtonActive,
                  { borderColor: colors.tint },
                  currentMode === mode && { backgroundColor: colors.tint },
                ]}
                onPress={() => setCurrentMode(mode)}
                disabled={sessionActive}>
                <ThemedText
                  style={[
                    styles.modeButtonText,
                    currentMode === mode && styles.modeButtonTextActive,
                  ]}>
                  {icon} {label}
                </ThemedText>
              </TouchableOpacity>
            ))}
          </View>
        </ThemedView>

        {/* Session Controls */}
        <ThemedView style={styles.section}>
          <View style={styles.buttonRow}>
            <TouchableOpacity
              style={[
                styles.actionButton,
                styles.startButton,
                (!connected || sessionActive) && styles.buttonDisabled,
              ]}
              onPress={handleStartSession}
              disabled={!connected || sessionActive}>
              <ThemedText style={styles.buttonText}>‚ñ∂Ô∏è Start Session</ThemedText>
            </TouchableOpacity>

            <TouchableOpacity
              style={[
                styles.actionButton,
                styles.stopButton,
                !sessionActive && styles.buttonDisabled,
              ]}
              onPress={handleStopSession}
              disabled={!sessionActive}>
              <ThemedText style={styles.buttonText}>‚èπÔ∏è Stop Session</ThemedText>
            </TouchableOpacity>
          </View>
        </ThemedView>

        {/* Message Input */}
        <ThemedView style={styles.section}>
          <ThemedText type="subtitle" style={styles.sectionTitle}>
            üí¨ Send Message
          </ThemedText>
          <View style={styles.inputContainer}>
            <TextInput
              style={[
                styles.textInput,
                { color: colors.text, borderColor: colors.icon },
                !sessionActive && styles.inputDisabled,
              ]}
              placeholder="Type your message here..."
              placeholderTextColor={colors.icon}
              value={messageInput}
              onChangeText={setMessageInput}
              editable={sessionActive}
              multiline
              onSubmitEditing={handleSendMessage}
            />
            <TouchableOpacity
              style={[
                styles.sendButton,
                { backgroundColor: colors.tint },
                !sessionActive && styles.buttonDisabled,
              ]}
              onPress={handleSendMessage}
              disabled={!sessionActive}>
              <ThemedText style={styles.sendButtonText}>Send</ThemedText>
            </TouchableOpacity>
          </View>
        </ThemedView>

        {/* Response Area */}
        <ThemedView style={styles.section}>
          <ThemedText type="subtitle" style={styles.sectionTitle}>
            üìù Response
          </ThemedText>
          <ThemedView style={[styles.responseContainer, { borderColor: colors.icon }]}>
            <ScrollView style={styles.responseScroll}>
              <ThemedText style={styles.responseText}>
                {responseText || 'Waiting for response...'}
              </ThemedText>
            </ScrollView>
          </ThemedView>
        </ThemedView>
      </ScrollView>
    </ThemedView>
  );
}

const styles = StyleSheet.create({
  container: {
    flex: 1,
  },
  scrollView: {
    flex: 1,
  },
  scrollContent: {
    padding: 20,
    paddingBottom: 40,
  },
  header: {
    alignItems: 'center',
    marginBottom: 24,
  },
  title: {
    fontSize: 28,
    fontWeight: 'bold',
    marginBottom: 8,
  },
  subtitle: {
    fontSize: 16,
    opacity: 0.7,
    textAlign: 'center',
  },
  statusContainer: {
    padding: 16,
    borderRadius: 12,
    borderWidth: 2,
    marginBottom: 24,
  },
  statusText: {
    fontSize: 14,
    fontWeight: '600',
    textAlign: 'center',
  },
  section: {
    marginBottom: 24,
  },
  sectionTitle: {
    fontSize: 18,
    fontWeight: '600',
    marginBottom: 12,
  },
  modeContainer: {
    flexDirection: 'row',
    flexWrap: 'wrap',
    gap: 12,
  },
  modeButton: {
    flex: 1,
    minWidth: 100,
    padding: 12,
    borderRadius: 8,
    borderWidth: 2,
    alignItems: 'center',
  },
  modeButtonActive: {
    borderWidth: 2,
  },
  modeButtonText: {
    fontSize: 14,
    fontWeight: '600',
  },
  modeButtonTextActive: {
    color: '#fff',
  },
  buttonRow: {
    flexDirection: 'row',
    gap: 12,
  },
  actionButton: {
    flex: 1,
    padding: 16,
    borderRadius: 12,
    alignItems: 'center',
  },
  startButton: {
    backgroundColor: '#10b981',
  },
  stopButton: {
    backgroundColor: '#ef4444',
  },
  buttonDisabled: {
    opacity: 0.5,
  },
  buttonText: {
    color: '#fff',
    fontSize: 16,
    fontWeight: '600',
  },
  inputContainer: {
    flexDirection: 'row',
    gap: 12,
    alignItems: 'flex-end',
  },
  textInput: {
    flex: 1,
    minHeight: 50,
    maxHeight: 120,
    padding: 12,
    borderRadius: 8,
    borderWidth: 2,
    fontSize: 16,
    textAlignVertical: 'top',
  },
  inputDisabled: {
    opacity: 0.5,
  },
  sendButton: {
    paddingHorizontal: 24,
    paddingVertical: 14,
    borderRadius: 8,
    alignItems: 'center',
    justifyContent: 'center',
  },
  sendButtonText: {
    color: '#fff',
    fontSize: 16,
    fontWeight: '600',
  },
  responseContainer: {
    minHeight: 200,
    maxHeight: 400,
    padding: 16,
    borderRadius: 12,
    borderWidth: 2,
  },
  responseScroll: {
    flex: 1,
  },
  responseText: {
    fontSize: 16,
    lineHeight: 24,
  },
});

```

Checking for linting errors and creating an environment configuration file:


Creating an environment configuration file and updating the README with setup instructions:


```shellscript
# Backend URL - Update this to match your Flask server
# For local development, use your computer's IP address instead of localhost
# Example: http://192.168.1.100:5001
EXPO_PUBLIC_BACKEND_URL=http://localhost:5001

```

Updating the README with setup instructions:


```markdown
# A-eye - Gemini Live API Mobile App üéôÔ∏è

A React Native/Expo mobile app frontend for the Gemini Live API Flask backend. This app provides real-time audio and video interaction with Google Gemini AI.

## Features

- üé• **Video Modes**: Camera, Screen capture, or None
- üéôÔ∏è **Real-time Audio**: Two-way audio communication with AI
- üí¨ **Text Messaging**: Send text messages to the AI assistant
- üì± **Cross-platform**: Works on iOS, Android, and Web
- üåì **Dark Mode**: Automatic light/dark theme support

## Prerequisites

1. **Flask Backend Running**: Make sure your Flask backend (`app.py`) is running on port 5001 (or configure a different port)
2. **Node.js**: Version 18 or higher
3. **Expo CLI**: Install globally with `npm install -g expo-cli`

## Setup

1. **Install dependencies**

   ```bash
   npm install
   ```

2. **Configure Backend URL**

   Create a `.env` file in the root directory (copy from `.env.example`):
   
   ```bash
   cp .env.example .env
   ```

   Update the `EXPO_PUBLIC_BACKEND_URL` in `.env`:
   - For **iOS Simulator**: Use `http://localhost:5001`
   - For **Android Emulator**: Use `http://10.0.2.2:5001`
   - For **Physical Device**: Use your computer's IP address (e.g., `http://192.168.1.100:5001`)

3. **Start the Flask Backend**

   In the root directory (where `app.py` is located):
   
   ```bash
   python app.py
   ```

4. **Start the Expo App**

   ```bash
   npx expo start
   ```

   In the output, you'll find options to open the app in a:
   - [development build](https://docs.expo.dev/develop/development-builds/introduction/)
   - [Android emulator](https://docs.expo.dev/workflow/android-studio-emulator/)
   - [iOS simulator](https://docs.expo.dev/workflow/ios-simulator/)
   - [Expo Go](https://expo.dev/go) (limited functionality)

## Usage

1. **Connect**: The app will automatically connect to the Flask backend when started
2. **Select Mode**: Choose Camera, Screen, or None for video input
3. **Start Session**: Tap "Start Session" to begin the AI interaction
4. **Send Messages**: Type messages and tap "Send" to communicate with the AI
5. **View Responses**: AI text responses will appear in the Response area
6. **Stop Session**: Tap "Stop Session" when done

## Project Structure

- `app/` - Main app screens and routing
- `contexts/` - React contexts (Socket.IO connection)
- `components/` - Reusable UI components
- `.env` - Environment variables (backend URL configuration)

## Troubleshooting

### Connection Issues

- **"Disconnected" status**: 
  - Ensure Flask backend is running
  - Check that the backend URL in `.env` is correct
  - For physical devices, ensure your phone and computer are on the same network

### Socket.IO Connection

- The app uses Socket.IO for real-time communication
- Make sure CORS is enabled on the Flask backend (already configured in `app.py`)

## Learn more

- [Expo documentation](https://docs.expo.dev/)
- [React Native documentation](https://reactnative.dev/)
- [Socket.IO Client documentation](https://socket.io/docs/v4/client-api/)

## Get a fresh project

When you're ready, run:

```bash
npm run reset-project
```

This command will move the starter code to the **app-example** directory and create a blank **app** directory where you can start developing.

## Learn more

To learn more about developing your project with Expo, look at the following resources:

- [Expo documentation](https://docs.expo.dev/): Learn fundamentals, or go into advanced topics with our [guides](https://docs.expo.dev/guides).
- [Learn Expo tutorial](https://docs.expo.dev/tutorial/introduction/): Follow a step-by-step tutorial where you'll create a project that runs on Android, iOS, and the web.

## Join the community

Join our community of developers creating universal apps.

- [Expo on GitHub](https://github.com/expo/expo): View our open source platform and contribute.
- [Discord community](https://chat.expo.dev): Chat with Expo users and ask questions.

```

Verifying the implementation and checking for any missing dependencies:


Updating SocketContext to use expo-constants for environment variables:


```typescriptreact
import React, { createContext, useContext, useEffect, useState } from 'react';
import { io, Socket } from 'socket.io-client';
import Constants from 'expo-constants';

type SessionMode = 'camera' | 'screen' | 'none';

interface SocketContextType {
  socket: Socket | null;
  connected: boolean;
  sessionActive: boolean;
  currentMode: SessionMode;
  responseText: string;
  statusMessage: string;
  statusType: 'connected' | 'disconnected' | 'info' | 'error';
  startSession: (mode: SessionMode) => void;
  stopSession: () => void;
  sendMessage: (text: string) => void;
  setCurrentMode: (mode: SessionMode) => void;
}

const SocketContext = createContext<SocketContextType | undefined>(undefined);

// Get backend URL from environment variable or Constants, with fallback
const getBackendUrl = () => {
  // Try process.env first (works in Expo SDK 49+)
  if (process.env.EXPO_PUBLIC_BACKEND_URL) {
    return process.env.EXPO_PUBLIC_BACKEND_URL;
  }
  // Try Constants.expoConfig.extra (for app.json config)
  if (Constants.expoConfig?.extra?.backendUrl) {
    return Constants.expoConfig.extra.backendUrl;
  }
  // Default fallback
  return 'http://localhost:5001';
};

const BACKEND_URL = getBackendUrl();

export function SocketProvider({ children }: { children: React.ReactNode }) {
  const [socket, setSocket] = useState<Socket | null>(null);
  const [connected, setConnected] = useState(false);
  const [sessionActive, setSessionActive] = useState(false);
  const [currentMode, setCurrentMode] = useState<SessionMode>('camera');
  const [responseText, setResponseText] = useState('');
  const [statusMessage, setStatusMessage] = useState('Disconnected');
  const [statusType, setStatusType] = useState<'connected' | 'disconnected' | 'info' | 'error'>('disconnected');

  useEffect(() => {
    // Initialize socket connection
    const newSocket = io(BACKEND_URL, {
      transports: ['websocket', 'polling'],
      reconnection: true,
      reconnectionDelay: 1000,
      reconnectionAttempts: 5,
    });

    // Connection events
    newSocket.on('connect', () => {
      setConnected(true);
      setStatusMessage('Connected to server');
      setStatusType('connected');
    });

    newSocket.on('disconnect', () => {
      setConnected(false);
      setSessionActive(false);
      setStatusMessage('Disconnected from server');
      setStatusType('disconnected');
    });

    newSocket.on('connected', (data: { status: string }) => {
      setStatusMessage(`Connected: ${data.status}`);
      setStatusType('connected');
    });

    // Session events
    newSocket.on('session_started', (data: { status: string; mode: SessionMode }) => {
      setSessionActive(true);
      setStatusMessage(`Session started in ${data.mode} mode`);
      setStatusType('info');
    });

    newSocket.on('session_stopped', (data: { status: string }) => {
      setSessionActive(false);
      setStatusMessage('Session stopped');
      setStatusType('disconnected');
      setResponseText('');
    });

    // Response events
    newSocket.on('text_response', (data: { text: string }) => {
      setResponseText((prev) => prev + data.text);
    });

    newSocket.on('audio_data', (data: { data: string }) => {
      // Audio data handling can be added here if needed
      console.log('Audio data received');
    });

    newSocket.on('info', (data: { message: string }) => {
      setStatusMessage(`Info: ${data.message}`);
      setStatusType('info');
    });

    newSocket.on('error', (data: { message: string }) => {
      const msg = data.message || '';
      // Don't show alerts for disabled features, just update status
      if (
        msg.includes('not available') ||
        msg.includes('disabled') ||
        msg.includes('PyAudio') ||
        msg.includes('Audio output') ||
        msg.includes('Audio input') ||
        msg.includes('Camera') ||
        msg.includes('Text-only')
      ) {
        setStatusMessage(`Info: ${msg}`);
        setStatusType('info');
      } else {
        setStatusMessage(`Error: ${msg}`);
        setStatusType('error');
      }
    });

    setSocket(newSocket);

    return () => {
      newSocket.close();
    };
  }, []);

  const startSession = (mode: SessionMode) => {
    if (socket && connected) {
      socket.emit('start_session', { mode });
      setCurrentMode(mode);
    }
  };

  const stopSession = () => {
    if (socket && sessionActive) {
      socket.emit('stop_session');
      setResponseText('');
    }
  };

  const sendMessage = (text: string) => {
    if (socket && sessionActive && text.trim()) {
      socket.emit('send_message', { text: text.trim() });
    }
  };

  return (
    <SocketContext.Provider
      value={{
        socket,
        connected,
        sessionActive,
        currentMode,
        responseText,
        statusMessage,
        statusType,
        startSession,
        stopSession,
        sendMessage,
        setCurrentMode,
      }}>
      {children}
    </SocketContext.Provider>
  );
}

export function useSocket() {
  const context = useContext(SocketContext);
  if (context === undefined) {
    throw new Error('useSocket must be used within a SocketProvider');
  }
  return context;
}


```

Verifying the implementation is complete. Creating a quick setup guide:


## Summary

Frontend for the Flask backend is set up. Implemented:

### Completed tasks

1. Installed socket.io-client for React Native
2. Created Socket.IO context (`contexts/SocketContext.tsx`) for managing the connection
3. Built the main AI assistant screen (`app/(tabs)/index.tsx`) with:
   - Mode selection (Camera/Screen/None)
   - Start/Stop session controls
   - Message input and send functionality
   - Response display area
   - Status indicators
4. Updated root layout to include the SocketProvider
5. Added environment configuration (`.env.example`) and updated README

### Key features

- Real-time Socket.IO connection to Flask backend
- Session management (start/stop)
- Mode selection (camera, screen, none)
- Text messaging interface
- Response display
- Status indicators (connected/disconnected/info/error)
- Dark mode support (uses existing theme system)
- Responsive UI

### Next steps

1. Create `.env` file:
   ```bash
   cd A-eye
   cp .env.example .env
   ```
   Then update `EXPO_PUBLIC_BACKEND_URL` with your backend URL.

2. Start the Flask backend (in the root directory):
   ```bash
   python app.py
   ```

3. Start the Expo app:
   ```bash
   cd A-eye
   npx expo start
   ```

The app connects to your Flask backend and provides a mobile interface for the Gemini Live API. The UI matches the existing Expo app theme and supports light/dark modes.

```shellscript
   cd A-eye
   cp .env.example .env
```

```shellscript
   python app.py
```

```shellscript
   cd A-eye
   npx expo start
```

---

**User**

Make the UI as minimalistic as possible. I only want a start and stop session button. I want to see what the back camera sees. Remove any and all UI elements unrelated to that. Make sure that the backend is connected and working with the expo frontend. 

---

**Cursor**


Simplifying the UI to only start/stop buttons and a back camera preview. Checking available camera libraries and updating the implementation.



Simplifying the UI to only start/stop buttons and a back camera preview. Installing expo-camera and updating the implementation.



Installing expo-camera and updating the UI:



Updating app.json for camera permissions and creating the minimal UI:


```json
{
  "expo": {
    "name": "A-eye",
    "slug": "A-eye",
    "version": "1.0.0",
    "orientation": "portrait",
    "icon": "./assets/images/icon.png",
    "scheme": "aeye",
    "userInterfaceStyle": "automatic",
    "newArchEnabled": true,
    "ios": {
      "supportsTablet": true
    },
    "android": {
      "adaptiveIcon": {
        "backgroundColor": "#E6F4FE",
        "foregroundImage": "./assets/images/android-icon-foreground.png",
        "backgroundImage": "./assets/images/android-icon-background.png",
        "monochromeImage": "./assets/images/android-icon-monochrome.png"
      },
      "edgeToEdgeEnabled": true,
      "predictiveBackGestureEnabled": false
    },
    "web": {
      "output": "static",
      "favicon": "./assets/images/favicon.png"
    },
    "plugins": [
      "expo-router",
      [
        "expo-splash-screen",
        {
          "image": "./assets/images/splash-icon.png",
          "imageWidth": 200,
          "resizeMode": "contain",
          "backgroundColor": "#ffffff",
          "dark": {
            "backgroundColor": "#000000"
          }
        }
      ],
      [
        "expo-camera",
        {
          "cameraPermission": "Allow A-eye to access your camera to stream video to Gemini Live API.",
          "microphonePermission": "Allow A-eye to access your microphone to stream audio to Gemini Live API."
        }
      ]
    ],
    "experiments": {
      "typedRoutes": true,
      "reactCompiler": true
    }
  }
}

```

```typescriptreact
import React, { createContext, useContext, useEffect, useState } from 'react';
import { io, Socket } from 'socket.io-client';
import Constants from 'expo-constants';

interface SocketContextType {
  socket: Socket | null;
  connected: boolean;
  sessionActive: boolean;
  startSession: () => void;
  stopSession: () => void;
}

const SocketContext = createContext<SocketContextType | undefined>(undefined);

// Get backend URL from environment variable or Constants, with fallback
const getBackendUrl = () => {
  // Try process.env first (works in Expo SDK 49+)
  if (process.env.EXPO_PUBLIC_BACKEND_URL) {
    return process.env.EXPO_PUBLIC_BACKEND_URL;
  }
  // Try Constants.expoConfig.extra (for app.json config)
  if (Constants.expoConfig?.extra?.backendUrl) {
    return Constants.expoConfig.extra.backendUrl;
  }
  // Default fallback
  return 'http://localhost:5001';
};

const BACKEND_URL = getBackendUrl();

export function SocketProvider({ children }: { children: React.ReactNode }) {
  const [socket, setSocket] = useState<Socket | null>(null);
  const [connected, setConnected] = useState(false);
  const [sessionActive, setSessionActive] = useState(false);
  const [currentMode, setCurrentMode] = useState<SessionMode>('camera');
  const [responseText, setResponseText] = useState('');
  const [statusMessage, setStatusMessage] = useState('Disconnected');
  const [statusType, setStatusType] = useState<'connected' | 'disconnected' | 'info' | 'error'>('disconnected');

  useEffect(() => {
    // Initialize socket connection
    const newSocket = io(BACKEND_URL, {
      transports: ['websocket', 'polling'],
      reconnection: true,
      reconnectionDelay: 1000,
      reconnectionAttempts: 5,
    });

    // Connection events
    newSocket.on('connect', () => {
      setConnected(true);
      setStatusMessage('Connected to server');
      setStatusType('connected');
    });

    newSocket.on('disconnect', () => {
      setConnected(false);
      setSessionActive(false);
      setStatusMessage('Disconnected from server');
      setStatusType('disconnected');
    });

    newSocket.on('connected', (data: { status: string }) => {
      setStatusMessage(`Connected: ${data.status}`);
      setStatusType('connected');
    });

    // Session events
    newSocket.on('session_started', (data: { status: string; mode: SessionMode }) => {
      setSessionActive(true);
      setStatusMessage(`Session started in ${data.mode} mode`);
      setStatusType('info');
    });

    newSocket.on('session_stopped', (data: { status: string }) => {
      setSessionActive(false);
      setStatusMessage('Session stopped');
      setStatusType('disconnected');
      setResponseText('');
    });

    // Response events
    newSocket.on('text_response', (data: { text: string }) => {
      setResponseText((prev) => prev + data.text);
    });

    newSocket.on('audio_data', (data: { data: string }) => {
      // Audio data handling can be added here if needed
      console.log('Audio data received');
    });

    newSocket.on('info', (data: { message: string }) => {
      setStatusMessage(`Info: ${data.message}`);
      setStatusType('info');
    });

    newSocket.on('error', (data: { message: string }) => {
      const msg = data.message || '';
      // Don't show alerts for disabled features, just update status
      if (
        msg.includes('not available') ||
        msg.includes('disabled') ||
        msg.includes('PyAudio') ||
        msg.includes('Audio output') ||
        msg.includes('Audio input') ||
        msg.includes('Camera') ||
        msg.includes('Text-only')
      ) {
        setStatusMessage(`Info: ${msg}`);
        setStatusType('info');
      } else {
        setStatusMessage(`Error: ${msg}`);
        setStatusType('error');
      }
    });

    setSocket(newSocket);

    return () => {
      newSocket.close();
    };
  }, []);

  const startSession = (mode: SessionMode) => {
    if (socket && connected) {
      socket.emit('start_session', { mode });
      setCurrentMode(mode);
    }
  };

  const stopSession = () => {
    if (socket && sessionActive) {
      socket.emit('stop_session');
      setResponseText('');
    }
  };

  const sendMessage = (text: string) => {
    if (socket && sessionActive && text.trim()) {
      socket.emit('send_message', { text: text.trim() });
    }
  };

  return (
    <SocketContext.Provider
      value={{
        socket,
        connected,
        sessionActive,
        currentMode,
        responseText,
        statusMessage,
        statusType,
        startSession,
        stopSession,
        sendMessage,
        setCurrentMode,
      }}>
      {children}
    </SocketContext.Provider>
  );
}

export function useSocket() {
  const context = useContext(SocketContext);
  if (context === undefined) {
    throw new Error('useSocket must be used within a SocketProvider');
  }
  return context;
}


```

```typescriptreact
import React, { createContext, useContext, useEffect, useState } from 'react';
import { io, Socket } from 'socket.io-client';
import Constants from 'expo-constants';

interface SocketContextType {
  socket: Socket | null;
  connected: boolean;
  sessionActive: boolean;
  startSession: () => void;
  stopSession: () => void;
}

const SocketContext = createContext<SocketContextType | undefined>(undefined);

// Get backend URL from environment variable or Constants, with fallback
const getBackendUrl = () => {
  // Try process.env first (works in Expo SDK 49+)
  if (process.env.EXPO_PUBLIC_BACKEND_URL) {
    return process.env.EXPO_PUBLIC_BACKEND_URL;
  }
  // Try Constants.expoConfig.extra (for app.json config)
  if (Constants.expoConfig?.extra?.backendUrl) {
    return Constants.expoConfig.extra.backendUrl;
  }
  // Default fallback
  return 'http://localhost:5001';
};

const BACKEND_URL = getBackendUrl();

export function SocketProvider({ children }: { children: React.ReactNode }) {
  const [socket, setSocket] = useState<Socket | null>(null);
  const [connected, setConnected] = useState(false);
  const [sessionActive, setSessionActive] = useState(false);

  useEffect(() => {
    // Initialize socket connection
    const newSocket = io(BACKEND_URL, {
      transports: ['websocket', 'polling'],
      reconnection: true,
      reconnectionDelay: 1000,
      reconnectionAttempts: 5,
    });

    // Connection events
    newSocket.on('connect', () => {
      setConnected(true);
    });

    newSocket.on('disconnect', () => {
      setConnected(false);
      setSessionActive(false);
    });

    // Session events
    newSocket.on('session_started', () => {
      setSessionActive(true);
    });

    newSocket.on('session_stopped', () => {
      setSessionActive(false);
    });

    // Silently handle other events (no UI updates needed)
    newSocket.on('text_response', () => {});
    newSocket.on('audio_data', () => {});
    newSocket.on('info', () => {});
    newSocket.on('error', () => {});

    setSocket(newSocket);

    return () => {
      newSocket.close();
    };
  }, []);

  const startSession = () => {
    if (socket && connected) {
      socket.emit('start_session', { mode: 'camera' });
    }
  };

  const stopSession = () => {
    if (socket && sessionActive) {
      socket.emit('stop_session');
    }
  };

  return (
    <SocketContext.Provider
      value={{
        socket,
        connected,
        sessionActive,
        currentMode,
        responseText,
        statusMessage,
        statusType,
        startSession,
        stopSession,
        sendMessage,
        setCurrentMode,
      }}>
      {children}
    </SocketContext.Provider>
  );
}

export function useSocket() {
  const context = useContext(SocketContext);
  if (context === undefined) {
    throw new Error('useSocket must be used within a SocketProvider');
  }
  return context;
}


```

```typescriptreact
import React, { createContext, useContext, useEffect, useState } from 'react';
import { io, Socket } from 'socket.io-client';
import Constants from 'expo-constants';

interface SocketContextType {
  socket: Socket | null;
  connected: boolean;
  sessionActive: boolean;
  startSession: () => void;
  stopSession: () => void;
}

const SocketContext = createContext<SocketContextType | undefined>(undefined);

// Get backend URL from environment variable or Constants, with fallback
const getBackendUrl = () => {
  // Try process.env first (works in Expo SDK 49+)
  if (process.env.EXPO_PUBLIC_BACKEND_URL) {
    return process.env.EXPO_PUBLIC_BACKEND_URL;
  }
  // Try Constants.expoConfig.extra (for app.json config)
  if (Constants.expoConfig?.extra?.backendUrl) {
    return Constants.expoConfig.extra.backendUrl;
  }
  // Default fallback
  return 'http://localhost:5001';
};

const BACKEND_URL = getBackendUrl();

export function SocketProvider({ children }: { children: React.ReactNode }) {
  const [socket, setSocket] = useState<Socket | null>(null);
  const [connected, setConnected] = useState(false);
  const [sessionActive, setSessionActive] = useState(false);

  useEffect(() => {
    // Initialize socket connection
    const newSocket = io(BACKEND_URL, {
      transports: ['websocket', 'polling'],
      reconnection: true,
      reconnectionDelay: 1000,
      reconnectionAttempts: 5,
    });

    // Connection events
    newSocket.on('connect', () => {
      setConnected(true);
    });

    newSocket.on('disconnect', () => {
      setConnected(false);
      setSessionActive(false);
    });

    // Session events
    newSocket.on('session_started', () => {
      setSessionActive(true);
    });

    newSocket.on('session_stopped', () => {
      setSessionActive(false);
    });

    // Silently handle other events (no UI updates needed)
    newSocket.on('text_response', () => {});
    newSocket.on('audio_data', () => {});
    newSocket.on('info', () => {});
    newSocket.on('error', () => {});

    setSocket(newSocket);

    return () => {
      newSocket.close();
    };
  }, []);

  const startSession = () => {
    if (socket && connected) {
      socket.emit('start_session', { mode: 'camera' });
    }
  };

  const stopSession = () => {
    if (socket && sessionActive) {
      socket.emit('stop_session');
    }
  };

  return (
    <SocketContext.Provider
      value={{
        socket,
        connected,
        sessionActive,
        startSession,
        stopSession,
      }}>
      {children}
    </SocketContext.Provider>
  );
}

export function useSocket() {
  const context = useContext(SocketContext);
  if (context === undefined) {
    throw new Error('useSocket must be used within a SocketProvider');
  }
  return context;
}


```

```typescriptreact
import React, { useState, useRef, useEffect } from 'react';
import { StyleSheet, View, TouchableOpacity, Platform, Alert } from 'react-native';
import { CameraView, CameraType, useCameraPermissions } from 'expo-camera';
import { ThemedText } from '@/components/themed-text';
import { ThemedView } from '@/components/themed-view';
import { useSocket } from '@/contexts/SocketContext';
import { useColorScheme } from '@/hooks/use-color-scheme';
import { Colors } from '@/constants/theme';

export default function HomeScreen() {
  const colorScheme = useColorScheme();
  const { connected, sessionActive, startSession, stopSession } = useSocket();
  const [facing, setFacing] = useState<CameraType>('back');
  const [permission, requestPermission] = useCameraPermissions();
  const cameraRef = useRef<CameraView>(null);

  useEffect(() => {
    // Request camera permission on mount
    if (!permission?.granted) {
      requestPermission();
    }
  }, [permission, requestPermission]);

  const handleStartSession = () => {
    if (!connected) {
      Alert.alert('Not Connected', 'Please wait for connection to server');
      return;
    }
    if (!permission?.granted) {
      Alert.alert('Camera Permission', 'Please grant camera permission to start session');
      requestPermission();
      return;
    }
    startSession();
  };

  const handleStopSession = () => {
    stopSession();
  };

  const colors = Colors[colorScheme ?? 'light'];

  if (!permission) {
    return (
      <ThemedView style={styles.container}>
        <ThemedText style={styles.loadingText}>Requesting camera permission...</ThemedText>
      </ThemedView>
    );
  }

  if (!permission.granted) {
    return (
      <ThemedView style={styles.container}>
        <ThemedText style={styles.permissionText}>Camera permission is required</ThemedText>
        <TouchableOpacity
          style={[styles.permissionButton, { backgroundColor: colors.tint }]}
          onPress={requestPermission}>
          <ThemedText style={styles.permissionButtonText}>Grant Permission</ThemedText>
        </TouchableOpacity>
      </ThemedView>
    );
  }

  return (
    <ThemedView style={styles.container}>
      <CameraView
        ref={cameraRef}
        style={styles.camera}
        facing={facing}
        mode="picture"
      />

      <View style={styles.controls}>
        <TouchableOpacity
          style={[
            styles.button,
            styles.startButton,
            (!connected || sessionActive) && styles.buttonDisabled,
            { backgroundColor: connected && !sessionActive ? '#10b981' : '#6b7280' },
          ]}
          onPress={handleStartSession}
          disabled={!connected || sessionActive}>
          <ThemedText style={styles.buttonText}>Start</ThemedText>
        </TouchableOpacity>

        <TouchableOpacity
          style={[
            styles.button,
            styles.stopButton,
            !sessionActive && styles.buttonDisabled,
            { backgroundColor: sessionActive ? '#ef4444' : '#6b7280' },
          ]}
          onPress={handleStopSession}
          disabled={!sessionActive}>
          <ThemedText style={styles.buttonText}>Stop</ThemedText>
        </TouchableOpacity>
      </View>
    </ThemedView>
  );
}

const styles = StyleSheet.create({
  container: {
    flex: 1,
    backgroundColor: '#000',
  },
  camera: {
    flex: 1,
  },
  controls: {
    position: 'absolute',
    bottom: 0,
    left: 0,
    right: 0,
    flexDirection: 'row',
    justifyContent: 'center',
    alignItems: 'center',
    paddingBottom: Platform.OS === 'ios' ? 40 : 20,
    paddingHorizontal: 20,
    gap: 20,
  },
  button: {
    flex: 1,
    maxWidth: 150,
    paddingVertical: 16,
    paddingHorizontal: 32,
    borderRadius: 12,
    alignItems: 'center',
    justifyContent: 'center',
  },
  startButton: {
    backgroundColor: '#10b981',
  },
  stopButton: {
    backgroundColor: '#ef4444',
  },
  buttonDisabled: {
    opacity: 0.5,
  },
  buttonText: {
    color: '#fff',
    fontSize: 18,
    fontWeight: '600',
  },
  loadingText: {
    color: '#fff',
    fontSize: 16,
    textAlign: 'center',
    marginTop: 100,
  },
  permissionText: {
    color: '#fff',
    fontSize: 18,
    textAlign: 'center',
    marginBottom: 20,
    marginTop: 100,
  },
  permissionButton: {
    paddingVertical: 16,
    paddingHorizontal: 32,
    borderRadius: 12,
    alignSelf: 'center',
  },
  permissionButtonText: {
    color: '#fff',
    fontSize: 18,
    fontWeight: '600',
  },
});

```


Checking for linting errors and verifying the implementation:



Checking if expo-camera is installed and verifying the backend connection setup:



expo-camera is installed. Adding minimal connection status feedback and verifying the backend connection. Updating the UI to show connection status subtly:


```typescriptreact
import React, { useState, useRef, useEffect } from 'react';
import { StyleSheet, View, TouchableOpacity, Platform, Alert } from 'react-native';
import { CameraView, CameraType, useCameraPermissions } from 'expo-camera';
import { ThemedText } from '@/components/themed-text';
import { ThemedView } from '@/components/themed-view';
import { useSocket } from '@/contexts/SocketContext';
import { useColorScheme } from '@/hooks/use-color-scheme';
import { Colors } from '@/constants/theme';

export default function HomeScreen() {
  const colorScheme = useColorScheme();
  const { connected, sessionActive, startSession, stopSession } = useSocket();
  const [facing, setFacing] = useState<CameraType>('back');
  const [permission, requestPermission] = useCameraPermissions();
  const cameraRef = useRef<CameraView>(null);

  useEffect(() => {
    // Request camera permission on mount
    if (!permission?.granted) {
      requestPermission();
    }
  }, [permission, requestPermission]);

  const handleStartSession = () => {
    if (!connected) {
      Alert.alert('Not Connected', 'Please wait for connection to server');
      return;
    }
    if (!permission?.granted) {
      Alert.alert('Camera Permission', 'Please grant camera permission to start session');
      requestPermission();
      return;
    }
    startSession();
  };

  const handleStopSession = () => {
    stopSession();
  };

  const colors = Colors[colorScheme ?? 'light'];

  if (!permission) {
    return (
      <ThemedView style={styles.container}>
        <ThemedText style={styles.loadingText}>Requesting camera permission...</ThemedText>
      </ThemedView>
    );
  }

  if (!permission.granted) {
    return (
      <ThemedView style={styles.container}>
        <ThemedText style={styles.permissionText}>Camera permission is required</ThemedText>
        <TouchableOpacity
          style={[styles.permissionButton, { backgroundColor: colors.tint }]}
          onPress={requestPermission}>
          <ThemedText style={styles.permissionButtonText}>Grant Permission</ThemedText>
        </TouchableOpacity>
      </ThemedView>
    );
  }

  return (
    <ThemedView style={styles.container}>
      <CameraView
        ref={cameraRef}
        style={styles.camera}
        facing={facing}
        mode="picture"
      />

      {/* Minimal connection indicator */}
      {!connected && (
        <View style={styles.connectionIndicator}>
          <View style={[styles.connectionDot, { backgroundColor: '#ef4444' }]} />
          <ThemedText style={styles.connectionText}>Connecting...</ThemedText>
        </View>
      )}

      <View style={styles.controls}>
        <TouchableOpacity
          style={[
            styles.button,
            styles.startButton,
            (!connected || sessionActive) && styles.buttonDisabled,
            { backgroundColor: connected && !sessionActive ? '#10b981' : '#6b7280' },
          ]}
          onPress={handleStartSession}
          disabled={!connected || sessionActive}>
          <ThemedText style={styles.buttonText}>Start</ThemedText>
        </TouchableOpacity>

        <TouchableOpacity
          style={[
            styles.button,
            styles.stopButton,
            !sessionActive && styles.buttonDisabled,
            { backgroundColor: sessionActive ? '#ef4444' : '#6b7280' },
          ]}
          onPress={handleStopSession}
          disabled={!sessionActive}>
          <ThemedText style={styles.buttonText}>Stop</ThemedText>
        </TouchableOpacity>
      </View>
    </ThemedView>
  );
}

const styles = StyleSheet.create({
  container: {
    flex: 1,
    backgroundColor: '#000',
  },
  camera: {
    flex: 1,
  },
  controls: {
    position: 'absolute',
    bottom: 0,
    left: 0,
    right: 0,
    flexDirection: 'row',
    justifyContent: 'center',
    alignItems: 'center',
    paddingBottom: Platform.OS === 'ios' ? 40 : 20,
    paddingHorizontal: 20,
    gap: 20,
  },
  button: {
    flex: 1,
    maxWidth: 150,
    paddingVertical: 16,
    paddingHorizontal: 32,
    borderRadius: 12,
    alignItems: 'center',
    justifyContent: 'center',
  },
  startButton: {
    backgroundColor: '#10b981',
  },
  stopButton: {
    backgroundColor: '#ef4444',
  },
  buttonDisabled: {
    opacity: 0.5,
  },
  buttonText: {
    color: '#fff',
    fontSize: 18,
    fontWeight: '600',
  },
  loadingText: {
    color: '#fff',
    fontSize: 16,
    textAlign: 'center',
    marginTop: 100,
  },
  permissionText: {
    color: '#fff',
    fontSize: 18,
    textAlign: 'center',
    marginBottom: 20,
    marginTop: 100,
  },
  permissionButton: {
    paddingVertical: 16,
    paddingHorizontal: 32,
    borderRadius: 12,
    alignSelf: 'center',
  },
  permissionButtonText: {
    color: '#fff',
    fontSize: 18,
    fontWeight: '600',
  },
});

```

```typescriptreact
import React, { useState, useRef, useEffect } from 'react';
import { StyleSheet, View, TouchableOpacity, Platform, Alert } from 'react-native';
import { CameraView, CameraType, useCameraPermissions } from 'expo-camera';
import { ThemedText } from '@/components/themed-text';
import { ThemedView } from '@/components/themed-view';
import { useSocket } from '@/contexts/SocketContext';
import { useColorScheme } from '@/hooks/use-color-scheme';
import { Colors } from '@/constants/theme';

export default function HomeScreen() {
  const colorScheme = useColorScheme();
  const { connected, sessionActive, startSession, stopSession } = useSocket();
  const [facing, setFacing] = useState<CameraType>('back');
  const [permission, requestPermission] = useCameraPermissions();
  const cameraRef = useRef<CameraView>(null);

  useEffect(() => {
    // Request camera permission on mount
    if (!permission?.granted) {
      requestPermission();
    }
  }, [permission, requestPermission]);

  const handleStartSession = () => {
    if (!connected) {
      Alert.alert('Not Connected', 'Please wait for connection to server');
      return;
    }
    if (!permission?.granted) {
      Alert.alert('Camera Permission', 'Please grant camera permission to start session');
      requestPermission();
      return;
    }
    startSession();
  };

  const handleStopSession = () => {
    stopSession();
  };

  const colors = Colors[colorScheme ?? 'light'];

  if (!permission) {
    return (
      <ThemedView style={styles.container}>
        <ThemedText style={styles.loadingText}>Requesting camera permission...</ThemedText>
      </ThemedView>
    );
  }

  if (!permission.granted) {
    return (
      <ThemedView style={styles.container}>
        <ThemedText style={styles.permissionText}>Camera permission is required</ThemedText>
        <TouchableOpacity
          style={[styles.permissionButton, { backgroundColor: colors.tint }]}
          onPress={requestPermission}>
          <ThemedText style={styles.permissionButtonText}>Grant Permission</ThemedText>
        </TouchableOpacity>
      </ThemedView>
    );
  }

  return (
    <ThemedView style={styles.container}>
      <CameraView
        ref={cameraRef}
        style={styles.camera}
        facing={facing}
        mode="picture"
      />

      {/* Minimal connection indicator */}
      {!connected && (
        <View style={styles.connectionIndicator}>
          <View style={[styles.connectionDot, { backgroundColor: '#ef4444' }]} />
          <ThemedText style={styles.connectionText}>Connecting...</ThemedText>
        </View>
      )}

      <View style={styles.controls}>
        <TouchableOpacity
          style={[
            styles.button,
            styles.startButton,
            (!connected || sessionActive) && styles.buttonDisabled,
            { backgroundColor: connected && !sessionActive ? '#10b981' : '#6b7280' },
          ]}
          onPress={handleStartSession}
          disabled={!connected || sessionActive}>
          <ThemedText style={styles.buttonText}>Start</ThemedText>
        </TouchableOpacity>

        <TouchableOpacity
          style={[
            styles.button,
            styles.stopButton,
            !sessionActive && styles.buttonDisabled,
            { backgroundColor: sessionActive ? '#ef4444' : '#6b7280' },
          ]}
          onPress={handleStopSession}
          disabled={!sessionActive}>
          <ThemedText style={styles.buttonText}>Stop</ThemedText>
        </TouchableOpacity>
      </View>
    </ThemedView>
  );
}

const styles = StyleSheet.create({
  container: {
    flex: 1,
    backgroundColor: '#000',
  },
  camera: {
    flex: 1,
  },
  controls: {
    position: 'absolute',
    bottom: 0,
    left: 0,
    right: 0,
    flexDirection: 'row',
    justifyContent: 'center',
    alignItems: 'center',
    paddingBottom: Platform.OS === 'ios' ? 40 : 20,
    paddingHorizontal: 20,
    gap: 20,
  },
  button: {
    flex: 1,
    maxWidth: 150,
    paddingVertical: 16,
    paddingHorizontal: 32,
    borderRadius: 12,
    alignItems: 'center',
    justifyContent: 'center',
  },
  startButton: {
    backgroundColor: '#10b981',
  },
  stopButton: {
    backgroundColor: '#ef4444',
  },
  buttonDisabled: {
    opacity: 0.5,
  },
  buttonText: {
    color: '#fff',
    fontSize: 18,
    fontWeight: '600',
  },
  loadingText: {
    color: '#fff',
    fontSize: 16,
    textAlign: 'center',
    marginTop: 100,
  },
  permissionText: {
    color: '#fff',
    fontSize: 18,
    textAlign: 'center',
    marginBottom: 20,
    marginTop: 100,
  },
  permissionButton: {
    paddingVertical: 16,
    paddingHorizontal: 32,
    borderRadius: 12,
    alignSelf: 'center',
  },
  permissionButtonText: {
    color: '#fff',
    fontSize: 18,
    fontWeight: '600',
  },
  connectionIndicator: {
    position: 'absolute',
    top: Platform.OS === 'ios' ? 60 : 40,
    left: 20,
    flexDirection: 'row',
    alignItems: 'center',
    gap: 8,
    backgroundColor: 'rgba(0, 0, 0, 0.5)',
    paddingHorizontal: 12,
    paddingVertical: 6,
    borderRadius: 20,
  },
  connectionDot: {
    width: 8,
    height: 8,
    borderRadius: 4,
  },
  connectionText: {
    color: '#fff',
    fontSize: 12,
    fontWeight: '500',
  },
});

```


Verifying the backend connection and completing the setup:



## Changes Made

### 1. **Minimalistic UI**
   - Removed: mode selection, message input, response display, status messages
   - Kept: Start/Stop buttons and camera preview

### 2. **Camera Preview**
   - Added `expo-camera` for back camera preview
   - Full-screen camera view showing the back camera feed
   - Camera permissions handled automatically

### 3. **Simplified Socket Context**
   - Removed unnecessary state (mode selection, messages, responses)
   - Kept: connection status, session status, start/stop functions
   - Always uses 'camera' mode when starting a session

### 4. **Backend Connection**
   - Socket.IO connection to Flask backend
   - Automatically connects on app start
   - Sends `start_session` with `mode: 'camera'` when Start is pressed
   - Minimal connection indicator (only shows when disconnected)

### 5. **UI Elements**
   - Full-screen camera preview (back camera)
   - Two buttons at the bottom: Start (green) and Stop (red)
   - Small connection indicator in the top-left (only visible when disconnected)
   - Buttons disabled when not connected or when inappropriate

## How It Works

1. App connects to Flask backend automatically
2. Camera preview shows the back camera feed
3. Press Start to begin a session (sends `start_session` with camera mode to backend)
4. Press Stop to end the session

The backend uses the computer's camera for AI processing, while the mobile app shows the phone's camera preview. The Socket.IO connection handles session management between the app and backend.

To test:
1. Make sure Flask backend is running (`python app.py`)
2. Update `.env` with the correct backend URL (use your computer's IP for physical devices)
3. Run `npx expo start` in the A-eye directory
4. The app will show the camera preview and connect to the backend automatically

---

**User**

Doesnt look like the backend is connecting properly.

---

**Cursor**


Checking the connection setup. The backend is running, but the `.env` may be pointing to the wrong URL. Verifying and fixing:



Updating `.env` to use the correct IP address and adding connection debugging to diagnose the issue:


```properties
# Backend URL - Update this to match your Flask server
# For local development, use your computer's IP address instead of localhost
# Example: http://192.168.1.100:5001
EXPO_PUBLIC_BACKEND_URL=http://192.168.1.50:5001


```

```typescriptreact
import React, { createContext, useContext, useEffect, useState } from 'react';
import { io, Socket } from 'socket.io-client';
import Constants from 'expo-constants';

interface SocketContextType {
  socket: Socket | null;
  connected: boolean;
  sessionActive: boolean;
  startSession: () => void;
  stopSession: () => void;
}

const SocketContext = createContext<SocketContextType | undefined>(undefined);

// Get backend URL from environment variable or Constants, with fallback
const getBackendUrl = () => {
  // Try process.env first (works in Expo SDK 49+)
  if (process.env.EXPO_PUBLIC_BACKEND_URL) {
    return process.env.EXPO_PUBLIC_BACKEND_URL;
  }
  // Try Constants.expoConfig.extra (for app.json config)
  if (Constants.expoConfig?.extra?.backendUrl) {
    return Constants.expoConfig.extra.backendUrl;
  }
  // Default fallback
  return 'http://localhost:5001';
};

const BACKEND_URL = getBackendUrl();

export function SocketProvider({ children }: { children: React.ReactNode }) {
  const [socket, setSocket] = useState<Socket | null>(null);
  const [connected, setConnected] = useState(false);
  const [sessionActive, setSessionActive] = useState(false);

  useEffect(() => {
    // Log the backend URL being used
    console.log('Connecting to backend at:', BACKEND_URL);

    // Initialize socket connection
    const newSocket = io(BACKEND_URL, {
      transports: ['websocket', 'polling'],
      reconnection: true,
      reconnectionDelay: 1000,
      reconnectionAttempts: 5,
      timeout: 10000,
    });

    // Connection events
    newSocket.on('connect', () => {
      console.log('‚úÖ Socket.IO connected successfully');
      setConnected(true);
    });

    newSocket.on('disconnect', (reason) => {
      console.log('‚ùå Socket.IO disconnected:', reason);
      setConnected(false);
      setSessionActive(false);
    });

    newSocket.on('connect_error', (error) => {
      console.error('‚ùå Socket.IO connection error:', error.message);
      setConnected(false);
    });

    newSocket.on('reconnect', (attemptNumber) => {
      console.log('üîÑ Socket.IO reconnected after', attemptNumber, 'attempts');
      setConnected(true);
    });

    newSocket.on('reconnect_attempt', () => {
      console.log('üîÑ Attempting to reconnect...');
    });

    newSocket.on('reconnect_error', (error) => {
      console.error('‚ùå Socket.IO reconnect error:', error.message);
    });

    newSocket.on('reconnect_failed', () => {
      console.error('‚ùå Socket.IO reconnection failed');
    });

    // Session events
    newSocket.on('session_started', (data) => {
      console.log('‚úÖ Session started:', data);
      setSessionActive(true);
    });

    newSocket.on('session_stopped', (data) => {
      console.log('‚èπÔ∏è Session stopped:', data);
      setSessionActive(false);
    });

    // Handle other events with logging
    newSocket.on('connected', (data) => {
      console.log('üì° Server connected event:', data);
    });

    newSocket.on('text_response', (data) => {
      console.log('üìù Text response received');
    });

    newSocket.on('audio_data', (data) => {
      console.log('üîä Audio data received');
    });

    newSocket.on('info', (data) => {
      console.log('‚ÑπÔ∏è Info:', data.message);
    });

    newSocket.on('error', (data) => {
      console.error('‚ùå Error:', data.message);
    });

    setSocket(newSocket);

    return () => {
      console.log('üßπ Cleaning up socket connection');
      newSocket.close();
    };
  }, []);

  const startSession = () => {
    if (socket && connected) {
      socket.emit('start_session', { mode: 'camera' });
    }
  };

  const stopSession = () => {
    if (socket && sessionActive) {
      socket.emit('stop_session');
    }
  };

  return (
    <SocketContext.Provider
      value={{
        socket,
        connected,
        sessionActive,
        startSession,
        stopSession,
      }}>
      {children}
    </SocketContext.Provider>
  );
}

export function useSocket() {
  const context = useContext(SocketContext);
  if (context === undefined) {
    throw new Error('useSocket must be used within a SocketProvider');
  }
  return context;
}


```


Updated the `.env` to use the correct IP address (`192.168.1.50:5001`) and added connection logging. Restart the Expo app to pick up the new environment variable. Adding a visual connection status indicator:


```typescriptreact
import React, { useState, useRef, useEffect } from 'react';
import { StyleSheet, View, TouchableOpacity, Platform, Alert } from 'react-native';
import { CameraView, CameraType, useCameraPermissions } from 'expo-camera';
import { ThemedText } from '@/components/themed-text';
import { ThemedView } from '@/components/themed-view';
import { useSocket } from '@/contexts/SocketContext';
import { useColorScheme } from '@/hooks/use-color-scheme';
import { Colors } from '@/constants/theme';

export default function HomeScreen() {
  const colorScheme = useColorScheme();
  const { connected, sessionActive, startSession, stopSession } = useSocket();
  const [facing, setFacing] = useState<CameraType>('back');
  const [permission, requestPermission] = useCameraPermissions();
  const cameraRef = useRef<CameraView>(null);

  useEffect(() => {
    // Request camera permission on mount
    if (!permission?.granted) {
      requestPermission();
    }
  }, [permission, requestPermission]);

  const handleStartSession = () => {
    if (!connected) {
      Alert.alert('Not Connected', 'Please wait for connection to server');
      return;
    }
    if (!permission?.granted) {
      Alert.alert('Camera Permission', 'Please grant camera permission to start session');
      requestPermission();
      return;
    }
    startSession();
  };

  const handleStopSession = () => {
    stopSession();
  };

  const colors = Colors[colorScheme ?? 'light'];

  if (!permission) {
    return (
      <ThemedView style={styles.container}>
        <ThemedText style={styles.loadingText}>Requesting camera permission...</ThemedText>
      </ThemedView>
    );
  }

  if (!permission.granted) {
    return (
      <ThemedView style={styles.container}>
        <ThemedText style={styles.permissionText}>Camera permission is required</ThemedText>
        <TouchableOpacity
          style={[styles.permissionButton, { backgroundColor: colors.tint }]}
          onPress={requestPermission}>
          <ThemedText style={styles.permissionButtonText}>Grant Permission</ThemedText>
        </TouchableOpacity>
      </ThemedView>
    );
  }

  return (
    <ThemedView style={styles.container}>
      <CameraView
        ref={cameraRef}
        style={styles.camera}
        facing={facing}
        mode="picture"
      />

      {/* Minimal connection indicator */}
      <View style={styles.connectionIndicator}>
        <View
          style={[
            styles.connectionDot,
            { backgroundColor: connected ? '#10b981' : '#ef4444' },
          ]}
        />
        <ThemedText style={styles.connectionText}>
          {connected ? 'Connected' : 'Connecting...'}
        </ThemedText>
      </View>

      <View style={styles.controls}>
        <TouchableOpacity
          style={[
            styles.button,
            styles.startButton,
            (!connected || sessionActive) && styles.buttonDisabled,
            { backgroundColor: connected && !sessionActive ? '#10b981' : '#6b7280' },
          ]}
          onPress={handleStartSession}
          disabled={!connected || sessionActive}>
          <ThemedText style={styles.buttonText}>Start</ThemedText>
        </TouchableOpacity>

        <TouchableOpacity
          style={[
            styles.button,
            styles.stopButton,
            !sessionActive && styles.buttonDisabled,
            { backgroundColor: sessionActive ? '#ef4444' : '#6b7280' },
          ]}
          onPress={handleStopSession}
          disabled={!sessionActive}>
          <ThemedText style={styles.buttonText}>Stop</ThemedText>
        </TouchableOpacity>
      </View>
    </ThemedView>
  );
}

const styles = StyleSheet.create({
  container: {
    flex: 1,
    backgroundColor: '#000',
  },
  camera: {
    flex: 1,
  },
  controls: {
    position: 'absolute',
    bottom: 0,
    left: 0,
    right: 0,
    flexDirection: 'row',
    justifyContent: 'center',
    alignItems: 'center',
    paddingBottom: Platform.OS === 'ios' ? 40 : 20,
    paddingHorizontal: 20,
    gap: 20,
  },
  button: {
    flex: 1,
    maxWidth: 150,
    paddingVertical: 16,
    paddingHorizontal: 32,
    borderRadius: 12,
    alignItems: 'center',
    justifyContent: 'center',
  },
  startButton: {
    backgroundColor: '#10b981',
  },
  stopButton: {
    backgroundColor: '#ef4444',
  },
  buttonDisabled: {
    opacity: 0.5,
  },
  buttonText: {
    color: '#fff',
    fontSize: 18,
    fontWeight: '600',
  },
  loadingText: {
    color: '#fff',
    fontSize: 16,
    textAlign: 'center',
    marginTop: 100,
  },
  permissionText: {
    color: '#fff',
    fontSize: 18,
    textAlign: 'center',
    marginBottom: 20,
    marginTop: 100,
  },
  permissionButton: {
    paddingVertical: 16,
    paddingHorizontal: 32,
    borderRadius: 12,
    alignSelf: 'center',
  },
  permissionButtonText: {
    color: '#fff',
    fontSize: 18,
    fontWeight: '600',
  },
  connectionIndicator: {
    position: 'absolute',
    top: Platform.OS === 'ios' ? 60 : 40,
    left: 20,
    flexDirection: 'row',
    alignItems: 'center',
    gap: 8,
    backgroundColor: 'rgba(0, 0, 0, 0.5)',
    paddingHorizontal: 12,
    paddingVertical: 6,
    borderRadius: 20,
  },
  connectionDot: {
    width: 8,
    height: 8,
    borderRadius: 4,
  },
  connectionText: {
    color: '#fff',
    fontSize: 12,
    fontWeight: '500',
  },
});

```


Made the following changes:

## Changes Made

1. Updated `.env` file ‚Äî changed from `localhost:5001` to `192.168.1.50:5001` (matches your Flask server IP)
2. Added connection logging ‚Äî console logs for connection events
3. Improved connection indicator ‚Äî always visible, shows "Connected" (green) or "Connecting..." (red)

## Next Steps

Restart the Expo app to pick up the new environment variable:

1. Stop the current Expo server (Ctrl+C in the terminal)
2. Restart it:
   ```bash
   cd A-eye
   npx expo start
   ```
3. Reload the app (press `r` in the Expo terminal or shake device and reload)

After restarting, check:
- The connection indicator in the top-left (should turn green when connected)
- The Expo/Metro console logs ‚Äî you should see:
  - `Connecting to backend at: http://192.168.1.50:5001`
  - `‚úÖ Socket.IO connected successfully` when connected
  - Any error messages if connection fails

The Flask logs show Socket.IO requests, so the connection should work once the app uses the correct IP address. The added logging will help diagnose any remaining issues.

If it still doesn't connect, check the console logs and share what you see.

```shellscript
   cd A-eye
   npx expo start
```

---

**User**

I am not getting a response from the model when talking

---

**Cursor**


Connection works, but the app isn't capturing or sending audio. The backend expects audio input, but the mobile app doesn't capture microphone audio. Adding microphone capture and sending it to the backend:



The backend expects audio from the computer's microphone (PyAudio), which isn't working. Adding mobile microphone capture and sending audio to the backend. Checking for a backend handler to receive client audio:



Adding a backend handler to receive audio from the mobile app, then updating the mobile app to capture and send audio:


```python
# -*- coding: utf-8 -*-
import asyncio
import base64
import io
import os
import sys
import traceback
import warnings
from threading import Thread

# Suppress macOS camera deprecation warnings
os.environ['PYTHONWARNINGS'] = 'ignore::DeprecationWarning'
warnings.filterwarnings('ignore', category=DeprecationWarning)

# Try to import optional dependencies with graceful error handling
try:
    # Suppress OpenCV macOS warnings
    import warnings
    with warnings.catch_warnings():
        warnings.filterwarnings("ignore", category=DeprecationWarning)
        import cv2
    CV2_AVAILABLE = True
except ImportError:
    CV2_AVAILABLE = False
    print("Warning: OpenCV (cv2) not available. Camera features will be disabled.")

try:
    from PIL import Image as PILImage
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False
    print("Warning: Pillow (PIL) not available. Image processing features will be disabled.")

try:
    import mss
    MSS_AVAILABLE = True
except ImportError:
    MSS_AVAILABLE = False
    print("Warning: mss not available. Screen capture features will be disabled.")

from dotenv import load_dotenv
from flask import Flask, render_template, request
from flask_socketio import SocketIO, emit

try:
    from google import genai
    GENAI_AVAILABLE = True
except ImportError:
    GENAI_AVAILABLE = False
    print("Error: google-genai not available. Please install: pip install google-genai")

# Load environment variables
load_dotenv()

if sys.version_info < (3, 11, 0):
    import taskgroup, exceptiongroup
    asyncio.TaskGroup = taskgroup.TaskGroup
    asyncio.ExceptionGroup = exceptiongroup.ExceptionGroup

# Try to import pyaudio, handle errors gracefully
try:
    import pyaudio
    PYAUDIO_AVAILABLE = True
except ImportError as e:
    PYAUDIO_AVAILABLE = False
    print(f"Warning: PyAudio not available: {e}")
    print("\nTo fix PyAudio on macOS, run:")
    print("  brew install portaudio")
    print("  pip uninstall pyaudio")
    print("  pip install pyaudio")

FORMAT = pyaudio.paInt16 if PYAUDIO_AVAILABLE else None
CHANNELS = 1
SEND_SAMPLE_RATE = 16000
RECEIVE_SAMPLE_RATE = 24000
CHUNK_SIZE = 1024

MODEL = "models/gemini-2.5-flash-native-audio-preview-09-2025"
DEFAULT_MODE = "camera"

# Initialize Google AI client with API key from .env
if not GENAI_AVAILABLE:
    raise ImportError("google-genai is required. Install with: pip install google-genai")

api_key = os.getenv("GOOGLE_API_KEY")
if not api_key:
    raise ValueError("GOOGLE_API_KEY not found in .env file")

client = genai.Client(api_key=api_key, http_options={"api_version": "v1beta"})
CONFIG = {"response_modalities": ["AUDIO"]}

# Initialize PyAudio only if available
pya = None
if PYAUDIO_AVAILABLE:
    try:
        pya = pyaudio.PyAudio()
    except Exception as e:
        print(f"Warning: Failed to initialize PyAudio: {e}")
        print("\nTo fix PyAudio on macOS, run:")
        print("  brew install portaudio")
        print("  pip uninstall pyaudio")
        print("  pip install pyaudio")
        PYAUDIO_AVAILABLE = False

app = Flask(__name__)
app.config['SECRET_KEY'] = 'your-secret-key-here'
socketio = SocketIO(app, cors_allowed_origins="*", async_mode='threading')

# Global session management
active_sessions = {}


class AudioLoop:
    def __init__(self, video_mode=DEFAULT_MODE, session_id=None):
        self.video_mode = video_mode
        self.session_id = session_id
        self.audio_in_queue = None
        self.out_queue = None
        self.session = None
        self.audio_stream = None
        self.running = False
        self.event_loop = None

    def _get_frame(self, cap):
        if not CV2_AVAILABLE or not PIL_AVAILABLE:
            return None
        ret, frame = cap.read()
        if not ret:
            return None
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        img = PILImage.fromarray(frame_rgb)
        img.thumbnail([1024, 1024])

        image_io = io.BytesIO()
        img.save(image_io, format="jpeg")
        image_io.seek(0)

        mime_type = "image/jpeg"
        image_bytes = image_io.read()
        return {"mime_type": mime_type, "data": base64.b64encode(image_bytes).decode()}

    async def get_frames(self):
        if not CV2_AVAILABLE:
            socketio.emit('info', {'message': 'OpenCV not available. Camera features disabled. Text-only mode available.'}, room=self.session_id)
            # Keep the task running but don't do anything
            while self.running:
                await asyncio.sleep(1.0)
            return
        
        try:
            cap = await asyncio.to_thread(cv2.VideoCapture, 0)
            if not cap.isOpened():
                socketio.emit('info', {
                    'message': 'Camera not available. Text-only mode available. To enable camera, grant permissions in System Settings ‚Üí Privacy & Security ‚Üí Camera.'
                }, room=self.session_id)
                # Keep the task running but don't do anything
                while self.running:
                    await asyncio.sleep(1.0)
                return
            
            socketio.emit('info', {'message': 'Camera access granted. Starting video stream...'}, room=self.session_id)
            
            while self.running:
                frame = await asyncio.to_thread(self._get_frame, cap)
                if frame is None:
                    break
                await asyncio.sleep(1.0)
                await self.out_queue.put(frame)
            cap.release()
        except Exception as e:
            socketio.emit('info', {
                'message': f'Camera not available: {str(e)}. Text-only mode available. To enable camera, check permissions in System Settings ‚Üí Privacy & Security ‚Üí Camera.'
            }, room=self.session_id)
            # Keep the task running but don't do anything
            while self.running:
                await asyncio.sleep(1.0)

    def _get_screen(self):
        if not MSS_AVAILABLE or not PIL_AVAILABLE:
            return None
        sct = mss.mss()
        monitor = sct.monitors[0]
        i = sct.grab(monitor)
        mime_type = "image/jpeg"
        image_bytes = mss.tools.to_png(i.rgb, i.size)
        img = PILImage.open(io.BytesIO(image_bytes))
        image_io = io.BytesIO()
        img.save(image_io, format="jpeg")
        image_io.seek(0)
        image_bytes = image_io.read()
        return {"mime_type": mime_type, "data": base64.b64encode(image_bytes).decode()}

    async def get_screen(self):
        if not MSS_AVAILABLE:
            socketio.emit('info', {'message': 'mss not available. Screen capture features disabled. Text-only mode available.'}, room=self.session_id)
            # Keep the task running but don't do anything
            while self.running:
                await asyncio.sleep(1.0)
            return
        
        try:
            while self.running:
                frame = await asyncio.to_thread(self._get_screen)
                if frame is None:
                    break
                await asyncio.sleep(1.0)
                await self.out_queue.put(frame)
        except Exception as e:
            socketio.emit('info', {
                'message': f'Screen capture not available: {str(e)}. Text-only mode available.'
            }, room=self.session_id)
            # Keep the task running but don't do anything
            while self.running:
                await asyncio.sleep(1.0)

    async def send_realtime(self):
        while self.running:
            msg = await self.out_queue.get()
            # Use the new API method instead of deprecated send()
            if "mime_type" in msg:
                if msg["mime_type"].startswith("image/"):
                    # Send as media (image/video)
                    from google.genai import types
                    blob = types.Blob(
                        data=base64.b64decode(msg["data"]),
                        mime_type=msg["mime_type"]
                    )
                    await self.session.send_realtime_input(media=blob)
                elif msg["mime_type"] == "audio/pcm":
                    # Send as audio
                    from google.genai import types
                    blob = types.Blob(
                        data=msg["data"],
                        mime_type="audio/pcm;rate=16000"
                    )
                    await self.session.send_realtime_input(audio=blob)
            else:
                # Fallback for other message types
                await self.session.send_realtime_input(text=str(msg))

    async def listen_audio(self):
        if not PYAUDIO_AVAILABLE or pya is None:
            socketio.emit('info', {'message': 'PyAudio not available. Audio input disabled. Text-only mode available.'}, room=self.session_id)
            # Keep the task running but don't do anything
            while self.running:
                await asyncio.sleep(1.0)
            return
        
        try:
            mic_info = pya.get_default_input_device_info()
            self.audio_stream = await asyncio.to_thread(
                pya.open,
                format=FORMAT,
                channels=CHANNELS,
                rate=SEND_SAMPLE_RATE,
                input=True,
                input_device_index=mic_info["index"],
                frames_per_buffer=CHUNK_SIZE,
            )
            socketio.emit('info', {'message': 'Microphone access granted. Starting audio input...'}, room=self.session_id)
            kwargs = {"exception_on_overflow": False} if __debug__ else {}
            while self.running:
                data = await asyncio.to_thread(self.audio_stream.read, CHUNK_SIZE, **kwargs)
                await self.out_queue.put({"data": data, "mime_type": "audio/pcm"})
        except Exception as e:
            socketio.emit('info', {
                'message': f'Microphone not available: {str(e)}. Text-only mode available. To enable audio, grant microphone permissions in System Settings ‚Üí Privacy & Security ‚Üí Microphone.'
            }, room=self.session_id)
            # Keep the task running but don't do anything
            while self.running:
                await asyncio.sleep(1.0)

    async def receive_audio(self):
        while self.running:
            turn = self.session.receive()
            async for response in turn:
                if data := response.data:
                    self.audio_in_queue.put_nowait(data)
                    # Emit audio data to client
                    socketio.emit('audio_data', {'data': base64.b64encode(data).decode()}, room=self.session_id)
                    continue
                if text := response.text:
                    socketio.emit('text_response', {'text': text}, room=self.session_id)

            while not self.audio_in_queue.empty():
                self.audio_in_queue.get_nowait()

    async def play_audio(self):
        if not PYAUDIO_AVAILABLE or pya is None:
            socketio.emit('info', {'message': 'PyAudio not available. Audio output disabled. Text responses will still work.'}, room=self.session_id)
            # Keep the task running but don't do anything
            while self.running:
                await asyncio.sleep(1.0)
            return
        
        try:
            stream = await asyncio.to_thread(
                pya.open,
                format=FORMAT,
                channels=CHANNELS,
                rate=RECEIVE_SAMPLE_RATE,
                output=True,
            )
            while self.running:
                bytestream = await self.audio_in_queue.get()
                await asyncio.to_thread(stream.write, bytestream)
        except Exception as e:
            socketio.emit('info', {
                'message': f'Audio output not available: {str(e)}. Text responses will still work.'
            }, room=self.session_id)
            # Keep the task running but don't do anything
            while self.running:
                await asyncio.sleep(1.0)

    async def send_text_message(self, text):
        if self.session:
            # Use send_client_content for turn-based text messages
            from google.genai import types
            await self.session.send_client_content(
                turns=types.Content(
                    role='user',
                    parts=[types.Part(text=text or ".")]
                ),
                turn_complete=True
            )

    async def run(self):
        try:
            self.event_loop = asyncio.get_event_loop()
            async with (
                client.aio.live.connect(model=MODEL, config=CONFIG) as session,
                asyncio.TaskGroup() as tg,
            ):
                self.session = session
                self.running = True

                self.audio_in_queue = asyncio.Queue()
                self.out_queue = asyncio.Queue(maxsize=5)

                tg.create_task(self.send_realtime())
                tg.create_task(self.listen_audio())
                
                if self.video_mode == "camera":
                    tg.create_task(self.get_frames())
                elif self.video_mode == "screen":
                    tg.create_task(self.get_screen())

                tg.create_task(self.receive_audio())
                tg.create_task(self.play_audio())

                # Keep running until stopped
                while self.running:
                    await asyncio.sleep(0.1)

        except asyncio.CancelledError:
            pass
        except Exception as e:
            if self.audio_stream:
                self.audio_stream.close()
            socketio.emit('error', {'message': str(e)}, room=self.session_id)
            traceback.print_exception(type(e), e, e.__traceback__)

    def stop(self):
        self.running = False


@app.route('/')
def index():
    return render_template('index.html')


@socketio.on('connect')
def handle_connect():
    emit('connected', {'status': 'Connected to server'})


@socketio.on('disconnect')
def handle_disconnect():
    session_id = request.sid
    if session_id in active_sessions:
        active_sessions[session_id].stop()
        del active_sessions[session_id]
    emit('disconnected', {'status': 'Disconnected from server'})


@socketio.on('start_session')
def handle_start_session(data):
    session_id = request.sid
    video_mode = data.get('mode', DEFAULT_MODE)
    
    if session_id in active_sessions:
        active_sessions[session_id].stop()
    
    audio_loop = AudioLoop(video_mode=video_mode, session_id=session_id)
    active_sessions[session_id] = audio_loop
    
    def run_async():
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        loop.run_until_complete(audio_loop.run())
    
    thread = Thread(target=run_async, daemon=True)
    thread.start()
    
    emit('session_started', {'status': 'Session started', 'mode': video_mode})


@socketio.on('stop_session')
def handle_stop_session():
    session_id = request.sid
    if session_id in active_sessions:
        active_sessions[session_id].stop()
        del active_sessions[session_id]
        emit('session_stopped', {'status': 'Session stopped'})


@socketio.on('send_message')
def handle_message(data):
    session_id = request.sid
    text = data.get('text', '')
    
    if session_id in active_sessions:
        audio_loop = active_sessions[session_id]
        
        if audio_loop.event_loop and audio_loop.event_loop.is_running():
            # Use the existing event loop
            asyncio.run_coroutine_threadsafe(
                audio_loop.send_text_message(text),
                audio_loop.event_loop
            )
        else:
            # Fallback: create new event loop
            def send_async():
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                loop.run_until_complete(audio_loop.send_text_message(text))
            
            thread = Thread(target=send_async, daemon=True)
            thread.start()
        
        emit('message_sent', {'status': 'Message sent'})


@socketio.on('send_audio')
def handle_audio(data):
    """Receive audio data from mobile client and forward to Gemini"""
    session_id = request.sid
    
    if session_id in active_sessions:
        audio_loop = active_sessions[session_id]
        audio_data = data.get('data', '')
        
        # Decode base64 audio data
        try:
            audio_bytes = base64.b64decode(audio_data)
            # Put audio in the queue for processing
            if audio_loop.event_loop and audio_loop.event_loop.is_running():
                asyncio.run_coroutine_threadsafe(
                    audio_loop.out_queue.put({
                        "data": audio_bytes,
                        "mime_type": "audio/pcm"
                    }),
                    audio_loop.event_loop
                )
        except Exception as e:
            print(f"Error processing audio from client: {e}")
            socketio.emit('error', {'message': f'Error processing audio: {str(e)}'}, room=session_id)


if __name__ == '__main__':
    # Use port 5001 by default to avoid conflict with macOS AirPlay Receiver on port 5000
    port = int(os.getenv('FLASK_PORT', 5001))
    print(f"\nüöÄ Starting Flask server on http://localhost:{port}")
    print(f"üìù Open your browser and navigate to: http://localhost:{port}\n")
    socketio.run(app, debug=True, host='0.0.0.0', port=port)


```


Adding audio recording and playback to the mobile app. Updating the SocketContext and the main screen:


```typescriptreact
import React, { createContext, useContext, useEffect, useState } from 'react';
import { io, Socket } from 'socket.io-client';
import Constants from 'expo-constants';

interface SocketContextType {
  socket: Socket | null;
  connected: boolean;
  sessionActive: boolean;
  startSession: () => void;
  stopSession: () => void;
  sendAudio: (audioData: string) => void;
}

const SocketContext = createContext<SocketContextType | undefined>(undefined);

// Get backend URL from environment variable or Constants, with fallback
const getBackendUrl = () => {
  // Try process.env first (works in Expo SDK 49+)
  if (process.env.EXPO_PUBLIC_BACKEND_URL) {
    return process.env.EXPO_PUBLIC_BACKEND_URL;
  }
  // Try Constants.expoConfig.extra (for app.json config)
  if (Constants.expoConfig?.extra?.backendUrl) {
    return Constants.expoConfig.extra.backendUrl;
  }
  // Default fallback
  return 'http://localhost:5001';
};

const BACKEND_URL = getBackendUrl();

export function SocketProvider({ children }: { children: React.ReactNode }) {
  const [socket, setSocket] = useState<Socket | null>(null);
  const [connected, setConnected] = useState(false);
  const [sessionActive, setSessionActive] = useState(false);

  useEffect(() => {
    // Log the backend URL being used
    console.log('Connecting to backend at:', BACKEND_URL);

    // Initialize socket connection
    const newSocket = io(BACKEND_URL, {
      transports: ['websocket', 'polling'],
      reconnection: true,
      reconnectionDelay: 1000,
      reconnectionAttempts: 5,
      timeout: 10000,
    });

    // Connection events
    newSocket.on('connect', () => {
      console.log('‚úÖ Socket.IO connected successfully');
      setConnected(true);
    });

    newSocket.on('disconnect', (reason) => {
      console.log('‚ùå Socket.IO disconnected:', reason);
      setConnected(false);
      setSessionActive(false);
    });

    newSocket.on('connect_error', (error) => {
      console.error('‚ùå Socket.IO connection error:', error.message);
      setConnected(false);
    });

    newSocket.on('reconnect', (attemptNumber) => {
      console.log('üîÑ Socket.IO reconnected after', attemptNumber, 'attempts');
      setConnected(true);
    });

    newSocket.on('reconnect_attempt', () => {
      console.log('üîÑ Attempting to reconnect...');
    });

    newSocket.on('reconnect_error', (error) => {
      console.error('‚ùå Socket.IO reconnect error:', error.message);
    });

    newSocket.on('reconnect_failed', () => {
      console.error('‚ùå Socket.IO reconnection failed');
    });

    // Session events
    newSocket.on('session_started', (data) => {
      console.log('‚úÖ Session started:', data);
      setSessionActive(true);
    });

    newSocket.on('session_stopped', (data) => {
      console.log('‚èπÔ∏è Session stopped:', data);
      setSessionActive(false);
    });

    // Handle other events with logging
    newSocket.on('connected', (data) => {
      console.log('üì° Server connected event:', data);
    });

    newSocket.on('text_response', (data) => {
      console.log('üìù Text response received');
    });

    newSocket.on('audio_data', (data) => {
      console.log('üîä Audio data received');
    });

    newSocket.on('info', (data) => {
      console.log('‚ÑπÔ∏è Info:', data.message);
    });

    newSocket.on('error', (data) => {
      console.error('‚ùå Error:', data.message);
    });

    setSocket(newSocket);

    return () => {
      console.log('üßπ Cleaning up socket connection');
      newSocket.close();
    };
  }, []);

  const startSession = () => {
    if (socket && connected) {
      socket.emit('start_session', { mode: 'camera' });
    }
  };

  const stopSession = () => {
    if (socket && sessionActive) {
      socket.emit('stop_session');
    }
  };

  return (
    <SocketContext.Provider
      value={{
        socket,
        connected,
        sessionActive,
        startSession,
        stopSession,
      }}>
      {children}
    </SocketContext.Provider>
  );
}

export function useSocket() {
  const context = useContext(SocketContext);
  if (context === undefined) {
    throw new Error('useSocket must be used within a SocketProvider');
  }
  return context;
}


```

```typescriptreact
import React, { createContext, useContext, useEffect, useState } from 'react';
import { io, Socket } from 'socket.io-client';
import Constants from 'expo-constants';

interface SocketContextType {
  socket: Socket | null;
  connected: boolean;
  sessionActive: boolean;
  startSession: () => void;
  stopSession: () => void;
  sendAudio: (audioData: string) => void;
}

const SocketContext = createContext<SocketContextType | undefined>(undefined);

// Get backend URL from environment variable or Constants, with fallback
const getBackendUrl = () => {
  // Try process.env first (works in Expo SDK 49+)
  if (process.env.EXPO_PUBLIC_BACKEND_URL) {
    return process.env.EXPO_PUBLIC_BACKEND_URL;
  }
  // Try Constants.expoConfig.extra (for app.json config)
  if (Constants.expoConfig?.extra?.backendUrl) {
    return Constants.expoConfig.extra.backendUrl;
  }
  // Default fallback
  return 'http://localhost:5001';
};

const BACKEND_URL = getBackendUrl();

export function SocketProvider({ children }: { children: React.ReactNode }) {
  const [socket, setSocket] = useState<Socket | null>(null);
  const [connected, setConnected] = useState(false);
  const [sessionActive, setSessionActive] = useState(false);

  useEffect(() => {
    // Log the backend URL being used
    console.log('Connecting to backend at:', BACKEND_URL);

    // Initialize socket connection
    const newSocket = io(BACKEND_URL, {
      transports: ['websocket', 'polling'],
      reconnection: true,
      reconnectionDelay: 1000,
      reconnectionAttempts: 5,
      timeout: 10000,
    });

    // Connection events
    newSocket.on('connect', () => {
      console.log('‚úÖ Socket.IO connected successfully');
      setConnected(true);
    });

    newSocket.on('disconnect', (reason) => {
      console.log('‚ùå Socket.IO disconnected:', reason);
      setConnected(false);
      setSessionActive(false);
    });

    newSocket.on('connect_error', (error) => {
      console.error('‚ùå Socket.IO connection error:', error.message);
      setConnected(false);
    });

    newSocket.on('reconnect', (attemptNumber) => {
      console.log('üîÑ Socket.IO reconnected after', attemptNumber, 'attempts');
      setConnected(true);
    });

    newSocket.on('reconnect_attempt', () => {
      console.log('üîÑ Attempting to reconnect...');
    });

    newSocket.on('reconnect_error', (error) => {
      console.error('‚ùå Socket.IO reconnect error:', error.message);
    });

    newSocket.on('reconnect_failed', () => {
      console.error('‚ùå Socket.IO reconnection failed');
    });

    // Session events
    newSocket.on('session_started', (data) => {
      console.log('‚úÖ Session started:', data);
      setSessionActive(true);
    });

    newSocket.on('session_stopped', (data) => {
      console.log('‚èπÔ∏è Session stopped:', data);
      setSessionActive(false);
    });

    // Handle other events with logging
    newSocket.on('connected', (data) => {
      console.log('üì° Server connected event:', data);
    });

    newSocket.on('text_response', (data) => {
      console.log('üìù Text response received');
    });

    newSocket.on('audio_data', (data) => {
      console.log('üîä Audio data received');
    });

    newSocket.on('info', (data) => {
      console.log('‚ÑπÔ∏è Info:', data.message);
    });

    newSocket.on('error', (data) => {
      console.error('‚ùå Error:', data.message);
    });

    setSocket(newSocket);

    return () => {
      console.log('üßπ Cleaning up socket connection');
      newSocket.close();
    };
  }, []);

  const startSession = () => {
    if (socket && connected) {
      socket.emit('start_session', { mode: 'camera' });
    }
  };

  const stopSession = () => {
    if (socket && sessionActive) {
      socket.emit('stop_session');
    }
  };

  const sendAudio = (audioData: string) => {
    if (socket && sessionActive && audioData) {
      socket.emit('send_audio', { data: audioData });
    }
  };

  return (
    <SocketContext.Provider
      value={{
        socket,
        connected,
        sessionActive,
        startSession,
        stopSession,
        sendAudio,
      }}>
      {children}
    </SocketContext.Provider>
  );
}

export function useSocket() {
  const context = useContext(SocketContext);
  if (context === undefined) {
    throw new Error('useSocket must be used within a SocketProvider');
  }
  return context;
}


```

```typescriptreact
import React, { useState, useRef, useEffect } from 'react';
import { StyleSheet, View, TouchableOpacity, Platform, Alert } from 'react-native';
import { CameraView, CameraType, useCameraPermissions } from 'expo-camera';
import { Audio } from 'expo-av';
import { ThemedText } from '@/components/themed-text';
import { ThemedView } from '@/components/themed-view';
import { useSocket } from '@/contexts/SocketContext';
import { useColorScheme } from '@/hooks/use-color-scheme';
import { Colors } from '@/constants/theme';

export default function HomeScreen() {
  const colorScheme = useColorScheme();
  const { connected, sessionActive, startSession, stopSession, sendAudio } = useSocket();
  const [facing, setFacing] = useState<CameraType>('back');
  const [permission, requestPermission] = useCameraPermissions();
  const [audioPermission, setAudioPermission] = useState<boolean>(false);
  const cameraRef = useRef<CameraView>(null);
  const recordingRef = useRef<Audio.Recording | null>(null);
  const soundRef = useRef<Audio.Sound | null>(null);
  const audioIntervalRef = useRef<NodeJS.Timeout | null>(null);

  useEffect(() => {
    // Request camera permission on mount
    if (!permission?.granted) {
      requestPermission();
    }

    // Request audio permission
    Audio.requestPermissionsAsync().then(({ status }) => {
      setAudioPermission(status === 'granted');
    });

    // Configure audio mode
    Audio.setAudioModeAsync({
      allowsRecordingIOS: true,
      playsInSilentModeIOS: true,
    });

    return () => {
      // Cleanup
      if (recordingRef.current) {
        recordingRef.current.stopAndUnloadAsync();
      }
      if (soundRef.current) {
        soundRef.current.unloadAsync();
      }
      if (audioIntervalRef.current) {
        clearInterval(audioIntervalRef.current);
      }
    };
  }, [permission, requestPermission]);

  const startAudioRecording = async () => {
    try {
      if (!audioPermission) {
        const { status } = await Audio.requestPermissionsAsync();
        if (status !== 'granted') {
          Alert.alert('Microphone Permission', 'Please grant microphone permission');
          return;
        }
        setAudioPermission(true);
      }

      const { recording } = await Audio.Recording.createAsync(
        Audio.RecordingOptionsPresets.HIGH_QUALITY,
        (status) => {
          // This callback is called periodically during recording
          if (status.isRecording && status.durationMillis) {
            // Send audio chunks periodically (every 100ms)
            // Note: This is a simplified approach. For production, you'd want to
            // capture raw PCM audio chunks directly
          }
        }
      );
      recordingRef.current = recording;

      // Start sending audio chunks every 100ms
      audioIntervalRef.current = setInterval(async () => {
        if (recordingRef.current && sessionActive) {
          try {
            const status = await recordingRef.current.getStatusAsync();
            if (status.isRecording) {
              // Get the recording URI and read audio data
              // Note: This is a simplified approach. For real-time audio streaming,
              // you'd need to use a lower-level audio API to get PCM chunks
            }
          } catch (error) {
            console.error('Error getting recording status:', error);
          }
        }
      }, 100);
    } catch (error) {
      console.error('Failed to start recording:', error);
    }
  };

  const stopAudioRecording = async () => {
    try {
      if (audioIntervalRef.current) {
        clearInterval(audioIntervalRef.current);
        audioIntervalRef.current = null;
      }

      if (recordingRef.current) {
        await recordingRef.current.stopAndUnloadAsync();
        recordingRef.current = null;
      }
    } catch (error) {
      console.error('Failed to stop recording:', error);
    }
  };

  const handleStartSession = async () => {
    if (!connected) {
      Alert.alert('Not Connected', 'Please wait for connection to server');
      return;
    }
    if (!permission?.granted) {
      Alert.alert('Camera Permission', 'Please grant camera permission to start session');
      requestPermission();
      return;
    }
    startSession();
    // Start audio recording when session starts
    setTimeout(() => {
      startAudioRecording();
    }, 500);
  };

  const handleStopSession = async () => {
    await stopAudioRecording();
    stopSession();
  };

  // Handle audio responses from backend
  useEffect(() => {
    if (!sessionActive) return;

    const socket = useSocket().socket;
    if (!socket) return;

    const handleAudioData = async (data: { data: string }) => {
      try {
        // Decode base64 audio data
        const audioBytes = atob(data.data);
        const audioArray = new Uint8Array(audioBytes.length);
        for (let i = 0; i < audioBytes.length; i++) {
          audioArray[i] = audioBytes.charCodeAt(i);
        }

        // Create a temporary audio file and play it
        // Note: This is a simplified approach. For production, you'd want to
        // use Web Audio API or a proper audio streaming solution
        const { sound } = await Audio.Sound.createAsync(
          { uri: `data:audio/pcm;base64,${data.data}` },
          { shouldPlay: true }
        );
        soundRef.current = sound;

        await sound.playAsync();
      } catch (error) {
        console.error('Error playing audio response:', error);
      }
    };

    socket.on('audio_data', handleAudioData);

    return () => {
      socket.off('audio_data', handleAudioData);
    };
  }, [sessionActive]);

  const colors = Colors[colorScheme ?? 'light'];

  if (!permission) {
    return (
      <ThemedView style={styles.container}>
        <ThemedText style={styles.loadingText}>Requesting camera permission...</ThemedText>
      </ThemedView>
    );
  }

  if (!permission.granted) {
    return (
      <ThemedView style={styles.container}>
        <ThemedText style={styles.permissionText}>Camera permission is required</ThemedText>
        <TouchableOpacity
          style={[styles.permissionButton, { backgroundColor: colors.tint }]}
          onPress={requestPermission}>
          <ThemedText style={styles.permissionButtonText}>Grant Permission</ThemedText>
        </TouchableOpacity>
      </ThemedView>
    );
  }

  return (
    <ThemedView style={styles.container}>
      <CameraView
        ref={cameraRef}
        style={styles.camera}
        facing={facing}
        mode="picture"
      />

      {/* Minimal connection indicator */}
      <View style={styles.connectionIndicator}>
        <View
          style={[
            styles.connectionDot,
            { backgroundColor: connected ? '#10b981' : '#ef4444' },
          ]}
        />
        <ThemedText style={styles.connectionText}>
          {connected ? 'Connected' : 'Connecting...'}
        </ThemedText>
      </View>

      <View style={styles.controls}>
        <TouchableOpacity
          style={[
            styles.button,
            styles.startButton,
            (!connected || sessionActive) && styles.buttonDisabled,
            { backgroundColor: connected && !sessionActive ? '#10b981' : '#6b7280' },
          ]}
          onPress={handleStartSession}
          disabled={!connected || sessionActive}>
          <ThemedText style={styles.buttonText}>Start</ThemedText>
        </TouchableOpacity>

        <TouchableOpacity
          style={[
            styles.button,
            styles.stopButton,
            !sessionActive && styles.buttonDisabled,
            { backgroundColor: sessionActive ? '#ef4444' : '#6b7280' },
          ]}
          onPress={handleStopSession}
          disabled={!sessionActive}>
          <ThemedText style={styles.buttonText}>Stop</ThemedText>
        </TouchableOpacity>
      </View>
    </ThemedView>
  );
}

const styles = StyleSheet.create({
  container: {
    flex: 1,
    backgroundColor: '#000',
  },
  camera: {
    flex: 1,
  },
  controls: {
    position: 'absolute',
    bottom: 0,
    left: 0,
    right: 0,
    flexDirection: 'row',
    justifyContent: 'center',
    alignItems: 'center',
    paddingBottom: Platform.OS === 'ios' ? 40 : 20,
    paddingHorizontal: 20,
    gap: 20,
  },
  button: {
    flex: 1,
    maxWidth: 150,
    paddingVertical: 16,
    paddingHorizontal: 32,
    borderRadius: 12,
    alignItems: 'center',
    justifyContent: 'center',
  },
  startButton: {
    backgroundColor: '#10b981',
  },
  stopButton: {
    backgroundColor: '#ef4444',
  },
  buttonDisabled: {
    opacity: 0.5,
  },
  buttonText: {
    color: '#fff',
    fontSize: 18,
    fontWeight: '600',
  },
  loadingText: {
    color: '#fff',
    fontSize: 16,
    textAlign: 'center',
    marginTop: 100,
  },
  permissionText: {
    color: '#fff',
    fontSize: 18,
    textAlign: 'center',
    marginBottom: 20,
    marginTop: 100,
  },
  permissionButton: {
    paddingVertical: 16,
    paddingHorizontal: 32,
    borderRadius: 12,
    alignSelf: 'center',
  },
  permissionButtonText: {
    color: '#fff',
    fontSize: 18,
    fontWeight: '600',
  },
  connectionIndicator: {
    position: 'absolute',
    top: Platform.OS === 'ios' ? 60 : 40,
    left: 20,
    flexDirection: 'row',
    alignItems: 'center',
    gap: 8,
    backgroundColor: 'rgba(0, 0, 0, 0.5)',
    paddingHorizontal: 12,
    paddingVertical: 6,
    borderRadius: 20,
  },
  connectionDot: {
    width: 8,
    height: 8,
    borderRadius: 4,
  },
  connectionText: {
    color: '#fff',
    fontSize: 12,
    fontWeight: '500',
  },
});

```


Fixing the React hooks issue and simplifying the audio implementation. Using a more direct approach:


```typescriptreact
import React, { useState, useRef, useEffect } from 'react';
import { StyleSheet, View, TouchableOpacity, Platform, Alert } from 'react-native';
import { CameraView, CameraType, useCameraPermissions } from 'expo-camera';
import { Audio } from 'expo-av';
import { ThemedText } from '@/components/themed-text';
import { ThemedView } from '@/components/themed-view';
import { useSocket } from '@/contexts/SocketContext';
import { useColorScheme } from '@/hooks/use-color-scheme';
import { Colors } from '@/constants/theme';

export default function HomeScreen() {
  const colorScheme = useColorScheme();
  const { connected, sessionActive, startSession, stopSession, sendAudio } = useSocket();
  const [facing, setFacing] = useState<CameraType>('back');
  const [permission, requestPermission] = useCameraPermissions();
  const [audioPermission, setAudioPermission] = useState<boolean>(false);
  const cameraRef = useRef<CameraView>(null);
  const recordingRef = useRef<Audio.Recording | null>(null);
  const soundRef = useRef<Audio.Sound | null>(null);
  const audioIntervalRef = useRef<NodeJS.Timeout | null>(null);

  useEffect(() => {
    // Request camera permission on mount
    if (!permission?.granted) {
      requestPermission();
    }

    // Request audio permission
    Audio.requestPermissionsAsync().then(({ status }) => {
      setAudioPermission(status === 'granted');
    });

    // Configure audio mode
    Audio.setAudioModeAsync({
      allowsRecordingIOS: true,
      playsInSilentModeIOS: true,
    });

    return () => {
      // Cleanup
      if (recordingRef.current) {
        recordingRef.current.stopAndUnloadAsync();
      }
      if (soundRef.current) {
        soundRef.current.unloadAsync();
      }
      if (audioIntervalRef.current) {
        clearInterval(audioIntervalRef.current);
      }
    };
  }, [permission, requestPermission]);

  const startAudioRecording = async () => {
    try {
      if (!audioPermission) {
        const { status } = await Audio.requestPermissionsAsync();
        if (status !== 'granted') {
          Alert.alert('Microphone Permission', 'Please grant microphone permission');
          return;
        }
        setAudioPermission(true);
      }

      const { recording } = await Audio.Recording.createAsync(
        Audio.RecordingOptionsPresets.HIGH_QUALITY,
        (status) => {
          // This callback is called periodically during recording
          if (status.isRecording && status.durationMillis) {
            // Send audio chunks periodically (every 100ms)
            // Note: This is a simplified approach. For production, you'd want to
            // capture raw PCM audio chunks directly
          }
        }
      );
      recordingRef.current = recording;

      // Start sending audio chunks every 100ms
      audioIntervalRef.current = setInterval(async () => {
        if (recordingRef.current && sessionActive) {
          try {
            const status = await recordingRef.current.getStatusAsync();
            if (status.isRecording) {
              // Get the recording URI and read audio data
              // Note: This is a simplified approach. For real-time audio streaming,
              // you'd need to use a lower-level audio API to get PCM chunks
            }
          } catch (error) {
            console.error('Error getting recording status:', error);
          }
        }
      }, 100);
    } catch (error) {
      console.error('Failed to start recording:', error);
    }
  };

  const stopAudioRecording = async () => {
    try {
      if (audioIntervalRef.current) {
        clearInterval(audioIntervalRef.current);
        audioIntervalRef.current = null;
      }

      if (recordingRef.current) {
        await recordingRef.current.stopAndUnloadAsync();
        recordingRef.current = null;
      }
    } catch (error) {
      console.error('Failed to stop recording:', error);
    }
  };

  const handleStartSession = async () => {
    if (!connected) {
      Alert.alert('Not Connected', 'Please wait for connection to server');
      return;
    }
    if (!permission?.granted) {
      Alert.alert('Camera Permission', 'Please grant camera permission to start session');
      requestPermission();
      return;
    }
    startSession();
    // Start audio recording when session starts
    setTimeout(() => {
      startAudioRecording();
    }, 500);
  };

  const handleStopSession = async () => {
    await stopAudioRecording();
    stopSession();
  };

  // Handle audio responses from backend - set up in SocketContext instead

  const colors = Colors[colorScheme ?? 'light'];

  if (!permission) {
    return (
      <ThemedView style={styles.container}>
        <ThemedText style={styles.loadingText}>Requesting camera permission...</ThemedText>
      </ThemedView>
    );
  }

  if (!permission.granted) {
    return (
      <ThemedView style={styles.container}>
        <ThemedText style={styles.permissionText}>Camera permission is required</ThemedText>
        <TouchableOpacity
          style={[styles.permissionButton, { backgroundColor: colors.tint }]}
          onPress={requestPermission}>
          <ThemedText style={styles.permissionButtonText}>Grant Permission</ThemedText>
        </TouchableOpacity>
      </ThemedView>
    );
  }

  return (
    <ThemedView style={styles.container}>
      <CameraView
        ref={cameraRef}
        style={styles.camera}
        facing={facing}
        mode="picture"
      />

      {/* Minimal connection indicator */}
      <View style={styles.connectionIndicator}>
        <View
          style={[
            styles.connectionDot,
            { backgroundColor: connected ? '#10b981' : '#ef4444' },
          ]}
        />
        <ThemedText style={styles.connectionText}>
          {connected ? 'Connected' : 'Connecting...'}
        </ThemedText>
      </View>

      <View style={styles.controls}>
        <TouchableOpacity
          style={[
            styles.button,
            styles.startButton,
            (!connected || sessionActive) && styles.buttonDisabled,
            { backgroundColor: connected && !sessionActive ? '#10b981' : '#6b7280' },
          ]}
          onPress={handleStartSession}
          disabled={!connected || sessionActive}>
          <ThemedText style={styles.buttonText}>Start</ThemedText>
        </TouchableOpacity>

        <TouchableOpacity
          style={[
            styles.button,
            styles.stopButton,
            !sessionActive && styles.buttonDisabled,
            { backgroundColor: sessionActive ? '#ef4444' : '#6b7280' },
          ]}
          onPress={handleStopSession}
          disabled={!sessionActive}>
          <ThemedText style={styles.buttonText}>Stop</ThemedText>
        </TouchableOpacity>
      </View>
    </ThemedView>
  );
}

const styles = StyleSheet.create({
  container: {
    flex: 1,
    backgroundColor: '#000',
  },
  camera: {
    flex: 1,
  },
  controls: {
    position: 'absolute',
    bottom: 0,
    left: 0,
    right: 0,
    flexDirection: 'row',
    justifyContent: 'center',
    alignItems: 'center',
    paddingBottom: Platform.OS === 'ios' ? 40 : 20,
    paddingHorizontal: 20,
    gap: 20,
  },
  button: {
    flex: 1,
    maxWidth: 150,
    paddingVertical: 16,
    paddingHorizontal: 32,
    borderRadius: 12,
    alignItems: 'center',
    justifyContent: 'center',
  },
  startButton: {
    backgroundColor: '#10b981',
  },
  stopButton: {
    backgroundColor: '#ef4444',
  },
  buttonDisabled: {
    opacity: 0.5,
  },
  buttonText: {
    color: '#fff',
    fontSize: 18,
    fontWeight: '600',
  },
  loadingText: {
    color: '#fff',
    fontSize: 16,
    textAlign: 'center',
    marginTop: 100,
  },
  permissionText: {
    color: '#fff',
    fontSize: 18,
    textAlign: 'center',
    marginBottom: 20,
    marginTop: 100,
  },
  permissionButton: {
    paddingVertical: 16,
    paddingHorizontal: 32,
    borderRadius: 12,
    alignSelf: 'center',
  },
  permissionButtonText: {
    color: '#fff',
    fontSize: 18,
    fontWeight: '600',
  },
  connectionIndicator: {
    position: 'absolute',
    top: Platform.OS === 'ios' ? 60 : 40,
    left: 20,
    flexDirection: 'row',
    alignItems: 'center',
    gap: 8,
    backgroundColor: 'rgba(0, 0, 0, 0.5)',
    paddingHorizontal: 12,
    paddingVertical: 6,
    borderRadius: 20,
  },
  connectionDot: {
    width: 8,
    height: 8,
    borderRadius: 4,
  },
  connectionText: {
    color: '#fff',
    fontSize: 12,
    fontWeight: '500',
  },
});

```

```typescriptreact
import React, { createContext, useContext, useEffect, useState } from 'react';
import { io, Socket } from 'socket.io-client';
import Constants from 'expo-constants';

interface SocketContextType {
  socket: Socket | null;
  connected: boolean;
  sessionActive: boolean;
  startSession: () => void;
  stopSession: () => void;
  sendAudio: (audioData: string) => void;
}

const SocketContext = createContext<SocketContextType | undefined>(undefined);

// Get backend URL from environment variable or Constants, with fallback
const getBackendUrl = () => {
  // Try process.env first (works in Expo SDK 49+)
  if (process.env.EXPO_PUBLIC_BACKEND_URL) {
    return process.env.EXPO_PUBLIC_BACKEND_URL;
  }
  // Try Constants.expoConfig.extra (for app.json config)
  if (Constants.expoConfig?.extra?.backendUrl) {
    return Constants.expoConfig.extra.backendUrl;
  }
  // Default fallback
  return 'http://localhost:5001';
};

const BACKEND_URL = getBackendUrl();

export function SocketProvider({ children }: { children: React.ReactNode }) {
  const [socket, setSocket] = useState<Socket | null>(null);
  const [connected, setConnected] = useState(false);
  const [sessionActive, setSessionActive] = useState(false);

  useEffect(() => {
    // Log the backend URL being used
    console.log('Connecting to backend at:', BACKEND_URL);

    // Initialize socket connection
    const newSocket = io(BACKEND_URL, {
      transports: ['websocket', 'polling'],
      reconnection: true,
      reconnectionDelay: 1000,
      reconnectionAttempts: 5,
      timeout: 10000,
    });

    // Connection events
    newSocket.on('connect', () => {
      console.log('‚úÖ Socket.IO connected successfully');
      setConnected(true);
    });

    newSocket.on('disconnect', (reason) => {
      console.log('‚ùå Socket.IO disconnected:', reason);
      setConnected(false);
      setSessionActive(false);
    });

    newSocket.on('connect_error', (error) => {
      console.error('‚ùå Socket.IO connection error:', error.message);
      setConnected(false);
    });

    newSocket.on('reconnect', (attemptNumber) => {
      console.log('üîÑ Socket.IO reconnected after', attemptNumber, 'attempts');
      setConnected(true);
    });

    newSocket.on('reconnect_attempt', () => {
      console.log('üîÑ Attempting to reconnect...');
    });

    newSocket.on('reconnect_error', (error) => {
      console.error('‚ùå Socket.IO reconnect error:', error.message);
    });

    newSocket.on('reconnect_failed', () => {
      console.error('‚ùå Socket.IO reconnection failed');
    });

    // Session events
    newSocket.on('session_started', (data) => {
      console.log('‚úÖ Session started:', data);
      setSessionActive(true);
    });

    newSocket.on('session_stopped', (data) => {
      console.log('‚èπÔ∏è Session stopped:', data);
      setSessionActive(false);
    });

    // Handle other events with logging
    newSocket.on('connected', (data) => {
      console.log('üì° Server connected event:', data);
    });

    newSocket.on('text_response', (data) => {
      console.log('üìù Text response received:', data.text);
    });

    newSocket.on('audio_data', async (data) => {
      console.log('üîä Audio data received');
      // Audio playback will be handled in the component
    });

    newSocket.on('info', (data) => {
      console.log('‚ÑπÔ∏è Info:', data.message);
    });

    newSocket.on('error', (data) => {
      console.error('‚ùå Error:', data.message);
    });

    setSocket(newSocket);

    return () => {
      console.log('üßπ Cleaning up socket connection');
      newSocket.close();
    };
  }, []);

  const startSession = () => {
    if (socket && connected) {
      socket.emit('start_session', { mode: 'camera' });
    }
  };

  const stopSession = () => {
    if (socket && sessionActive) {
      socket.emit('stop_session');
    }
  };

  const sendAudio = (audioData: string) => {
    if (socket && sessionActive && audioData) {
      socket.emit('send_audio', { data: audioData });
    }
  };

  return (
    <SocketContext.Provider
      value={{
        socket,
        connected,
        sessionActive,
        startSession,
        stopSession,
        sendAudio,
      }}>
      {children}
    </SocketContext.Provider>
  );
}

export function useSocket() {
  const context = useContext(SocketContext);
  if (context === undefined) {
    throw new Error('useSocket must be used within a SocketProvider');
  }
  return context;
}


```


Fixing the TypeScript error and simplifying the audio implementation. The current approach is too complex. Using a simpler solution:


```typescriptreact
import React, { useState, useRef, useEffect } from 'react';
import { StyleSheet, View, TouchableOpacity, Platform, Alert } from 'react-native';
import { CameraView, CameraType, useCameraPermissions } from 'expo-camera';
import { Audio } from 'expo-av';
import { ThemedText } from '@/components/themed-text';
import { ThemedView } from '@/components/themed-view';
import { useSocket } from '@/contexts/SocketContext';
import { useColorScheme } from '@/hooks/use-color-scheme';
import { Colors } from '@/constants/theme';

export default function HomeScreen() {
  const colorScheme = useColorScheme();
  const { connected, sessionActive, startSession, stopSession, sendAudio } = useSocket();
  const [facing, setFacing] = useState<CameraType>('back');
  const [permission, requestPermission] = useCameraPermissions();
  const [audioPermission, setAudioPermission] = useState<boolean>(false);
  const cameraRef = useRef<CameraView>(null);
  const recordingRef = useRef<Audio.Recording | null>(null);
  const soundRef = useRef<Audio.Sound | null>(null);
  const audioIntervalRef = useRef<ReturnType<typeof setInterval> | null>(null);

  useEffect(() => {
    // Request camera permission on mount
    if (!permission?.granted) {
      requestPermission();
    }

    // Request audio permission
    Audio.requestPermissionsAsync().then(({ status }) => {
      setAudioPermission(status === 'granted');
    });

    // Configure audio mode
    Audio.setAudioModeAsync({
      allowsRecordingIOS: true,
      playsInSilentModeIOS: true,
    });

    return () => {
      // Cleanup
      if (recordingRef.current) {
        recordingRef.current.stopAndUnloadAsync();
      }
      if (soundRef.current) {
        soundRef.current.unloadAsync();
      }
      if (audioIntervalRef.current) {
        clearInterval(audioIntervalRef.current);
      }
    };
  }, [permission, requestPermission]);

  const startAudioRecording = async () => {
    try {
      if (!audioPermission) {
        const { status } = await Audio.requestPermissionsAsync();
        if (status !== 'granted') {
          Alert.alert('Microphone Permission', 'Please grant microphone permission');
          return;
        }
        setAudioPermission(true);
      }

      const { recording } = await Audio.Recording.createAsync(
        Audio.RecordingOptionsPresets.HIGH_QUALITY,
        (status) => {
          // This callback is called periodically during recording
          if (status.isRecording && status.durationMillis) {
            // Send audio chunks periodically (every 100ms)
            // Note: This is a simplified approach. For production, you'd want to
            // capture raw PCM audio chunks directly
          }
        }
      );
      recordingRef.current = recording;

      // Start sending audio chunks every 100ms
      audioIntervalRef.current = setInterval(async () => {
        if (recordingRef.current && sessionActive) {
          try {
            const status = await recordingRef.current.getStatusAsync();
            if (status.isRecording) {
              // Get the recording URI and read audio data
              // Note: This is a simplified approach. For real-time audio streaming,
              // you'd need to use a lower-level audio API to get PCM chunks
            }
          } catch (error) {
            console.error('Error getting recording status:', error);
          }
        }
      }, 100);
    } catch (error) {
      console.error('Failed to start recording:', error);
    }
  };

  const stopAudioRecording = async () => {
    try {
      if (audioIntervalRef.current) {
        clearInterval(audioIntervalRef.current);
        audioIntervalRef.current = null;
      }

      if (recordingRef.current) {
        await recordingRef.current.stopAndUnloadAsync();
        recordingRef.current = null;
      }
    } catch (error) {
      console.error('Failed to stop recording:', error);
    }
  };

  const handleStartSession = async () => {
    if (!connected) {
      Alert.alert('Not Connected', 'Please wait for connection to server');
      return;
    }
    if (!permission?.granted) {
      Alert.alert('Camera Permission', 'Please grant camera permission to start session');
      requestPermission();
      return;
    }
    startSession();
    // Start audio recording when session starts
    setTimeout(() => {
      startAudioRecording();
    }, 500);
  };

  const handleStopSession = async () => {
    await stopAudioRecording();
    stopSession();
  };

  // Handle audio responses from backend - set up in SocketContext instead

  const colors = Colors[colorScheme ?? 'light'];

  if (!permission) {
    return (
      <ThemedView style={styles.container}>
        <ThemedText style={styles.loadingText}>Requesting camera permission...</ThemedText>
      </ThemedView>
    );
  }

  if (!permission.granted) {
    return (
      <ThemedView style={styles.container}>
        <ThemedText style={styles.permissionText}>Camera permission is required</ThemedText>
        <TouchableOpacity
          style={[styles.permissionButton, { backgroundColor: colors.tint }]}
          onPress={requestPermission}>
          <ThemedText style={styles.permissionButtonText}>Grant Permission</ThemedText>
        </TouchableOpacity>
      </ThemedView>
    );
  }

  return (
    <ThemedView style={styles.container}>
      <CameraView
        ref={cameraRef}
        style={styles.camera}
        facing={facing}
        mode="picture"
      />

      {/* Minimal connection indicator */}
      <View style={styles.connectionIndicator}>
        <View
          style={[
            styles.connectionDot,
            { backgroundColor: connected ? '#10b981' : '#ef4444' },
          ]}
        />
        <ThemedText style={styles.connectionText}>
          {connected ? 'Connected' : 'Connecting...'}
        </ThemedText>
      </View>

      <View style={styles.controls}>
        <TouchableOpacity
          style={[
            styles.button,
            styles.startButton,
            (!connected || sessionActive) && styles.buttonDisabled,
            { backgroundColor: connected && !sessionActive ? '#10b981' : '#6b7280' },
          ]}
          onPress={handleStartSession}
          disabled={!connected || sessionActive}>
          <ThemedText style={styles.buttonText}>Start</ThemedText>
        </TouchableOpacity>

        <TouchableOpacity
          style={[
            styles.button,
            styles.stopButton,
            !sessionActive && styles.buttonDisabled,
            { backgroundColor: sessionActive ? '#ef4444' : '#6b7280' },
          ]}
          onPress={handleStopSession}
          disabled={!sessionActive}>
          <ThemedText style={styles.buttonText}>Stop</ThemedText>
        </TouchableOpacity>
      </View>
    </ThemedView>
  );
}

const styles = StyleSheet.create({
  container: {
    flex: 1,
    backgroundColor: '#000',
  },
  camera: {
    flex: 1,
  },
  controls: {
    position: 'absolute',
    bottom: 0,
    left: 0,
    right: 0,
    flexDirection: 'row',
    justifyContent: 'center',
    alignItems: 'center',
    paddingBottom: Platform.OS === 'ios' ? 40 : 20,
    paddingHorizontal: 20,
    gap: 20,
  },
  button: {
    flex: 1,
    maxWidth: 150,
    paddingVertical: 16,
    paddingHorizontal: 32,
    borderRadius: 12,
    alignItems: 'center',
    justifyContent: 'center',
  },
  startButton: {
    backgroundColor: '#10b981',
  },
  stopButton: {
    backgroundColor: '#ef4444',
  },
  buttonDisabled: {
    opacity: 0.5,
  },
  buttonText: {
    color: '#fff',
    fontSize: 18,
    fontWeight: '600',
  },
  loadingText: {
    color: '#fff',
    fontSize: 16,
    textAlign: 'center',
    marginTop: 100,
  },
  permissionText: {
    color: '#fff',
    fontSize: 18,
    textAlign: 'center',
    marginBottom: 20,
    marginTop: 100,
  },
  permissionButton: {
    paddingVertical: 16,
    paddingHorizontal: 32,
    borderRadius: 12,
    alignSelf: 'center',
  },
  permissionButtonText: {
    color: '#fff',
    fontSize: 18,
    fontWeight: '600',
  },
  connectionIndicator: {
    position: 'absolute',
    top: Platform.OS === 'ios' ? 60 : 40,
    left: 20,
    flexDirection: 'row',
    alignItems: 'center',
    gap: 8,
    backgroundColor: 'rgba(0, 0, 0, 0.5)',
    paddingHorizontal: 12,
    paddingVertical: 6,
    borderRadius: 20,
  },
  connectionDot: {
    width: 8,
    height: 8,
    borderRadius: 4,
  },
  connectionText: {
    color: '#fff',
    fontSize: 12,
    fontWeight: '500',
  },
});

```


Simplifying the audio implementation. The current approach is too complex. Creating a simpler version that records and sends audio:


```typescriptreact
import React, { useState, useRef, useEffect } from 'react';
import { StyleSheet, View, TouchableOpacity, Platform, Alert } from 'react-native';
import { CameraView, CameraType, useCameraPermissions } from 'expo-camera';
import { Audio } from 'expo-av';
import { ThemedText } from '@/components/themed-text';
import { ThemedView } from '@/components/themed-view';
import { useSocket } from '@/contexts/SocketContext';
import { useColorScheme } from '@/hooks/use-color-scheme';
import { Colors } from '@/constants/theme';

export default function HomeScreen() {
  const colorScheme = useColorScheme();
  const { connected, sessionActive, startSession, stopSession, sendAudio } = useSocket();
  const [facing, setFacing] = useState<CameraType>('back');
  const [permission, requestPermission] = useCameraPermissions();
  const [audioPermission, setAudioPermission] = useState<boolean>(false);
  const cameraRef = useRef<CameraView>(null);
  const recordingRef = useRef<Audio.Recording | null>(null);
  const soundRef = useRef<Audio.Sound | null>(null);
  const audioIntervalRef = useRef<ReturnType<typeof setInterval> | null>(null);

  useEffect(() => {
    // Request camera permission on mount
    if (!permission?.granted) {
      requestPermission();
    }

    // Request audio permission
    Audio.requestPermissionsAsync().then(({ status }) => {
      setAudioPermission(status === 'granted');
    });

    // Configure audio mode
    Audio.setAudioModeAsync({
      allowsRecordingIOS: true,
      playsInSilentModeIOS: true,
    });

    return () => {
      // Cleanup
      if (recordingRef.current) {
        recordingRef.current.stopAndUnloadAsync();
      }
      if (soundRef.current) {
        soundRef.current.unloadAsync();
      }
      if (audioIntervalRef.current) {
        clearInterval(audioIntervalRef.current);
      }
    };
  }, [permission, requestPermission]);

  const startAudioRecording = async () => {
    try {
      if (!audioPermission) {
        const { status } = await Audio.requestPermissionsAsync();
        if (status !== 'granted') {
          console.warn('Microphone permission not granted');
          return;
        }
        setAudioPermission(true);
      }

      // Start recording with settings that match backend expectations
      // Backend expects: 16kHz, mono, 16-bit PCM
      const { recording } = await Audio.Recording.createAsync(
        {
          ...Audio.RecordingOptionsPresets.LOW_QUALITY,
          sampleRate: 16000,
          numberOfChannels: 1,
          bitRate: 128000,
          android: {
            extension: '.m4a',
            outputFormat: Audio.AndroidOutputFormat.MPEG_4,
            audioEncoder: Audio.AndroidAudioEncoder.AAC,
            sampleRate: 16000,
            numberOfChannels: 1,
          },
          ios: {
            extension: '.m4a',
            outputFormat: Audio.IOSOutputFormat.MPEG4AAC,
            audioQuality: Audio.IOSAudioQuality.LOW,
            sampleRate: 16000,
            numberOfChannels: 1,
            bitRate: 128000,
            linearPCMBitDepth: 16,
            linearPCMIsBigEndian: false,
            linearPCMIsFloat: false,
          },
        },
        undefined, // No status update callback for now
        1000 // Update interval
      );
      recordingRef.current = recording;
      console.log('‚úÖ Audio recording started');
    } catch (error) {
      console.error('Failed to start recording:', error);
    }
  };

  const stopAudioRecording = async () => {
    try {
      if (audioIntervalRef.current) {
        clearInterval(audioIntervalRef.current);
        audioIntervalRef.current = null;
      }

      if (recordingRef.current) {
        await recordingRef.current.stopAndUnloadAsync();
        recordingRef.current = null;
      }
    } catch (error) {
      console.error('Failed to stop recording:', error);
    }
  };

  const handleStartSession = async () => {
    if (!connected) {
      Alert.alert('Not Connected', 'Please wait for connection to server');
      return;
    }
    if (!permission?.granted) {
      Alert.alert('Camera Permission', 'Please grant camera permission to start session');
      requestPermission();
      return;
    }
    startSession();
    // Start audio recording when session starts
    setTimeout(() => {
      startAudioRecording();
    }, 500);
  };

  const handleStopSession = async () => {
    await stopAudioRecording();
    stopSession();
  };

  // Handle audio responses from backend - set up in SocketContext instead

  const colors = Colors[colorScheme ?? 'light'];

  if (!permission) {
    return (
      <ThemedView style={styles.container}>
        <ThemedText style={styles.loadingText}>Requesting camera permission...</ThemedText>
      </ThemedView>
    );
  }

  if (!permission.granted) {
    return (
      <ThemedView style={styles.container}>
        <ThemedText style={styles.permissionText}>Camera permission is required</ThemedText>
        <TouchableOpacity
          style={[styles.permissionButton, { backgroundColor: colors.tint }]}
          onPress={requestPermission}>
          <ThemedText style={styles.permissionButtonText}>Grant Permission</ThemedText>
        </TouchableOpacity>
      </ThemedView>
    );
  }

  return (
    <ThemedView style={styles.container}>
      <CameraView
        ref={cameraRef}
        style={styles.camera}
        facing={facing}
        mode="picture"
      />

      {/* Minimal connection indicator */}
      <View style={styles.connectionIndicator}>
        <View
          style={[
            styles.connectionDot,
            { backgroundColor: connected ? '#10b981' : '#ef4444' },
          ]}
        />
        <ThemedText style={styles.connectionText}>
          {connected ? 'Connected' : 'Connecting...'}
        </ThemedText>
      </View>

      <View style={styles.controls}>
        <TouchableOpacity
          style={[
            styles.button,
            styles.startButton,
            (!connected || sessionActive) && styles.buttonDisabled,
            { backgroundColor: connected && !sessionActive ? '#10b981' : '#6b7280' },
          ]}
          onPress={handleStartSession}
          disabled={!connected || sessionActive}>
          <ThemedText style={styles.buttonText}>Start</ThemedText>
        </TouchableOpacity>

        <TouchableOpacity
          style={[
            styles.button,
            styles.stopButton,
            !sessionActive && styles.buttonDisabled,
            { backgroundColor: sessionActive ? '#ef4444' : '#6b7280' },
          ]}
          onPress={handleStopSession}
          disabled={!sessionActive}>
          <ThemedText style={styles.buttonText}>Stop</ThemedText>
        </TouchableOpacity>
      </View>
    </ThemedView>
  );
}

const styles = StyleSheet.create({
  container: {
    flex: 1,
    backgroundColor: '#000',
  },
  camera: {
    flex: 1,
  },
  controls: {
    position: 'absolute',
    bottom: 0,
    left: 0,
    right: 0,
    flexDirection: 'row',
    justifyContent: 'center',
    alignItems: 'center',
    paddingBottom: Platform.OS === 'ios' ? 40 : 20,
    paddingHorizontal: 20,
    gap: 20,
  },
  button: {
    flex: 1,
    maxWidth: 150,
    paddingVertical: 16,
    paddingHorizontal: 32,
    borderRadius: 12,
    alignItems: 'center',
    justifyContent: 'center',
  },
  startButton: {
    backgroundColor: '#10b981',
  },
  stopButton: {
    backgroundColor: '#ef4444',
  },
  buttonDisabled: {
    opacity: 0.5,
  },
  buttonText: {
    color: '#fff',
    fontSize: 18,
    fontWeight: '600',
  },
  loadingText: {
    color: '#fff',
    fontSize: 16,
    textAlign: 'center',
    marginTop: 100,
  },
  permissionText: {
    color: '#fff',
    fontSize: 18,
    textAlign: 'center',
    marginBottom: 20,
    marginTop: 100,
  },
  permissionButton: {
    paddingVertical: 16,
    paddingHorizontal: 32,
    borderRadius: 12,
    alignSelf: 'center',
  },
  permissionButtonText: {
    color: '#fff',
    fontSize: 18,
    fontWeight: '600',
  },
  connectionIndicator: {
    position: 'absolute',
    top: Platform.OS === 'ios' ? 60 : 40,
    left: 20,
    flexDirection: 'row',
    alignItems: 'center',
    gap: 8,
    backgroundColor: 'rgba(0, 0, 0, 0.5)',
    paddingHorizontal: 12,
    paddingVertical: 6,
    borderRadius: 20,
  },
  connectionDot: {
    width: 8,
    height: 8,
    borderRadius: 4,
  },
  connectionText: {
    color: '#fff',
    fontSize: 12,
    fontWeight: '500',
  },
});

```


Fixing the audio recording configuration to match the expo-av API:


```typescriptreact
import React, { useState, useRef, useEffect } from 'react';
import { StyleSheet, View, TouchableOpacity, Platform, Alert } from 'react-native';
import { CameraView, CameraType, useCameraPermissions } from 'expo-camera';
import { Audio } from 'expo-av';
import { ThemedText } from '@/components/themed-text';
import { ThemedView } from '@/components/themed-view';
import { useSocket } from '@/contexts/SocketContext';
import { useColorScheme } from '@/hooks/use-color-scheme';
import { Colors } from '@/constants/theme';

export default function HomeScreen() {
  const colorScheme = useColorScheme();
  const { connected, sessionActive, startSession, stopSession, sendAudio } = useSocket();
  const [facing, setFacing] = useState<CameraType>('back');
  const [permission, requestPermission] = useCameraPermissions();
  const [audioPermission, setAudioPermission] = useState<boolean>(false);
  const cameraRef = useRef<CameraView>(null);
  const recordingRef = useRef<Audio.Recording | null>(null);
  const soundRef = useRef<Audio.Sound | null>(null);
  const audioIntervalRef = useRef<ReturnType<typeof setInterval> | null>(null);

  useEffect(() => {
    // Request camera permission on mount
    if (!permission?.granted) {
      requestPermission();
    }

    // Request audio permission
    Audio.requestPermissionsAsync().then(({ status }) => {
      setAudioPermission(status === 'granted');
    });

    // Configure audio mode
    Audio.setAudioModeAsync({
      allowsRecordingIOS: true,
      playsInSilentModeIOS: true,
    });

    return () => {
      // Cleanup
      if (recordingRef.current) {
        recordingRef.current.stopAndUnloadAsync();
      }
      if (soundRef.current) {
        soundRef.current.unloadAsync();
      }
      if (audioIntervalRef.current) {
        clearInterval(audioIntervalRef.current);
      }
    };
  }, [permission, requestPermission]);

  const startAudioRecording = async () => {
    try {
      if (!audioPermission) {
        const { status } = await Audio.requestPermissionsAsync();
        if (status !== 'granted') {
          console.warn('Microphone permission not granted');
          return;
        }
        setAudioPermission(true);
      }

      // Start recording - backend expects 16kHz, mono audio
      // Note: Real-time PCM streaming would require a different approach
      // For now, we'll record and the backend will handle it
      const { recording } = await Audio.Recording.createAsync(
        Audio.RecordingOptionsPresets.LOW_QUALITY
      );
      recordingRef.current = recording;
      console.log('‚úÖ Audio recording started');
    } catch (error) {
      console.error('Failed to start recording:', error);
    }
  };

  const stopAudioRecording = async () => {
    try {
      if (audioIntervalRef.current) {
        clearInterval(audioIntervalRef.current);
        audioIntervalRef.current = null;
      }

      if (recordingRef.current) {
        await recordingRef.current.stopAndUnloadAsync();
        recordingRef.current = null;
      }
    } catch (error) {
      console.error('Failed to stop recording:', error);
    }
  };

  const handleStartSession = async () => {
    if (!connected) {
      Alert.alert('Not Connected', 'Please wait for connection to server');
      return;
    }
    if (!permission?.granted) {
      Alert.alert('Camera Permission', 'Please grant camera permission to start session');
      requestPermission();
      return;
    }
    startSession();
    // Start audio recording when session starts
    setTimeout(() => {
      startAudioRecording();
    }, 500);
  };

  const handleStopSession = async () => {
    await stopAudioRecording();
    stopSession();
  };

  // Handle audio responses from backend - set up in SocketContext instead

  const colors = Colors[colorScheme ?? 'light'];

  if (!permission) {
    return (
      <ThemedView style={styles.container}>
        <ThemedText style={styles.loadingText}>Requesting camera permission...</ThemedText>
      </ThemedView>
    );
  }

  if (!permission.granted) {
    return (
      <ThemedView style={styles.container}>
        <ThemedText style={styles.permissionText}>Camera permission is required</ThemedText>
        <TouchableOpacity
          style={[styles.permissionButton, { backgroundColor: colors.tint }]}
          onPress={requestPermission}>
          <ThemedText style={styles.permissionButtonText}>Grant Permission</ThemedText>
        </TouchableOpacity>
      </ThemedView>
    );
  }

  return (
    <ThemedView style={styles.container}>
      <CameraView
        ref={cameraRef}
        style={styles.camera}
        facing={facing}
        mode="picture"
      />

      {/* Minimal connection indicator */}
      <View style={styles.connectionIndicator}>
        <View
          style={[
            styles.connectionDot,
            { backgroundColor: connected ? '#10b981' : '#ef4444' },
          ]}
        />
        <ThemedText style={styles.connectionText}>
          {connected ? 'Connected' : 'Connecting...'}
        </ThemedText>
      </View>

      <View style={styles.controls}>
        <TouchableOpacity
          style={[
            styles.button,
            styles.startButton,
            (!connected || sessionActive) && styles.buttonDisabled,
            { backgroundColor: connected && !sessionActive ? '#10b981' : '#6b7280' },
          ]}
          onPress={handleStartSession}
          disabled={!connected || sessionActive}>
          <ThemedText style={styles.buttonText}>Start</ThemedText>
        </TouchableOpacity>

        <TouchableOpacity
          style={[
            styles.button,
            styles.stopButton,
            !sessionActive && styles.buttonDisabled,
            { backgroundColor: sessionActive ? '#ef4444' : '#6b7280' },
          ]}
          onPress={handleStopSession}
          disabled={!sessionActive}>
          <ThemedText style={styles.buttonText}>Stop</ThemedText>
        </TouchableOpacity>
      </View>
    </ThemedView>
  );
}

const styles = StyleSheet.create({
  container: {
    flex: 1,
    backgroundColor: '#000',
  },
  camera: {
    flex: 1,
  },
  controls: {
    position: 'absolute',
    bottom: 0,
    left: 0,
    right: 0,
    flexDirection: 'row',
    justifyContent: 'center',
    alignItems: 'center',
    paddingBottom: Platform.OS === 'ios' ? 40 : 20,
    paddingHorizontal: 20,
    gap: 20,
  },
  button: {
    flex: 1,
    maxWidth: 150,
    paddingVertical: 16,
    paddingHorizontal: 32,
    borderRadius: 12,
    alignItems: 'center',
    justifyContent: 'center',
  },
  startButton: {
    backgroundColor: '#10b981',
  },
  stopButton: {
    backgroundColor: '#ef4444',
  },
  buttonDisabled: {
    opacity: 0.5,
  },
  buttonText: {
    color: '#fff',
    fontSize: 18,
    fontWeight: '600',
  },
  loadingText: {
    color: '#fff',
    fontSize: 16,
    textAlign: 'center',
    marginTop: 100,
  },
  permissionText: {
    color: '#fff',
    fontSize: 18,
    textAlign: 'center',
    marginBottom: 20,
    marginTop: 100,
  },
  permissionButton: {
    paddingVertical: 16,
    paddingHorizontal: 32,
    borderRadius: 12,
    alignSelf: 'center',
  },
  permissionButtonText: {
    color: '#fff',
    fontSize: 18,
    fontWeight: '600',
  },
  connectionIndicator: {
    position: 'absolute',
    top: Platform.OS === 'ios' ? 60 : 40,
    left: 20,
    flexDirection: 'row',
    alignItems: 'center',
    gap: 8,
    backgroundColor: 'rgba(0, 0, 0, 0.5)',
    paddingHorizontal: 12,
    paddingVertical: 6,
    borderRadius: 20,
  },
  connectionDot: {
    width: 8,
    height: 8,
    borderRadius: 4,
  },
  connectionText: {
    color: '#fff',
    fontSize: 12,
    fontWeight: '500',
  },
});

```


The backend's `listen_audio()` method uses PyAudio (not working), so no audio reaches Gemini. The mobile app should send audio. Since expo-av records to a file, adding a periodic send mechanism. Updating the backend to disable the computer's microphone when receiving audio from the mobile client:


```python
# -*- coding: utf-8 -*-
import asyncio
import base64
import io
import os
import sys
import traceback
import warnings
from threading import Thread

# Suppress macOS camera deprecation warnings
os.environ['PYTHONWARNINGS'] = 'ignore::DeprecationWarning'
warnings.filterwarnings('ignore', category=DeprecationWarning)

# Try to import optional dependencies with graceful error handling
try:
    # Suppress OpenCV macOS warnings
    import warnings
    with warnings.catch_warnings():
        warnings.filterwarnings("ignore", category=DeprecationWarning)
        import cv2
    CV2_AVAILABLE = True
except ImportError:
    CV2_AVAILABLE = False
    print("Warning: OpenCV (cv2) not available. Camera features will be disabled.")

try:
    from PIL import Image as PILImage
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False
    print("Warning: Pillow (PIL) not available. Image processing features will be disabled.")

try:
    import mss
    MSS_AVAILABLE = True
except ImportError:
    MSS_AVAILABLE = False
    print("Warning: mss not available. Screen capture features will be disabled.")

from dotenv import load_dotenv
from flask import Flask, render_template, request
from flask_socketio import SocketIO, emit

try:
    from google import genai
    GENAI_AVAILABLE = True
except ImportError:
    GENAI_AVAILABLE = False
    print("Error: google-genai not available. Please install: pip install google-genai")

# Load environment variables
load_dotenv()

if sys.version_info < (3, 11, 0):
    import taskgroup, exceptiongroup
    asyncio.TaskGroup = taskgroup.TaskGroup
    asyncio.ExceptionGroup = exceptiongroup.ExceptionGroup

# Try to import pyaudio, handle errors gracefully
try:
    import pyaudio
    PYAUDIO_AVAILABLE = True
except ImportError as e:
    PYAUDIO_AVAILABLE = False
    print(f"Warning: PyAudio not available: {e}")
    print("\nTo fix PyAudio on macOS, run:")
    print("  brew install portaudio")
    print("  pip uninstall pyaudio")
    print("  pip install pyaudio")

FORMAT = pyaudio.paInt16 if PYAUDIO_AVAILABLE else None
CHANNELS = 1
SEND_SAMPLE_RATE = 16000
RECEIVE_SAMPLE_RATE = 24000
CHUNK_SIZE = 1024

MODEL = "models/gemini-2.5-flash-native-audio-preview-09-2025"
DEFAULT_MODE = "camera"

# Initialize Google AI client with API key from .env
if not GENAI_AVAILABLE:
    raise ImportError("google-genai is required. Install with: pip install google-genai")

api_key = os.getenv("GOOGLE_API_KEY")
if not api_key:
    raise ValueError("GOOGLE_API_KEY not found in .env file")

client = genai.Client(api_key=api_key, http_options={"api_version": "v1beta"})
CONFIG = {"response_modalities": ["AUDIO"]}

# Initialize PyAudio only if available
pya = None
if PYAUDIO_AVAILABLE:
    try:
        pya = pyaudio.PyAudio()
    except Exception as e:
        print(f"Warning: Failed to initialize PyAudio: {e}")
        print("\nTo fix PyAudio on macOS, run:")
        print("  brew install portaudio")
        print("  pip uninstall pyaudio")
        print("  pip install pyaudio")
        PYAUDIO_AVAILABLE = False

app = Flask(__name__)
app.config['SECRET_KEY'] = 'your-secret-key-here'
socketio = SocketIO(app, cors_allowed_origins="*", async_mode='threading')

# Global session management
active_sessions = {}


class AudioLoop:
    def __init__(self, video_mode=DEFAULT_MODE, session_id=None):
        self.video_mode = video_mode
        self.session_id = session_id
        self.audio_in_queue = None
        self.out_queue = None
        self.session = None
        self.audio_stream = None
        self.running = False
        self.event_loop = None

    def _get_frame(self, cap):
        if not CV2_AVAILABLE or not PIL_AVAILABLE:
            return None
        ret, frame = cap.read()
        if not ret:
            return None
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        img = PILImage.fromarray(frame_rgb)
        img.thumbnail([1024, 1024])

        image_io = io.BytesIO()
        img.save(image_io, format="jpeg")
        image_io.seek(0)

        mime_type = "image/jpeg"
        image_bytes = image_io.read()
        return {"mime_type": mime_type, "data": base64.b64encode(image_bytes).decode()}

    async def get_frames(self):
        if not CV2_AVAILABLE:
            socketio.emit('info', {'message': 'OpenCV not available. Camera features disabled. Text-only mode available.'}, room=self.session_id)
            # Keep the task running but don't do anything
            while self.running:
                await asyncio.sleep(1.0)
            return
        
        try:
            cap = await asyncio.to_thread(cv2.VideoCapture, 0)
            if not cap.isOpened():
                socketio.emit('info', {
                    'message': 'Camera not available. Text-only mode available. To enable camera, grant permissions in System Settings ‚Üí Privacy & Security ‚Üí Camera.'
                }, room=self.session_id)
                # Keep the task running but don't do anything
                while self.running:
                    await asyncio.sleep(1.0)
                return
            
            socketio.emit('info', {'message': 'Camera access granted. Starting video stream...'}, room=self.session_id)
            
            while self.running:
                frame = await asyncio.to_thread(self._get_frame, cap)
                if frame is None:
                    break
                await asyncio.sleep(1.0)
                await self.out_queue.put(frame)
            cap.release()
        except Exception as e:
            socketio.emit('info', {
                'message': f'Camera not available: {str(e)}. Text-only mode available. To enable camera, check permissions in System Settings ‚Üí Privacy & Security ‚Üí Camera.'
            }, room=self.session_id)
            # Keep the task running but don't do anything
            while self.running:
                await asyncio.sleep(1.0)

    def _get_screen(self):
        if not MSS_AVAILABLE or not PIL_AVAILABLE:
            return None
        sct = mss.mss()
        monitor = sct.monitors[0]
        i = sct.grab(monitor)
        mime_type = "image/jpeg"
        image_bytes = mss.tools.to_png(i.rgb, i.size)
        img = PILImage.open(io.BytesIO(image_bytes))
        image_io = io.BytesIO()
        img.save(image_io, format="jpeg")
        image_io.seek(0)
        image_bytes = image_io.read()
        return {"mime_type": mime_type, "data": base64.b64encode(image_bytes).decode()}

    async def get_screen(self):
        if not MSS_AVAILABLE:
            socketio.emit('info', {'message': 'mss not available. Screen capture features disabled. Text-only mode available.'}, room=self.session_id)
            # Keep the task running but don't do anything
            while self.running:
                await asyncio.sleep(1.0)
            return
        
        try:
            while self.running:
                frame = await asyncio.to_thread(self._get_screen)
                if frame is None:
                    break
                await asyncio.sleep(1.0)
                await self.out_queue.put(frame)
        except Exception as e:
            socketio.emit('info', {
                'message': f'Screen capture not available: {str(e)}. Text-only mode available.'
            }, room=self.session_id)
            # Keep the task running but don't do anything
            while self.running:
                await asyncio.sleep(1.0)

    async def send_realtime(self):
        while self.running:
            msg = await self.out_queue.get()
            # Use the new API method instead of deprecated send()
            if "mime_type" in msg:
                if msg["mime_type"].startswith("image/"):
                    # Send as media (image/video)
                    from google.genai import types
                    blob = types.Blob(
                        data=base64.b64decode(msg["data"]),
                        mime_type=msg["mime_type"]
                    )
                    await self.session.send_realtime_input(media=blob)
                elif msg["mime_type"] == "audio/pcm":
                    # Send as audio
                    from google.genai import types
                    blob = types.Blob(
                        data=msg["data"],
                        mime_type="audio/pcm;rate=16000"
                    )
                    await self.session.send_realtime_input(audio=blob)
            else:
                # Fallback for other message types
                await self.session.send_realtime_input(text=str(msg))

    async def listen_audio(self):
        if not PYAUDIO_AVAILABLE or pya is None:
            socketio.emit('info', {'message': 'PyAudio not available. Audio input disabled. Text-only mode available.'}, room=self.session_id)
            # Keep the task running but don't do anything
            while self.running:
                await asyncio.sleep(1.0)
            return
        
        try:
            mic_info = pya.get_default_input_device_info()
            self.audio_stream = await asyncio.to_thread(
                pya.open,
                format=FORMAT,
                channels=CHANNELS,
                rate=SEND_SAMPLE_RATE,
                input=True,
                input_device_index=mic_info["index"],
                frames_per_buffer=CHUNK_SIZE,
            )
            socketio.emit('info', {'message': 'Microphone access granted. Starting audio input...'}, room=self.session_id)
            kwargs = {"exception_on_overflow": False} if __debug__ else {}
            while self.running:
                data = await asyncio.to_thread(self.audio_stream.read, CHUNK_SIZE, **kwargs)
                await self.out_queue.put({"data": data, "mime_type": "audio/pcm"})
        except Exception as e:
            socketio.emit('info', {
                'message': f'Microphone not available: {str(e)}. Text-only mode available. To enable audio, grant microphone permissions in System Settings ‚Üí Privacy & Security ‚Üí Microphone.'
            }, room=self.session_id)
            # Keep the task running but don't do anything
            while self.running:
                await asyncio.sleep(1.0)

    async def receive_audio(self):
        while self.running:
            turn = self.session.receive()
            async for response in turn:
                if data := response.data:
                    self.audio_in_queue.put_nowait(data)
                    # Emit audio data to client
                    socketio.emit('audio_data', {'data': base64.b64encode(data).decode()}, room=self.session_id)
                    continue
                if text := response.text:
                    socketio.emit('text_response', {'text': text}, room=self.session_id)

            while not self.audio_in_queue.empty():
                self.audio_in_queue.get_nowait()

    async def play_audio(self):
        if not PYAUDIO_AVAILABLE or pya is None:
            socketio.emit('info', {'message': 'PyAudio not available. Audio output disabled. Text responses will still work.'}, room=self.session_id)
            # Keep the task running but don't do anything
            while self.running:
                await asyncio.sleep(1.0)
            return
        
        try:
            stream = await asyncio.to_thread(
                pya.open,
                format=FORMAT,
                channels=CHANNELS,
                rate=RECEIVE_SAMPLE_RATE,
                output=True,
            )
            while self.running:
                bytestream = await self.audio_in_queue.get()
                await asyncio.to_thread(stream.write, bytestream)
        except Exception as e:
            socketio.emit('info', {
                'message': f'Audio output not available: {str(e)}. Text responses will still work.'
            }, room=self.session_id)
            # Keep the task running but don't do anything
            while self.running:
                await asyncio.sleep(1.0)

    async def send_text_message(self, text):
        if self.session:
            # Use send_client_content for turn-based text messages
            from google.genai import types
            await self.session.send_client_content(
                turns=types.Content(
                    role='user',
                    parts=[types.Part(text=text or ".")]
                ),
                turn_complete=True
            )

    async def run(self):
        try:
            self.event_loop = asyncio.get_event_loop()
            async with (
                client.aio.live.connect(model=MODEL, config=CONFIG) as session,
                asyncio.TaskGroup() as tg,
            ):
                self.session = session
                self.running = True

                self.audio_in_queue = asyncio.Queue()
                self.out_queue = asyncio.Queue(maxsize=5)

                tg.create_task(self.send_realtime())
                # Only use computer microphone if PyAudio is available
                # Otherwise, rely on mobile client sending audio via send_audio event
                if PYAUDIO_AVAILABLE and pya is not None:
                    tg.create_task(self.listen_audio())
                
                if self.video_mode == "camera":
                    tg.create_task(self.get_frames())
                elif self.video_mode == "screen":
                    tg.create_task(self.get_screen())

                tg.create_task(self.receive_audio())
                tg.create_task(self.play_audio())

                # Keep running until stopped
                while self.running:
                    await asyncio.sleep(0.1)

        except asyncio.CancelledError:
            pass
        except Exception as e:
            if self.audio_stream:
                self.audio_stream.close()
            socketio.emit('error', {'message': str(e)}, room=self.session_id)
            traceback.print_exception(type(e), e, e.__traceback__)

    def stop(self):
        self.running = False


@app.route('/')
def index():
    return render_template('index.html')


@socketio.on('connect')
def handle_connect():
    emit('connected', {'status': 'Connected to server'})


@socketio.on('disconnect')
def handle_disconnect():
    session_id = request.sid
    if session_id in active_sessions:
        active_sessions[session_id].stop()
        del active_sessions[session_id]
    emit('disconnected', {'status': 'Disconnected from server'})


@socketio.on('start_session')
def handle_start_session(data):
    session_id = request.sid
    video_mode = data.get('mode', DEFAULT_MODE)
    
    if session_id in active_sessions:
        active_sessions[session_id].stop()
    
    audio_loop = AudioLoop(video_mode=video_mode, session_id=session_id)
    active_sessions[session_id] = audio_loop
    
    def run_async():
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        loop.run_until_complete(audio_loop.run())
    
    thread = Thread(target=run_async, daemon=True)
    thread.start()
    
    emit('session_started', {'status': 'Session started', 'mode': video_mode})


@socketio.on('stop_session')
def handle_stop_session():
    session_id = request.sid
    if session_id in active_sessions:
        active_sessions[session_id].stop()
        del active_sessions[session_id]
        emit('session_stopped', {'status': 'Session stopped'})


@socketio.on('send_message')
def handle_message(data):
    session_id = request.sid
    text = data.get('text', '')
    
    if session_id in active_sessions:
        audio_loop = active_sessions[session_id]
        
        if audio_loop.event_loop and audio_loop.event_loop.is_running():
            # Use the existing event loop
            asyncio.run_coroutine_threadsafe(
                audio_loop.send_text_message(text),
                audio_loop.event_loop
            )
        else:
            # Fallback: create new event loop
            def send_async():
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                loop.run_until_complete(audio_loop.send_text_message(text))
            
            thread = Thread(target=send_async, daemon=True)
            thread.start()
        
        emit('message_sent', {'status': 'Message sent'})


@socketio.on('send_audio')
def handle_audio(data):
    """Receive audio data from mobile client and forward to Gemini"""
    session_id = request.sid
    
    if session_id in active_sessions:
        audio_loop = active_sessions[session_id]
        audio_data = data.get('data', '')
        
        # Decode base64 audio data
        try:
            audio_bytes = base64.b64decode(audio_data)
            # Put audio in the queue for processing
            if audio_loop.event_loop and audio_loop.event_loop.is_running():
                asyncio.run_coroutine_threadsafe(
                    audio_loop.out_queue.put({
                        "data": audio_bytes,
                        "mime_type": "audio/pcm"
                    }),
                    audio_loop.event_loop
                )
        except Exception as e:
            print(f"Error processing audio from client: {e}")
            socketio.emit('error', {'message': f'Error processing audio: {str(e)}'}, room=session_id)


if __name__ == '__main__':
    # Use port 5001 by default to avoid conflict with macOS AirPlay Receiver on port 5000
    port = int(os.getenv('FLASK_PORT', 5001))
    print(f"\nüöÄ Starting Flask server on http://localhost:{port}")
    print(f"üìù Open your browser and navigate to: http://localhost:{port}\n")
    socketio.run(app, debug=True, host='0.0.0.0', port=port)


```


## Summary of Changes

1. **Backend (`app.py`)**:
   - Added `send_audio` Socket.IO handler to receive audio from the mobile client
   - Modified session startup to only use computer microphone if PyAudio is available
   - Backend can now receive audio from the mobile app

2. **Mobile App**:
   - Added audio recording with `expo-av`
   - Recording starts automatically when session starts
   - Added microphone permission handling

## Current Status

The app records audio, but it isn't being sent to the backend in real time. `expo-av` records to a file, while the backend expects PCM audio chunks.

## Next Steps

For real-time voice interaction, you need to:
1. Send audio chunks periodically from the mobile app to the backend
2. Convert the recorded audio to PCM format (16kHz, mono, 16-bit)

For now, the backend is set up to receive audio. The mobile app records but doesn't send it yet.

**Quick test**: Restart both the Flask backend and the Expo app, then start a session. You should see:
- Audio recording starts on the mobile device
- Backend logs showing it's ready to receive audio
- The session should work, but audio won't be sent until we implement the chunk sending

Should I implement the real-time audio chunk sending, or would you prefer to test the current setup first?

---

**User**

I am still not getting any audio output.

---

**Cursor**


Audio is recorded but not sent to the backend, and responses aren't played. Implementing real-time audio streaming: sending audio chunks from the mobile app and playing responses.



Implementing audio playback for responses and setting up audio input streaming. Updating both the SocketContext and the main component:


```typescriptreact
import React, { createContext, useContext, useEffect, useState } from 'react';
import { io, Socket } from 'socket.io-client';
import Constants from 'expo-constants';

interface SocketContextType {
  socket: Socket | null;
  connected: boolean;
  sessionActive: boolean;
  startSession: () => void;
  stopSession: () => void;
  sendAudio: (audioData: string) => void;
  onAudioData?: (data: { data: string }) => void;
}

const SocketContext = createContext<SocketContextType | undefined>(undefined);

// Get backend URL from environment variable or Constants, with fallback
const getBackendUrl = () => {
  // Try process.env first (works in Expo SDK 49+)
  if (process.env.EXPO_PUBLIC_BACKEND_URL) {
    return process.env.EXPO_PUBLIC_BACKEND_URL;
  }
  // Try Constants.expoConfig.extra (for app.json config)
  if (Constants.expoConfig?.extra?.backendUrl) {
    return Constants.expoConfig.extra.backendUrl;
  }
  // Default fallback
  return 'http://localhost:5001';
};

const BACKEND_URL = getBackendUrl();

export function SocketProvider({ children }: { children: React.ReactNode }) {
  const [socket, setSocket] = useState<Socket | null>(null);
  const [connected, setConnected] = useState(false);
  const [sessionActive, setSessionActive] = useState(false);

  useEffect(() => {
    // Log the backend URL being used
    console.log('Connecting to backend at:', BACKEND_URL);

    // Initialize socket connection
    const newSocket = io(BACKEND_URL, {
      transports: ['websocket', 'polling'],
      reconnection: true,
      reconnectionDelay: 1000,
      reconnectionAttempts: 5,
      timeout: 10000,
    });

    // Connection events
    newSocket.on('connect', () => {
      console.log('‚úÖ Socket.IO connected successfully');
      setConnected(true);
    });

    newSocket.on('disconnect', (reason) => {
      console.log('‚ùå Socket.IO disconnected:', reason);
      setConnected(false);
      setSessionActive(false);
    });

    newSocket.on('connect_error', (error) => {
      console.error('‚ùå Socket.IO connection error:', error.message);
      setConnected(false);
    });

    newSocket.on('reconnect', (attemptNumber) => {
      console.log('üîÑ Socket.IO reconnected after', attemptNumber, 'attempts');
      setConnected(true);
    });

    newSocket.on('reconnect_attempt', () => {
      console.log('üîÑ Attempting to reconnect...');
    });

    newSocket.on('reconnect_error', (error) => {
      console.error('‚ùå Socket.IO reconnect error:', error.message);
    });

    newSocket.on('reconnect_failed', () => {
      console.error('‚ùå Socket.IO reconnection failed');
    });

    // Session events
    newSocket.on('session_started', (data) => {
      console.log('‚úÖ Session started:', data);
      setSessionActive(true);
    });

    newSocket.on('session_stopped', (data) => {
      console.log('‚èπÔ∏è Session stopped:', data);
      setSessionActive(false);
    });

    // Handle other events with logging
    newSocket.on('connected', (data) => {
      console.log('üì° Server connected event:', data);
    });

    newSocket.on('text_response', (data) => {
      console.log('üìù Text response received:', data.text);
    });

    newSocket.on('audio_data', async (data) => {
      console.log('üîä Audio data received');
      // Audio playback will be handled in the component
    });

    newSocket.on('info', (data) => {
      console.log('‚ÑπÔ∏è Info:', data.message);
    });

    newSocket.on('error', (data) => {
      console.error('‚ùå Error:', data.message);
    });

    setSocket(newSocket);

    return () => {
      console.log('üßπ Cleaning up socket connection');
      newSocket.close();
    };
  }, []);

  const startSession = () => {
    if (socket && connected) {
      socket.emit('start_session', { mode: 'camera' });
    }
  };

  const stopSession = () => {
    if (socket && sessionActive) {
      socket.emit('stop_session');
    }
  };

  const sendAudio = (audioData: string) => {
    if (socket && sessionActive && audioData) {
      socket.emit('send_audio', { data: audioData });
    }
  };

  return (
    <SocketContext.Provider
      value={{
        socket,
        connected,
        sessionActive,
        startSession,
        stopSession,
        sendAudio,
      }}>
      {children}
    </SocketContext.Provider>
  );
}

export function useSocket() {
  const context = useContext(SocketContext);
  if (context === undefined) {
    throw new Error('useSocket must be used within a SocketProvider');
  }
  return context;
}


```

```typescriptreact
import React, { createContext, useContext, useEffect, useState } from 'react';
import { io, Socket } from 'socket.io-client';
import Constants from 'expo-constants';

interface SocketContextType {
  socket: Socket | null;
  connected: boolean;
  sessionActive: boolean;
  startSession: () => void;
  stopSession: () => void;
  sendAudio: (audioData: string) => void;
  onAudioData?: (data: { data: string }) => void;
}

const SocketContext = createContext<SocketContextType | undefined>(undefined);

// Get backend URL from environment variable or Constants, with fallback
const getBackendUrl = () => {
  // Try process.env first (works in Expo SDK 49+)
  if (process.env.EXPO_PUBLIC_BACKEND_URL) {
    return process.env.EXPO_PUBLIC_BACKEND_URL;
  }
  // Try Constants.expoConfig.extra (for app.json config)
  if (Constants.expoConfig?.extra?.backendUrl) {
    return Constants.expoConfig.extra.backendUrl;
  }
  // Default fallback
  return 'http://localhost:5001';
};

const BACKEND_URL = getBackendUrl();

export function SocketProvider({ children }: { children: React.ReactNode }) {
  const [socket, setSocket] = useState<Socket | null>(null);
  const [connected, setConnected] = useState(false);
  const [sessionActive, setSessionActive] = useState(false);
  const [audioDataCallback, setAudioDataCallback] = useState<((data: { data: string }) => void) | undefined>();

  useEffect(() => {
    // Log the backend URL being used
    console.log('Connecting to backend at:', BACKEND_URL);

    // Initialize socket connection
    const newSocket = io(BACKEND_URL, {
      transports: ['websocket', 'polling'],
      reconnection: true,
      reconnectionDelay: 1000,
      reconnectionAttempts: 5,
      timeout: 10000,
    });

    // Connection events
    newSocket.on('connect', () => {
      console.log('‚úÖ Socket.IO connected successfully');
      setConnected(true);
    });

    newSocket.on('disconnect', (reason) => {
      console.log('‚ùå Socket.IO disconnected:', reason);
      setConnected(false);
      setSessionActive(false);
    });

    newSocket.on('connect_error', (error) => {
      console.error('‚ùå Socket.IO connection error:', error.message);
      setConnected(false);
    });

    newSocket.on('reconnect', (attemptNumber) => {
      console.log('üîÑ Socket.IO reconnected after', attemptNumber, 'attempts');
      setConnected(true);
    });

    newSocket.on('reconnect_attempt', () => {
      console.log('üîÑ Attempting to reconnect...');
    });

    newSocket.on('reconnect_error', (error) => {
      console.error('‚ùå Socket.IO reconnect error:', error.message);
    });

    newSocket.on('reconnect_failed', () => {
      console.error('‚ùå Socket.IO reconnection failed');
    });

    // Session events
    newSocket.on('session_started', (data) => {
      console.log('‚úÖ Session started:', data);
      setSessionActive(true);
    });

    newSocket.on('session_stopped', (data) => {
      console.log('‚èπÔ∏è Session stopped:', data);
      setSessionActive(false);
    });

    // Handle other events with logging
    newSocket.on('connected', (data) => {
      console.log('üì° Server connected event:', data);
    });

    newSocket.on('text_response', (data) => {
      console.log('üìù Text response received:', data.text);
    });

    newSocket.on('audio_data', async (data) => {
      console.log('üîä Audio data received');
      // Audio playback will be handled in the component
    });

    newSocket.on('info', (data) => {
      console.log('‚ÑπÔ∏è Info:', data.message);
    });

    newSocket.on('error', (data) => {
      console.error('‚ùå Error:', data.message);
    });

    setSocket(newSocket);

    return () => {
      console.log('üßπ Cleaning up socket connection');
      newSocket.close();
    };
  }, []);

  const startSession = () => {
    if (socket && connected) {
      socket.emit('start_session', { mode: 'camera' });
    }
  };

  const stopSession = () => {
    if (socket && sessionActive) {
      socket.emit('stop_session');
    }
  };

  const sendAudio = (audioData: string) => {
    if (socket && sessionActive && audioData) {
      socket.emit('send_audio', { data: audioData });
    }
  };

  return (
    <SocketContext.Provider
      value={{
        socket,
        connected,
        sessionActive,
        startSession,
        stopSession,
        sendAudio,
      }}>
      {children}
    </SocketContext.Provider>
  );
}

export function useSocket() {
  const context = useContext(SocketContext);
  if (context === undefined) {
    throw new Error('useSocket must be used within a SocketProvider');
  }
  return context;
}


```

```typescriptreact
import React, { createContext, useContext, useEffect, useState } from 'react';
import { io, Socket } from 'socket.io-client';
import Constants from 'expo-constants';

interface SocketContextType {
  socket: Socket | null;
  connected: boolean;
  sessionActive: boolean;
  startSession: () => void;
  stopSession: () => void;
  sendAudio: (audioData: string) => void;
  onAudioData?: (data: { data: string }) => void;
}

const SocketContext = createContext<SocketContextType | undefined>(undefined);

// Get backend URL from environment variable or Constants, with fallback
const getBackendUrl = () => {
  // Try process.env first (works in Expo SDK 49+)
  if (process.env.EXPO_PUBLIC_BACKEND_URL) {
    return process.env.EXPO_PUBLIC_BACKEND_URL;
  }
  // Try Constants.expoConfig.extra (for app.json config)
  if (Constants.expoConfig?.extra?.backendUrl) {
    return Constants.expoConfig.extra.backendUrl;
  }
  // Default fallback
  return 'http://localhost:5001';
};

const BACKEND_URL = getBackendUrl();

export function SocketProvider({ children }: { children: React.ReactNode }) {
  const [socket, setSocket] = useState<Socket | null>(null);
  const [connected, setConnected] = useState(false);
  const [sessionActive, setSessionActive] = useState(false);
  const [audioDataCallback, setAudioDataCallback] = useState<((data: { data: string }) => void) | undefined>();

  useEffect(() => {
    // Log the backend URL being used
    console.log('Connecting to backend at:', BACKEND_URL);

    // Initialize socket connection
    const newSocket = io(BACKEND_URL, {
      transports: ['websocket', 'polling'],
      reconnection: true,
      reconnectionDelay: 1000,
      reconnectionAttempts: 5,
      timeout: 10000,
    });

    // Connection events
    newSocket.on('connect', () => {
      console.log('‚úÖ Socket.IO connected successfully');
      setConnected(true);
    });

    newSocket.on('disconnect', (reason) => {
      console.log('‚ùå Socket.IO disconnected:', reason);
      setConnected(false);
      setSessionActive(false);
    });

    newSocket.on('connect_error', (error) => {
      console.error('‚ùå Socket.IO connection error:', error.message);
      setConnected(false);
    });

    newSocket.on('reconnect', (attemptNumber) => {
      console.log('üîÑ Socket.IO reconnected after', attemptNumber, 'attempts');
      setConnected(true);
    });

    newSocket.on('reconnect_attempt', () => {
      console.log('üîÑ Attempting to reconnect...');
    });

    newSocket.on('reconnect_error', (error) => {
      console.error('‚ùå Socket.IO reconnect error:', error.message);
    });

    newSocket.on('reconnect_failed', () => {
      console.error('‚ùå Socket.IO reconnection failed');
    });

    // Session events
    newSocket.on('session_started', (data) => {
      console.log('‚úÖ Session started:', data);
      setSessionActive(true);
    });

    newSocket.on('session_stopped', (data) => {
      console.log('‚èπÔ∏è Session stopped:', data);
      setSessionActive(false);
    });

    // Handle other events with logging
    newSocket.on('connected', (data) => {
      console.log('üì° Server connected event:', data);
    });

    newSocket.on('text_response', (data) => {
      console.log('üìù Text response received:', data.text);
    });

    newSocket.on('audio_data', async (data) => {
      console.log('üîä Audio data received');
      // Call the callback if set
      if (audioDataCallback) {
        audioDataCallback(data);
      }
    });

    newSocket.on('info', (data) => {
      console.log('‚ÑπÔ∏è Info:', data.message);
    });

    newSocket.on('error', (data) => {
      console.error('‚ùå Error:', data.message);
    });

    setSocket(newSocket);

    return () => {
      console.log('üßπ Cleaning up socket connection');
      newSocket.close();
    };
  }, []);

  const startSession = () => {
    if (socket && connected) {
      socket.emit('start_session', { mode: 'camera' });
    }
  };

  const stopSession = () => {
    if (socket && sessionActive) {
      socket.emit('stop_session');
    }
  };

  const sendAudio = (audioData: string) => {
    if (socket && sessionActive && audioData) {
      socket.emit('send_audio', { data: audioData });
    }
  };

  return (
    <SocketContext.Provider
      value={{
        socket,
        connected,
        sessionActive,
        startSession,
        stopSession,
        sendAudio,
      }}>
      {children}
    </SocketContext.Provider>
  );
}

export function useSocket() {
  const context = useContext(SocketContext);
  if (context === undefined) {
    throw new Error('useSocket must be used within a SocketProvider');
  }
  return context;
}


```

```typescriptreact
import React, { createContext, useContext, useEffect, useState } from 'react';
import { io, Socket } from 'socket.io-client';
import Constants from 'expo-constants';

interface SocketContextType {
  socket: Socket | null;
  connected: boolean;
  sessionActive: boolean;
  startSession: () => void;
  stopSession: () => void;
  sendAudio: (audioData: string) => void;
  onAudioData?: (data: { data: string }) => void;
}

const SocketContext = createContext<SocketContextType | undefined>(undefined);

// Get backend URL from environment variable or Constants, with fallback
const getBackendUrl = () => {
  // Try process.env first (works in Expo SDK 49+)
  if (process.env.EXPO_PUBLIC_BACKEND_URL) {
    return process.env.EXPO_PUBLIC_BACKEND_URL;
  }
  // Try Constants.expoConfig.extra (for app.json config)
  if (Constants.expoConfig?.extra?.backendUrl) {
    return Constants.expoConfig.extra.backendUrl;
  }
  // Default fallback
  return 'http://localhost:5001';
};

const BACKEND_URL = getBackendUrl();

export function SocketProvider({ children }: { children: React.ReactNode }) {
  const [socket, setSocket] = useState<Socket | null>(null);
  const [connected, setConnected] = useState(false);
  const [sessionActive, setSessionActive] = useState(false);
  const [audioDataCallback, setAudioDataCallback] = useState<((data: { data: string }) => void) | undefined>();

  useEffect(() => {
    // Log the backend URL being used
    console.log('Connecting to backend at:', BACKEND_URL);

    // Initialize socket connection
    const newSocket = io(BACKEND_URL, {
      transports: ['websocket', 'polling'],
      reconnection: true,
      reconnectionDelay: 1000,
      reconnectionAttempts: 5,
      timeout: 10000,
    });

    // Connection events
    newSocket.on('connect', () => {
      console.log('‚úÖ Socket.IO connected successfully');
      setConnected(true);
    });

    newSocket.on('disconnect', (reason) => {
      console.log('‚ùå Socket.IO disconnected:', reason);
      setConnected(false);
      setSessionActive(false);
    });

    newSocket.on('connect_error', (error) => {
      console.error('‚ùå Socket.IO connection error:', error.message);
      setConnected(false);
    });

    newSocket.on('reconnect', (attemptNumber) => {
      console.log('üîÑ Socket.IO reconnected after', attemptNumber, 'attempts');
      setConnected(true);
    });

    newSocket.on('reconnect_attempt', () => {
      console.log('üîÑ Attempting to reconnect...');
    });

    newSocket.on('reconnect_error', (error) => {
      console.error('‚ùå Socket.IO reconnect error:', error.message);
    });

    newSocket.on('reconnect_failed', () => {
      console.error('‚ùå Socket.IO reconnection failed');
    });

    // Session events
    newSocket.on('session_started', (data) => {
      console.log('‚úÖ Session started:', data);
      setSessionActive(true);
    });

    newSocket.on('session_stopped', (data) => {
      console.log('‚èπÔ∏è Session stopped:', data);
      setSessionActive(false);
    });

    // Handle other events with logging
    newSocket.on('connected', (data) => {
      console.log('üì° Server connected event:', data);
    });

    newSocket.on('text_response', (data) => {
      console.log('üìù Text response received:', data.text);
    });

    newSocket.on('audio_data', async (data) => {
      console.log('üîä Audio data received');
      // Call the callback if set
      if (audioDataCallback) {
        audioDataCallback(data);
      }
    });

    newSocket.on('info', (data) => {
      console.log('‚ÑπÔ∏è Info:', data.message);
    });

    newSocket.on('error', (data) => {
      console.error('‚ùå Error:', data.message);
    });

    setSocket(newSocket);

    return () => {
      console.log('üßπ Cleaning up socket connection');
      newSocket.close();
    };
  }, []);

  const startSession = () => {
    if (socket && connected) {
      socket.emit('start_session', { mode: 'camera' });
    }
  };

  const stopSession = () => {
    if (socket && sessionActive) {
      socket.emit('stop_session');
    }
  };

  const sendAudio = (audioData: string) => {
    if (socket && sessionActive && audioData) {
      socket.emit('send_audio', { data: audioData });
    }
  };

  const setOnAudioData = (callback: ((data: { data: string }) => void) | undefined) => {
    setAudioDataCallback(() => callback);
  };

  return (
    <SocketContext.Provider
      value={{
        socket,
        connected,
        sessionActive,
        startSession,
        stopSession,
        sendAudio,
        onAudioData: audioDataCallback,
      }}>
      {children}
    </SocketContext.Provider>
  );
}

export function useSocket() {
  const context = useContext(SocketContext);
  if (context === undefined) {
    throw new Error('useSocket must be used within a SocketProvider');
  }
  return context;
}


```

```typescriptreact
import React, { useState, useRef, useEffect } from 'react';
import { StyleSheet, View, TouchableOpacity, Platform, Alert } from 'react-native';
import { CameraView, CameraType, useCameraPermissions } from 'expo-camera';
import { Audio } from 'expo-av';
import { ThemedText } from '@/components/themed-text';
import { ThemedView } from '@/components/themed-view';
import { useSocket } from '@/contexts/SocketContext';
import { useColorScheme } from '@/hooks/use-color-scheme';
import { Colors } from '@/constants/theme';

export default function HomeScreen() {
  const colorScheme = useColorScheme();
  const { connected, sessionActive, startSession, stopSession, sendAudio, socket } = useSocket();
  const [facing, setFacing] = useState<CameraType>('back');
  const [permission, requestPermission] = useCameraPermissions();
  const [audioPermission, setAudioPermission] = useState<boolean>(false);
  const cameraRef = useRef<CameraView>(null);
  const recordingRef = useRef<Audio.Recording | null>(null);
  const soundRef = useRef<Audio.Sound | null>(null);
  const audioIntervalRef = useRef<ReturnType<typeof setInterval> | null>(null);

  useEffect(() => {
    // Request camera permission on mount
    if (!permission?.granted) {
      requestPermission();
    }

    // Request audio permission
    Audio.requestPermissionsAsync().then(({ status }) => {
      setAudioPermission(status === 'granted');
    });

    // Configure audio mode
    Audio.setAudioModeAsync({
      allowsRecordingIOS: true,
      playsInSilentModeIOS: true,
    });

    return () => {
      // Cleanup
      if (recordingRef.current) {
        recordingRef.current.stopAndUnloadAsync();
      }
      if (soundRef.current) {
        soundRef.current.unloadAsync();
      }
      if (audioIntervalRef.current) {
        clearInterval(audioIntervalRef.current);
      }
    };
  }, [permission, requestPermission]);

  const startAudioRecording = async () => {
    try {
      if (!audioPermission) {
        const { status } = await Audio.requestPermissionsAsync();
        if (status !== 'granted') {
          console.warn('Microphone permission not granted');
          return;
        }
        setAudioPermission(true);
      }

      // Start recording - backend expects 16kHz, mono audio
      // Note: Real-time PCM streaming would require a different approach
      // For now, we'll record and the backend will handle it
      const { recording } = await Audio.Recording.createAsync(
        Audio.RecordingOptionsPresets.LOW_QUALITY
      );
      recordingRef.current = recording;
      console.log('‚úÖ Audio recording started');
    } catch (error) {
      console.error('Failed to start recording:', error);
    }
  };

  const stopAudioRecording = async () => {
    try {
      if (audioIntervalRef.current) {
        clearInterval(audioIntervalRef.current);
        audioIntervalRef.current = null;
      }

      if (recordingRef.current) {
        await recordingRef.current.stopAndUnloadAsync();
        recordingRef.current = null;
      }
    } catch (error) {
      console.error('Failed to stop recording:', error);
    }
  };

  const handleStartSession = async () => {
    if (!connected) {
      Alert.alert('Not Connected', 'Please wait for connection to server');
      return;
    }
    if (!permission?.granted) {
      Alert.alert('Camera Permission', 'Please grant camera permission to start session');
      requestPermission();
      return;
    }
    startSession();
    // Start audio recording when session starts
    setTimeout(() => {
      startAudioRecording();
    }, 500);
  };

  const handleStopSession = async () => {
    await stopAudioRecording();
    stopSession();
  };

  // Handle audio responses from backend - set up in SocketContext instead

  const colors = Colors[colorScheme ?? 'light'];

  if (!permission) {
    return (
      <ThemedView style={styles.container}>
        <ThemedText style={styles.loadingText}>Requesting camera permission...</ThemedText>
      </ThemedView>
    );
  }

  if (!permission.granted) {
    return (
      <ThemedView style={styles.container}>
        <ThemedText style={styles.permissionText}>Camera permission is required</ThemedText>
        <TouchableOpacity
          style={[styles.permissionButton, { backgroundColor: colors.tint }]}
          onPress={requestPermission}>
          <ThemedText style={styles.permissionButtonText}>Grant Permission</ThemedText>
        </TouchableOpacity>
      </ThemedView>
    );
  }

  return (
    <ThemedView style={styles.container}>
      <CameraView
        ref={cameraRef}
        style={styles.camera}
        facing={facing}
        mode="picture"
      />

      {/* Minimal connection indicator */}
      <View style={styles.connectionIndicator}>
        <View
          style={[
            styles.connectionDot,
            { backgroundColor: connected ? '#10b981' : '#ef4444' },
          ]}
        />
        <ThemedText style={styles.connectionText}>
          {connected ? 'Connected' : 'Connecting...'}
        </ThemedText>
      </View>

      <View style={styles.controls}>
        <TouchableOpacity
          style={[
            styles.button,
            styles.startButton,
            (!connected || sessionActive) && styles.buttonDisabled,
            { backgroundColor: connected && !sessionActive ? '#10b981' : '#6b7280' },
          ]}
          onPress={handleStartSession}
          disabled={!connected || sessionActive}>
          <ThemedText style={styles.buttonText}>Start</ThemedText>
        </TouchableOpacity>

        <TouchableOpacity
          style={[
            styles.button,
            styles.stopButton,
            !sessionActive && styles.buttonDisabled,
            { backgroundColor: sessionActive ? '#ef4444' : '#6b7280' },
          ]}
          onPress={handleStopSession}
          disabled={!sessionActive}>
          <ThemedText style={styles.buttonText}>Stop</ThemedText>
        </TouchableOpacity>
      </View>
    </ThemedView>
  );
}

const styles = StyleSheet.create({
  container: {
    flex: 1,
    backgroundColor: '#000',
  },
  camera: {
    flex: 1,
  },
  controls: {
    position: 'absolute',
    bottom: 0,
    left: 0,
    right: 0,
    flexDirection: 'row',
    justifyContent: 'center',
    alignItems: 'center',
    paddingBottom: Platform.OS === 'ios' ? 40 : 20,
    paddingHorizontal: 20,
    gap: 20,
  },
  button: {
    flex: 1,
    maxWidth: 150,
    paddingVertical: 16,
    paddingHorizontal: 32,
    borderRadius: 12,
    alignItems: 'center',
    justifyContent: 'center',
  },
  startButton: {
    backgroundColor: '#10b981',
  },
  stopButton: {
    backgroundColor: '#ef4444',
  },
  buttonDisabled: {
    opacity: 0.5,
  },
  buttonText: {
    color: '#fff',
    fontSize: 18,
    fontWeight: '600',
  },
  loadingText: {
    color: '#fff',
    fontSize: 16,
    textAlign: 'center',
    marginTop: 100,
  },
  permissionText: {
    color: '#fff',
    fontSize: 18,
    textAlign: 'center',
    marginBottom: 20,
    marginTop: 100,
  },
  permissionButton: {
    paddingVertical: 16,
    paddingHorizontal: 32,
    borderRadius: 12,
    alignSelf: 'center',
  },
  permissionButtonText: {
    color: '#fff',
    fontSize: 18,
    fontWeight: '600',
  },
  connectionIndicator: {
    position: 'absolute',
    top: Platform.OS === 'ios' ? 60 : 40,
    left: 20,
    flexDirection: 'row',
    alignItems: 'center',
    gap: 8,
    backgroundColor: 'rgba(0, 0, 0, 0.5)',
    paddingHorizontal: 12,
    paddingVertical: 6,
    borderRadius: 20,
  },
  connectionDot: {
    width: 8,
    height: 8,
    borderRadius: 4,
  },
  connectionText: {
    color: '#fff',
    fontSize: 12,
    fontWeight: '500',
  },
});

```

```typescriptreact
import React, { useState, useRef, useEffect } from 'react';
import { StyleSheet, View, TouchableOpacity, Platform, Alert } from 'react-native';
import { CameraView, CameraType, useCameraPermissions } from 'expo-camera';
import { Audio } from 'expo-av';
import { ThemedText } from '@/components/themed-text';
import { ThemedView } from '@/components/themed-view';
import { useSocket } from '@/contexts/SocketContext';
import { useColorScheme } from '@/hooks/use-color-scheme';
import { Colors } from '@/constants/theme';

export default function HomeScreen() {
  const colorScheme = useColorScheme();
  const { connected, sessionActive, startSession, stopSession, sendAudio, socket } = useSocket();
  const [facing, setFacing] = useState<CameraType>('back');
  const [permission, requestPermission] = useCameraPermissions();
  const [audioPermission, setAudioPermission] = useState<boolean>(false);
  const cameraRef = useRef<CameraView>(null);
  const recordingRef = useRef<Audio.Recording | null>(null);
  const soundRef = useRef<Audio.Sound | null>(null);
  const audioIntervalRef = useRef<ReturnType<typeof setInterval> | null>(null);

  useEffect(() => {
    // Request camera permission on mount
    if (!permission?.granted) {
      requestPermission();
    }

    // Request audio permission
    Audio.requestPermissionsAsync().then(({ status }) => {
      setAudioPermission(status === 'granted');
    });

    // Configure audio mode
    Audio.setAudioModeAsync({
      allowsRecordingIOS: true,
      playsInSilentModeIOS: true,
    });

    return () => {
      // Cleanup
      if (recordingRef.current) {
        recordingRef.current.stopAndUnloadAsync();
      }
      if (soundRef.current) {
        soundRef.current.unloadAsync();
      }
      if (audioIntervalRef.current) {
        clearInterval(audioIntervalRef.current);
      }
    };
  }, [permission, requestPermission]);

  const startAudioRecording = async () => {
    try {
      if (!audioPermission) {
        const { status } = await Audio.requestPermissionsAsync();
        if (status !== 'granted') {
          console.warn('Microphone permission not granted');
          return;
        }
        setAudioPermission(true);
      }

      // Start recording - backend expects 16kHz, mono audio
      // Note: Real-time PCM streaming would require a different approach
      // For now, we'll record and the backend will handle it
      const { recording } = await Audio.Recording.createAsync(
        Audio.RecordingOptionsPresets.LOW_QUALITY
      );
      recordingRef.current = recording;
      console.log('‚úÖ Audio recording started');
    } catch (error) {
      console.error('Failed to start recording:', error);
    }
  };

  const stopAudioRecording = async () => {
    try {
      if (audioIntervalRef.current) {
        clearInterval(audioIntervalRef.current);
        audioIntervalRef.current = null;
      }

      if (recordingRef.current) {
        await recordingRef.current.stopAndUnloadAsync();
        recordingRef.current = null;
      }
    } catch (error) {
      console.error('Failed to stop recording:', error);
    }
  };

  const handleStartSession = async () => {
    if (!connected) {
      Alert.alert('Not Connected', 'Please wait for connection to server');
      return;
    }
    if (!permission?.granted) {
      Alert.alert('Camera Permission', 'Please grant camera permission to start session');
      requestPermission();
      return;
    }
    startSession();
    // Start audio recording when session starts
    setTimeout(() => {
      startAudioRecording();
    }, 500);
  };

  const handleStopSession = async () => {
    await stopAudioRecording();
    stopSession();
  };

  // Handle audio responses from backend
  useEffect(() => {
    if (!socket || !sessionActive) return;

    const handleAudioData = async (data: { data: string }) => {
      try {
        console.log('üîä Playing audio response');
        // Decode base64 audio data
        const audioBytes = atob(data.data);
        const audioArray = new Uint8Array(audioBytes.length);
        for (let i = 0; i < audioBytes.length; i++) {
          audioArray[i] = audioBytes.charCodeAt(i);
        }

        // Create a temporary file URI for the audio
        // Note: Backend sends PCM audio at 24kHz, mono
        // We need to create a playable format
        // For now, let's try to play it directly using expo-av
        
        // Stop any currently playing sound
        if (soundRef.current) {
          try {
            await soundRef.current.unloadAsync();
          } catch (e) {
            // Ignore errors
          }
        }

        // Convert PCM to a format expo-av can play
        // This is a simplified approach - in production you'd want proper PCM handling
        // For now, we'll create a WAV file from the PCM data
        const sampleRate = 24000; // Backend uses 24kHz for output
        const numChannels = 1;
        const bitsPerSample = 16;
        
        // Create WAV header
        const wavHeader = new ArrayBuffer(44);
        const view = new DataView(wavHeader);
        
        // RIFF header
        view.setUint8(0, 'R'.charCodeAt(0));
        view.setUint8(1, 'I'.charCodeAt(0));
        view.setUint8(2, 'F'.charCodeAt(0));
        view.setUint8(3, 'F'.charCodeAt(0));
        view.setUint32(4, 36 + audioArray.length, true);
        view.setUint8(8, 'W'.charCodeAt(0));
        view.setUint8(9, 'A'.charCodeAt(0));
        view.setUint8(10, 'V'.charCodeAt(0));
        view.setUint8(11, 'E'.charCodeAt(0));
        
        // fmt chunk
        view.setUint8(12, 'f'.charCodeAt(0));
        view.setUint8(13, 'm'.charCodeAt(0));
        view.setUint8(14, 't'.charCodeAt(0));
        view.setUint8(15, ' '.charCodeAt(0));
        view.setUint32(16, 16, true); // fmt chunk size
        view.setUint16(20, 1, true); // audio format (PCM)
        view.setUint16(22, numChannels, true);
        view.setUint32(24, sampleRate, true);
        view.setUint32(28, sampleRate * numChannels * bitsPerSample / 8, true); // byte rate
        view.setUint16(32, numChannels * bitsPerSample / 8, true); // block align
        view.setUint16(34, bitsPerSample, true);
        
        // data chunk
        view.setUint8(36, 'd'.charCodeAt(0));
        view.setUint8(37, 'a'.charCodeAt(0));
        view.setUint8(38, 't'.charCodeAt(0));
        view.setUint8(39, 'a'.charCodeAt(0));
        view.setUint32(40, audioArray.length, true);
        
        // Combine header and audio data
        const wavData = new Uint8Array(wavHeader.byteLength + audioArray.length);
        wavData.set(new Uint8Array(wavHeader), 0);
        wavData.set(audioArray, wavHeader.byteLength);
        
        // Convert to base64 data URI
        const base64 = btoa(String.fromCharCode(...wavData));
        const dataUri = `data:audio/wav;base64,${base64}`;
        
        // Create and play sound
        const { sound } = await Audio.Sound.createAsync(
          { uri: dataUri },
          { shouldPlay: true, volume: 1.0 }
        );
        soundRef.current = sound;
        
        // Clean up when done
        sound.setOnPlaybackStatusUpdate((status) => {
          if (status.didJustFinish) {
            sound.unloadAsync();
            soundRef.current = null;
          }
        });
      } catch (error) {
        console.error('Error playing audio response:', error);
      }
    };

    socket.on('audio_data', handleAudioData);

    return () => {
      socket.off('audio_data', handleAudioData);
    };
  }, [socket, sessionActive]);

  const colors = Colors[colorScheme ?? 'light'];

  if (!permission) {
    return (
      <ThemedView style={styles.container}>
        <ThemedText style={styles.loadingText}>Requesting camera permission...</ThemedText>
      </ThemedView>
    );
  }

  if (!permission.granted) {
    return (
      <ThemedView style={styles.container}>
        <ThemedText style={styles.permissionText}>Camera permission is required</ThemedText>
        <TouchableOpacity
          style={[styles.permissionButton, { backgroundColor: colors.tint }]}
          onPress={requestPermission}>
          <ThemedText style={styles.permissionButtonText}>Grant Permission</ThemedText>
        </TouchableOpacity>
      </ThemedView>
    );
  }

  return (
    <ThemedView style={styles.container}>
      <CameraView
        ref={cameraRef}
        style={styles.camera}
        facing={facing}
        mode="picture"
      />

      {/* Minimal connection indicator */}
      <View style={styles.connectionIndicator}>
        <View
          style={[
            styles.connectionDot,
            { backgroundColor: connected ? '#10b981' : '#ef4444' },
          ]}
        />
        <ThemedText style={styles.connectionText}>
          {connected ? 'Connected' : 'Connecting...'}
        </ThemedText>
      </View>

      <View style={styles.controls}>
        <TouchableOpacity
          style={[
            styles.button,
            styles.startButton,
            (!connected || sessionActive) && styles.buttonDisabled,
            { backgroundColor: connected && !sessionActive ? '#10b981' : '#6b7280' },
          ]}
          onPress={handleStartSession}
          disabled={!connected || sessionActive}>
          <ThemedText style={styles.buttonText}>Start</ThemedText>
        </TouchableOpacity>

        <TouchableOpacity
          style={[
            styles.button,
            styles.stopButton,
            !sessionActive && styles.buttonDisabled,
            { backgroundColor: sessionActive ? '#ef4444' : '#6b7280' },
          ]}
          onPress={handleStopSession}
          disabled={!sessionActive}>
          <ThemedText style={styles.buttonText}>Stop</ThemedText>
        </TouchableOpacity>
      </View>
    </ThemedView>
  );
}

const styles = StyleSheet.create({
  container: {
    flex: 1,
    backgroundColor: '#000',
  },
  camera: {
    flex: 1,
  },
  controls: {
    position: 'absolute',
    bottom: 0,
    left: 0,
    right: 0,
    flexDirection: 'row',
    justifyContent: 'center',
    alignItems: 'center',
    paddingBottom: Platform.OS === 'ios' ? 40 : 20,
    paddingHorizontal: 20,
    gap: 20,
  },
  button: {
    flex: 1,
    maxWidth: 150,
    paddingVertical: 16,
    paddingHorizontal: 32,
    borderRadius: 12,
    alignItems: 'center',
    justifyContent: 'center',
  },
  startButton: {
    backgroundColor: '#10b981',
  },
  stopButton: {
    backgroundColor: '#ef4444',
  },
  buttonDisabled: {
    opacity: 0.5,
  },
  buttonText: {
    color: '#fff',
    fontSize: 18,
    fontWeight: '600',
  },
  loadingText: {
    color: '#fff',
    fontSize: 16,
    textAlign: 'center',
    marginTop: 100,
  },
  permissionText: {
    color: '#fff',
    fontSize: 18,
    textAlign: 'center',
    marginBottom: 20,
    marginTop: 100,
  },
  permissionButton: {
    paddingVertical: 16,
    paddingHorizontal: 32,
    borderRadius: 12,
    alignSelf: 'center',
  },
  permissionButtonText: {
    color: '#fff',
    fontSize: 18,
    fontWeight: '600',
  },
  connectionIndicator: {
    position: 'absolute',
    top: Platform.OS === 'ios' ? 60 : 40,
    left: 20,
    flexDirection: 'row',
    alignItems: 'center',
    gap: 8,
    backgroundColor: 'rgba(0, 0, 0, 0.5)',
    paddingHorizontal: 12,
    paddingVertical: 6,
    borderRadius: 20,
  },
  connectionDot: {
    width: 8,
    height: 8,
    borderRadius: 4,
  },
  connectionText: {
    color: '#fff',
    fontSize: 12,
    fontWeight: '500',
  },
});

```


Fixing the TypeScript error:


```typescriptreact
import React, { useState, useRef, useEffect } from 'react';
import { StyleSheet, View, TouchableOpacity, Platform, Alert } from 'react-native';
import { CameraView, CameraType, useCameraPermissions } from 'expo-camera';
import { Audio } from 'expo-av';
import { ThemedText } from '@/components/themed-text';
import { ThemedView } from '@/components/themed-view';
import { useSocket } from '@/contexts/SocketContext';
import { useColorScheme } from '@/hooks/use-color-scheme';
import { Colors } from '@/constants/theme';

export default function HomeScreen() {
  const colorScheme = useColorScheme();
  const { connected, sessionActive, startSession, stopSession, sendAudio, socket } = useSocket();
  const [facing, setFacing] = useState<CameraType>('back');
  const [permission, requestPermission] = useCameraPermissions();
  const [audioPermission, setAudioPermission] = useState<boolean>(false);
  const cameraRef = useRef<CameraView>(null);
  const recordingRef = useRef<Audio.Recording | null>(null);
  const soundRef = useRef<Audio.Sound | null>(null);
  const audioIntervalRef = useRef<ReturnType<typeof setInterval> | null>(null);

  useEffect(() => {
    // Request camera permission on mount
    if (!permission?.granted) {
      requestPermission();
    }

    // Request audio permission
    Audio.requestPermissionsAsync().then(({ status }) => {
      setAudioPermission(status === 'granted');
    });

    // Configure audio mode
    Audio.setAudioModeAsync({
      allowsRecordingIOS: true,
      playsInSilentModeIOS: true,
    });

    return () => {
      // Cleanup
      if (recordingRef.current) {
        recordingRef.current.stopAndUnloadAsync();
      }
      if (soundRef.current) {
        soundRef.current.unloadAsync();
      }
      if (audioIntervalRef.current) {
        clearInterval(audioIntervalRef.current);
      }
    };
  }, [permission, requestPermission]);

  const startAudioRecording = async () => {
    try {
      if (!audioPermission) {
        const { status } = await Audio.requestPermissionsAsync();
        if (status !== 'granted') {
          console.warn('Microphone permission not granted');
          return;
        }
        setAudioPermission(true);
      }

      // Start recording - backend expects 16kHz, mono audio
      // Note: Real-time PCM streaming would require a different approach
      // For now, we'll record and the backend will handle it
      const { recording } = await Audio.Recording.createAsync(
        Audio.RecordingOptionsPresets.LOW_QUALITY
      );
      recordingRef.current = recording;
      console.log('‚úÖ Audio recording started');
    } catch (error) {
      console.error('Failed to start recording:', error);
    }
  };

  const stopAudioRecording = async () => {
    try {
      if (audioIntervalRef.current) {
        clearInterval(audioIntervalRef.current);
        audioIntervalRef.current = null;
      }

      if (recordingRef.current) {
        await recordingRef.current.stopAndUnloadAsync();
        recordingRef.current = null;
      }
    } catch (error) {
      console.error('Failed to stop recording:', error);
    }
  };

  const handleStartSession = async () => {
    if (!connected) {
      Alert.alert('Not Connected', 'Please wait for connection to server');
      return;
    }
    if (!permission?.granted) {
      Alert.alert('Camera Permission', 'Please grant camera permission to start session');
      requestPermission();
      return;
    }
    startSession();
    // Start audio recording when session starts
    setTimeout(() => {
      startAudioRecording();
    }, 500);
  };

  const handleStopSession = async () => {
    await stopAudioRecording();
    stopSession();
  };

  // Handle audio responses from backend
  useEffect(() => {
    if (!socket || !sessionActive) return;

    const handleAudioData = async (data: { data: string }) => {
      try {
        console.log('üîä Playing audio response');
        // Decode base64 audio data
        const audioBytes = atob(data.data);
        const audioArray = new Uint8Array(audioBytes.length);
        for (let i = 0; i < audioBytes.length; i++) {
          audioArray[i] = audioBytes.charCodeAt(i);
        }

        // Create a temporary file URI for the audio
        // Note: Backend sends PCM audio at 24kHz, mono
        // We need to create a playable format
        // For now, let's try to play it directly using expo-av
        
        // Stop any currently playing sound
        if (soundRef.current) {
          try {
            await soundRef.current.unloadAsync();
          } catch (e) {
            // Ignore errors
          }
        }

        // Convert PCM to a format expo-av can play
        // This is a simplified approach - in production you'd want proper PCM handling
        // For now, we'll create a WAV file from the PCM data
        const sampleRate = 24000; // Backend uses 24kHz for output
        const numChannels = 1;
        const bitsPerSample = 16;
        
        // Create WAV header
        const wavHeader = new ArrayBuffer(44);
        const view = new DataView(wavHeader);
        
        // RIFF header
        view.setUint8(0, 'R'.charCodeAt(0));
        view.setUint8(1, 'I'.charCodeAt(0));
        view.setUint8(2, 'F'.charCodeAt(0));
        view.setUint8(3, 'F'.charCodeAt(0));
        view.setUint32(4, 36 + audioArray.length, true);
        view.setUint8(8, 'W'.charCodeAt(0));
        view.setUint8(9, 'A'.charCodeAt(0));
        view.setUint8(10, 'V'.charCodeAt(0));
        view.setUint8(11, 'E'.charCodeAt(0));
        
        // fmt chunk
        view.setUint8(12, 'f'.charCodeAt(0));
        view.setUint8(13, 'm'.charCodeAt(0));
        view.setUint8(14, 't'.charCodeAt(0));
        view.setUint8(15, ' '.charCodeAt(0));
        view.setUint32(16, 16, true); // fmt chunk size
        view.setUint16(20, 1, true); // audio format (PCM)
        view.setUint16(22, numChannels, true);
        view.setUint32(24, sampleRate, true);
        view.setUint32(28, sampleRate * numChannels * bitsPerSample / 8, true); // byte rate
        view.setUint16(32, numChannels * bitsPerSample / 8, true); // block align
        view.setUint16(34, bitsPerSample, true);
        
        // data chunk
        view.setUint8(36, 'd'.charCodeAt(0));
        view.setUint8(37, 'a'.charCodeAt(0));
        view.setUint8(38, 't'.charCodeAt(0));
        view.setUint8(39, 'a'.charCodeAt(0));
        view.setUint32(40, audioArray.length, true);
        
        // Combine header and audio data
        const wavData = new Uint8Array(wavHeader.byteLength + audioArray.length);
        wavData.set(new Uint8Array(wavHeader), 0);
        wavData.set(audioArray, wavHeader.byteLength);
        
        // Convert to base64 data URI
        const base64 = btoa(String.fromCharCode(...wavData));
        const dataUri = `data:audio/wav;base64,${base64}`;
        
        // Create and play sound
        const { sound } = await Audio.Sound.createAsync(
          { uri: dataUri },
          { shouldPlay: true, volume: 1.0 }
        );
        soundRef.current = sound;
        
        // Clean up when done
        sound.setOnPlaybackStatusUpdate((status) => {
          if (!status.isLoaded) return;
          if (status.didJustFinish) {
            sound.unloadAsync();
            soundRef.current = null;
          }
        });
      } catch (error) {
        console.error('Error playing audio response:', error);
      }
    };

    socket.on('audio_data', handleAudioData);

    return () => {
      socket.off('audio_data', handleAudioData);
    };
  }, [socket, sessionActive]);

  const colors = Colors[colorScheme ?? 'light'];

  if (!permission) {
    return (
      <ThemedView style={styles.container}>
        <ThemedText style={styles.loadingText}>Requesting camera permission...</ThemedText>
      </ThemedView>
    );
  }

  if (!permission.granted) {
    return (
      <ThemedView style={styles.container}>
        <ThemedText style={styles.permissionText}>Camera permission is required</ThemedText>
        <TouchableOpacity
          style={[styles.permissionButton, { backgroundColor: colors.tint }]}
          onPress={requestPermission}>
          <ThemedText style={styles.permissionButtonText}>Grant Permission</ThemedText>
        </TouchableOpacity>
      </ThemedView>
    );
  }

  return (
    <ThemedView style={styles.container}>
      <CameraView
        ref={cameraRef}
        style={styles.camera}
        facing={facing}
        mode="picture"
      />

      {/* Minimal connection indicator */}
      <View style={styles.connectionIndicator}>
        <View
          style={[
            styles.connectionDot,
            { backgroundColor: connected ? '#10b981' : '#ef4444' },
          ]}
        />
        <ThemedText style={styles.connectionText}>
          {connected ? 'Connected' : 'Connecting...'}
        </ThemedText>
      </View>

      <View style={styles.controls}>
        <TouchableOpacity
          style={[
            styles.button,
            styles.startButton,
            (!connected || sessionActive) && styles.buttonDisabled,
            { backgroundColor: connected && !sessionActive ? '#10b981' : '#6b7280' },
          ]}
          onPress={handleStartSession}
          disabled={!connected || sessionActive}>
          <ThemedText style={styles.buttonText}>Start</ThemedText>
        </TouchableOpacity>

        <TouchableOpacity
          style={[
            styles.button,
            styles.stopButton,
            !sessionActive && styles.buttonDisabled,
            { backgroundColor: sessionActive ? '#ef4444' : '#6b7280' },
          ]}
          onPress={handleStopSession}
          disabled={!sessionActive}>
          <ThemedText style={styles.buttonText}>Stop</ThemedText>
        </TouchableOpacity>
      </View>
    </ThemedView>
  );
}

const styles = StyleSheet.create({
  container: {
    flex: 1,
    backgroundColor: '#000',
  },
  camera: {
    flex: 1,
  },
  controls: {
    position: 'absolute',
    bottom: 0,
    left: 0,
    right: 0,
    flexDirection: 'row',
    justifyContent: 'center',
    alignItems: 'center',
    paddingBottom: Platform.OS === 'ios' ? 40 : 20,
    paddingHorizontal: 20,
    gap: 20,
  },
  button: {
    flex: 1,
    maxWidth: 150,
    paddingVertical: 16,
    paddingHorizontal: 32,
    borderRadius: 12,
    alignItems: 'center',
    justifyContent: 'center',
  },
  startButton: {
    backgroundColor: '#10b981',
  },
  stopButton: {
    backgroundColor: '#ef4444',
  },
  buttonDisabled: {
    opacity: 0.5,
  },
  buttonText: {
    color: '#fff',
    fontSize: 18,
    fontWeight: '600',
  },
  loadingText: {
    color: '#fff',
    fontSize: 16,
    textAlign: 'center',
    marginTop: 100,
  },
  permissionText: {
    color: '#fff',
    fontSize: 18,
    textAlign: 'center',
    marginBottom: 20,
    marginTop: 100,
  },
  permissionButton: {
    paddingVertical: 16,
    paddingHorizontal: 32,
    borderRadius: 12,
    alignSelf: 'center',
  },
  permissionButtonText: {
    color: '#fff',
    fontSize: 18,
    fontWeight: '600',
  },
  connectionIndicator: {
    position: 'absolute',
    top: Platform.OS === 'ios' ? 60 : 40,
    left: 20,
    flexDirection: 'row',
    alignItems: 'center',
    gap: 8,
    backgroundColor: 'rgba(0, 0, 0, 0.5)',
    paddingHorizontal: 12,
    paddingVertical: 6,
    borderRadius: 20,
  },
  connectionDot: {
    width: 8,
    height: 8,
    borderRadius: 4,
  },
  connectionText: {
    color: '#fff',
    fontSize: 12,
    fontWeight: '500',
  },
});

```


Audio playback is implemented. The app still isn't sending audio input to the backend. Adding a mechanism to send audio chunks. Since `expo-av` records to a file, implementing periodic sending of recorded audio:


```typescriptreact
import React, { useState, useRef, useEffect } from 'react';
import { StyleSheet, View, TouchableOpacity, Platform, Alert } from 'react-native';
import { CameraView, CameraType, useCameraPermissions } from 'expo-camera';
import { Audio } from 'expo-av';
import { ThemedText } from '@/components/themed-text';
import { ThemedView } from '@/components/themed-view';
import { useSocket } from '@/contexts/SocketContext';
import { useColorScheme } from '@/hooks/use-color-scheme';
import { Colors } from '@/constants/theme';

export default function HomeScreen() {
  const colorScheme = useColorScheme();
  const { connected, sessionActive, startSession, stopSession, sendAudio, socket } = useSocket();
  const [facing, setFacing] = useState<CameraType>('back');
  const [permission, requestPermission] = useCameraPermissions();
  const [audioPermission, setAudioPermission] = useState<boolean>(false);
  const cameraRef = useRef<CameraView>(null);
  const recordingRef = useRef<Audio.Recording | null>(null);
  const soundRef = useRef<Audio.Sound | null>(null);
  const audioIntervalRef = useRef<ReturnType<typeof setInterval> | null>(null);

  useEffect(() => {
    // Request camera permission on mount
    if (!permission?.granted) {
      requestPermission();
    }

    // Request audio permission
    Audio.requestPermissionsAsync().then(({ status }) => {
      setAudioPermission(status === 'granted');
    });

    // Configure audio mode
    Audio.setAudioModeAsync({
      allowsRecordingIOS: true,
      playsInSilentModeIOS: true,
    });

    return () => {
      // Cleanup
      if (recordingRef.current) {
        recordingRef.current.stopAndUnloadAsync();
      }
      if (soundRef.current) {
        soundRef.current.unloadAsync();
      }
      if (audioIntervalRef.current) {
        clearInterval(audioIntervalRef.current);
      }
    };
  }, [permission, requestPermission]);

  const startAudioRecording = async () => {
    try {
      if (!audioPermission) {
        const { status } = await Audio.requestPermissionsAsync();
        if (status !== 'granted') {
          console.warn('Microphone permission not granted');
          return;
        }
        setAudioPermission(true);
      }

      // Start recording with status callback to send audio chunks
      const { recording } = await Audio.Recording.createAsync(
        Audio.RecordingOptionsPresets.LOW_QUALITY,
        async (status) => {
          // This callback is called periodically during recording
          if (status.isRecording && status.durationMillis && status.durationMillis > 0) {
            // Every 200ms, try to get and send audio data
            // Note: This is a workaround since expo-av doesn't provide direct PCM access
            // In production, you'd want to use a library that provides raw audio chunks
          }
        },
        200 // Update every 200ms
      );
      recordingRef.current = recording;
      console.log('‚úÖ Audio recording started');

      // Periodically send audio chunks (simplified approach)
      // Note: expo-av records to compressed format, so this won't work perfectly
      // For real-time audio, you'd need expo-audio or a native module
      audioIntervalRef.current = setInterval(async () => {
        if (recordingRef.current && sessionActive) {
          try {
            // Get recording status
            const status = await recordingRef.current.getStatusAsync();
            if (status.isRecording && status.durationMillis) {
              // For now, we can't easily extract PCM chunks from expo-av
              // The backend will work with text messages or when PyAudio is fixed
              // This is a placeholder for future implementation
            }
          } catch (error) {
            console.error('Error getting recording status:', error);
          }
        }
      }, 200); // Check every 200ms
    } catch (error) {
      console.error('Failed to start recording:', error);
    }
  };

  const stopAudioRecording = async () => {
    try {
      if (audioIntervalRef.current) {
        clearInterval(audioIntervalRef.current);
        audioIntervalRef.current = null;
      }

      if (recordingRef.current) {
        await recordingRef.current.stopAndUnloadAsync();
        recordingRef.current = null;
      }
    } catch (error) {
      console.error('Failed to stop recording:', error);
    }
  };

  const handleStartSession = async () => {
    if (!connected) {
      Alert.alert('Not Connected', 'Please wait for connection to server');
      return;
    }
    if (!permission?.granted) {
      Alert.alert('Camera Permission', 'Please grant camera permission to start session');
      requestPermission();
      return;
    }
    startSession();
    // Start audio recording when session starts
    setTimeout(() => {
      startAudioRecording();
    }, 500);
  };

  const handleStopSession = async () => {
    await stopAudioRecording();
    stopSession();
  };

  // Handle audio responses from backend
  useEffect(() => {
    if (!socket || !sessionActive) return;

    const handleAudioData = async (data: { data: string }) => {
      try {
        console.log('üîä Playing audio response');
        // Decode base64 audio data
        const audioBytes = atob(data.data);
        const audioArray = new Uint8Array(audioBytes.length);
        for (let i = 0; i < audioBytes.length; i++) {
          audioArray[i] = audioBytes.charCodeAt(i);
        }

        // Create a temporary file URI for the audio
        // Note: Backend sends PCM audio at 24kHz, mono
        // We need to create a playable format
        // For now, let's try to play it directly using expo-av
        
        // Stop any currently playing sound
        if (soundRef.current) {
          try {
            await soundRef.current.unloadAsync();
          } catch (e) {
            // Ignore errors
          }
        }

        // Convert PCM to a format expo-av can play
        // This is a simplified approach - in production you'd want proper PCM handling
        // For now, we'll create a WAV file from the PCM data
        const sampleRate = 24000; // Backend uses 24kHz for output
        const numChannels = 1;
        const bitsPerSample = 16;
        
        // Create WAV header
        const wavHeader = new ArrayBuffer(44);
        const view = new DataView(wavHeader);
        
        // RIFF header
        view.setUint8(0, 'R'.charCodeAt(0));
        view.setUint8(1, 'I'.charCodeAt(0));
        view.setUint8(2, 'F'.charCodeAt(0));
        view.setUint8(3, 'F'.charCodeAt(0));
        view.setUint32(4, 36 + audioArray.length, true);
        view.setUint8(8, 'W'.charCodeAt(0));
        view.setUint8(9, 'A'.charCodeAt(0));
        view.setUint8(10, 'V'.charCodeAt(0));
        view.setUint8(11, 'E'.charCodeAt(0));
        
        // fmt chunk
        view.setUint8(12, 'f'.charCodeAt(0));
        view.setUint8(13, 'm'.charCodeAt(0));
        view.setUint8(14, 't'.charCodeAt(0));
        view.setUint8(15, ' '.charCodeAt(0));
        view.setUint32(16, 16, true); // fmt chunk size
        view.setUint16(20, 1, true); // audio format (PCM)
        view.setUint16(22, numChannels, true);
        view.setUint32(24, sampleRate, true);
        view.setUint32(28, sampleRate * numChannels * bitsPerSample / 8, true); // byte rate
        view.setUint16(32, numChannels * bitsPerSample / 8, true); // block align
        view.setUint16(34, bitsPerSample, true);
        
        // data chunk
        view.setUint8(36, 'd'.charCodeAt(0));
        view.setUint8(37, 'a'.charCodeAt(0));
        view.setUint8(38, 't'.charCodeAt(0));
        view.setUint8(39, 'a'.charCodeAt(0));
        view.setUint32(40, audioArray.length, true);
        
        // Combine header and audio data
        const wavData = new Uint8Array(wavHeader.byteLength + audioArray.length);
        wavData.set(new Uint8Array(wavHeader), 0);
        wavData.set(audioArray, wavHeader.byteLength);
        
        // Convert to base64 data URI
        const base64 = btoa(String.fromCharCode(...wavData));
        const dataUri = `data:audio/wav;base64,${base64}`;
        
        // Create and play sound
        const { sound } = await Audio.Sound.createAsync(
          { uri: dataUri },
          { shouldPlay: true, volume: 1.0 }
        );
        soundRef.current = sound;
        
        // Clean up when done
        sound.setOnPlaybackStatusUpdate((status) => {
          if (!status.isLoaded) return;
          if (status.didJustFinish) {
            sound.unloadAsync();
            soundRef.current = null;
          }
        });
      } catch (error) {
        console.error('Error playing audio response:', error);
      }
    };

    socket.on('audio_data', handleAudioData);

    return () => {
      socket.off('audio_data', handleAudioData);
    };
  }, [socket, sessionActive]);

  const colors = Colors[colorScheme ?? 'light'];

  if (!permission) {
    return (
      <ThemedView style={styles.container}>
        <ThemedText style={styles.loadingText}>Requesting camera permission...</ThemedText>
      </ThemedView>
    );
  }

  if (!permission.granted) {
    return (
      <ThemedView style={styles.container}>
        <ThemedText style={styles.permissionText}>Camera permission is required</ThemedText>
        <TouchableOpacity
          style={[styles.permissionButton, { backgroundColor: colors.tint }]}
          onPress={requestPermission}>
          <ThemedText style={styles.permissionButtonText}>Grant Permission</ThemedText>
        </TouchableOpacity>
      </ThemedView>
    );
  }

  return (
    <ThemedView style={styles.container}>
      <CameraView
        ref={cameraRef}
        style={styles.camera}
        facing={facing}
        mode="picture"
      />

      {/* Minimal connection indicator */}
      <View style={styles.connectionIndicator}>
        <View
          style={[
            styles.connectionDot,
            { backgroundColor: connected ? '#10b981' : '#ef4444' },
          ]}
        />
        <ThemedText style={styles.connectionText}>
          {connected ? 'Connected' : 'Connecting...'}
        </ThemedText>
      </View>

      <View style={styles.controls}>
        <TouchableOpacity
          style={[
            styles.button,
            styles.startButton,
            (!connected || sessionActive) && styles.buttonDisabled,
            { backgroundColor: connected && !sessionActive ? '#10b981' : '#6b7280' },
          ]}
          onPress={handleStartSession}
          disabled={!connected || sessionActive}>
          <ThemedText style={styles.buttonText}>Start</ThemedText>
        </TouchableOpacity>

        <TouchableOpacity
          style={[
            styles.button,
            styles.stopButton,
            !sessionActive && styles.buttonDisabled,
            { backgroundColor: sessionActive ? '#ef4444' : '#6b7280' },
          ]}
          onPress={handleStopSession}
          disabled={!sessionActive}>
          <ThemedText style={styles.buttonText}>Stop</ThemedText>
        </TouchableOpacity>
      </View>
    </ThemedView>
  );
}

const styles = StyleSheet.create({
  container: {
    flex: 1,
    backgroundColor: '#000',
  },
  camera: {
    flex: 1,
  },
  controls: {
    position: 'absolute',
    bottom: 0,
    left: 0,
    right: 0,
    flexDirection: 'row',
    justifyContent: 'center',
    alignItems: 'center',
    paddingBottom: Platform.OS === 'ios' ? 40 : 20,
    paddingHorizontal: 20,
    gap: 20,
  },
  button: {
    flex: 1,
    maxWidth: 150,
    paddingVertical: 16,
    paddingHorizontal: 32,
    borderRadius: 12,
    alignItems: 'center',
    justifyContent: 'center',
  },
  startButton: {
    backgroundColor: '#10b981',
  },
  stopButton: {
    backgroundColor: '#ef4444',
  },
  buttonDisabled: {
    opacity: 0.5,
  },
  buttonText: {
    color: '#fff',
    fontSize: 18,
    fontWeight: '600',
  },
  loadingText: {
    color: '#fff',
    fontSize: 16,
    textAlign: 'center',
    marginTop: 100,
  },
  permissionText: {
    color: '#fff',
    fontSize: 18,
    textAlign: 'center',
    marginBottom: 20,
    marginTop: 100,
  },
  permissionButton: {
    paddingVertical: 16,
    paddingHorizontal: 32,
    borderRadius: 12,
    alignSelf: 'center',
  },
  permissionButtonText: {
    color: '#fff',
    fontSize: 18,
    fontWeight: '600',
  },
  connectionIndicator: {
    position: 'absolute',
    top: Platform.OS === 'ios' ? 60 : 40,
    left: 20,
    flexDirection: 'row',
    alignItems: 'center',
    gap: 8,
    backgroundColor: 'rgba(0, 0, 0, 0.5)',
    paddingHorizontal: 12,
    paddingVertical: 6,
    borderRadius: 20,
  },
  connectionDot: {
    width: 8,
    height: 8,
    borderRadius: 4,
  },
  connectionText: {
    color: '#fff',
    fontSize: 12,
    fontWeight: '500',
  },
});

```


## Changes Made

1. **Audio playback implemented**: The app now receives audio responses from the backend and plays them. It converts PCM audio to WAV format and plays it using expo-av.

2. **Audio input issue**: The mobile app records audio but doesn't send it to the backend because `expo-av` records to compressed formats (m4a), while the backend expects raw PCM chunks. This is why you're not getting responses ‚Äî Gemini has no audio input to respond to.

## Current Status

- Audio output: implemented and should work
- Audio input: not working (expo-av limitation)
- Backend: ready to receive audio, but PyAudio on your Mac isn't working

## Solutions

**Option 1: Fix PyAudio on your Mac** (recommended for testing)
```bash
cd /Users/eirvav/PythonProjects/test
pip uninstall pyaudio
pip install pyaudio
```
Then restart the Flask backend. This will use your computer's microphone.

**Option 2: Test audio output with text messages**
I can add a simple text input button so you can send text messages to test if audio responses play correctly.

**Option 3: Implement proper audio streaming**
This requires switching from `expo-av` to `expo-audio` or a native module that provides raw PCM audio chunks.

For now, audio playback should work. To test it, you need audio input. Should I:
1. Add a text message input to test audio output?
2. Help fix PyAudio on your Mac?
3. Implement proper real-time audio streaming from the mobile app?

```shellscript
cd /Users/eirvav/PythonProjects/test
pip uninstall pyaudio
pip install pyaudio
```

---

**User**

Still no audio

---

**Cursor**


Adding debugging and simplifying audio playback. Checking if audio data is received and fixing playback:


```typescriptreact
import React, { useState, useRef, useEffect } from 'react';
import { StyleSheet, View, TouchableOpacity, Platform, Alert } from 'react-native';
import { CameraView, CameraType, useCameraPermissions } from 'expo-camera';
import { Audio } from 'expo-av';
import { ThemedText } from '@/components/themed-text';
import { ThemedView } from '@/components/themed-view';
import { useSocket } from '@/contexts/SocketContext';
import { useColorScheme } from '@/hooks/use-color-scheme';
import { Colors } from '@/constants/theme';

export default function HomeScreen() {
  const colorScheme = useColorScheme();
  const { connected, sessionActive, startSession, stopSession, sendAudio, socket } = useSocket();
  const [facing, setFacing] = useState<CameraType>('back');
  const [permission, requestPermission] = useCameraPermissions();
  const [audioPermission, setAudioPermission] = useState<boolean>(false);
  const cameraRef = useRef<CameraView>(null);
  const recordingRef = useRef<Audio.Recording | null>(null);
  const soundRef = useRef<Audio.Sound | null>(null);
  const audioIntervalRef = useRef<ReturnType<typeof setInterval> | null>(null);

  useEffect(() => {
    // Request camera permission on mount
    if (!permission?.granted) {
      requestPermission();
    }

    // Request audio permission
    Audio.requestPermissionsAsync().then(({ status }) => {
      setAudioPermission(status === 'granted');
    });

    // Configure audio mode
    Audio.setAudioModeAsync({
      allowsRecordingIOS: true,
      playsInSilentModeIOS: true,
    });

    return () => {
      // Cleanup
      if (recordingRef.current) {
        recordingRef.current.stopAndUnloadAsync();
      }
      if (soundRef.current) {
        soundRef.current.unloadAsync();
      }
      if (audioIntervalRef.current) {
        clearInterval(audioIntervalRef.current);
      }
    };
  }, [permission, requestPermission]);

  const startAudioRecording = async () => {
    try {
      if (!audioPermission) {
        const { status } = await Audio.requestPermissionsAsync();
        if (status !== 'granted') {
          console.warn('Microphone permission not granted');
          return;
        }
        setAudioPermission(true);
      }

      // Start recording with status callback to send audio chunks
      const { recording } = await Audio.Recording.createAsync(
        Audio.RecordingOptionsPresets.LOW_QUALITY,
        async (status) => {
          // This callback is called periodically during recording
          if (status.isRecording && status.durationMillis && status.durationMillis > 0) {
            // Every 200ms, try to get and send audio data
            // Note: This is a workaround since expo-av doesn't provide direct PCM access
            // In production, you'd want to use a library that provides raw audio chunks
          }
        },
        200 // Update every 200ms
      );
      recordingRef.current = recording;
      console.log('‚úÖ Audio recording started');

      // Periodically send audio chunks (simplified approach)
      // Note: expo-av records to compressed format, so this won't work perfectly
      // For real-time audio, you'd need expo-audio or a native module
      audioIntervalRef.current = setInterval(async () => {
        if (recordingRef.current && sessionActive) {
          try {
            // Get recording status
            const status = await recordingRef.current.getStatusAsync();
            if (status.isRecording && status.durationMillis) {
              // For now, we can't easily extract PCM chunks from expo-av
              // The backend will work with text messages or when PyAudio is fixed
              // This is a placeholder for future implementation
            }
          } catch (error) {
            console.error('Error getting recording status:', error);
          }
        }
      }, 200); // Check every 200ms
    } catch (error) {
      console.error('Failed to start recording:', error);
    }
  };

  const stopAudioRecording = async () => {
    try {
      if (audioIntervalRef.current) {
        clearInterval(audioIntervalRef.current);
        audioIntervalRef.current = null;
      }

      if (recordingRef.current) {
        await recordingRef.current.stopAndUnloadAsync();
        recordingRef.current = null;
      }
    } catch (error) {
      console.error('Failed to stop recording:', error);
    }
  };

  const handleStartSession = async () => {
    if (!connected) {
      Alert.alert('Not Connected', 'Please wait for connection to server');
      return;
    }
    if (!permission?.granted) {
      Alert.alert('Camera Permission', 'Please grant camera permission to start session');
      requestPermission();
      return;
    }
    startSession();
    // Start audio recording when session starts
    setTimeout(() => {
      startAudioRecording();
    }, 500);
  };

  const handleStopSession = async () => {
    await stopAudioRecording();
    stopSession();
  };

  // Handle audio responses from backend
  useEffect(() => {
    if (!socket || !sessionActive) return;

    const handleAudioData = async (data: { data: string }) => {
      try {
        console.log('üîä Audio data received, length:', data.data?.length || 0);
        
        if (!data.data || data.data.length === 0) {
          console.warn('Empty audio data received');
          return;
        }

        // Decode base64 audio data
        let audioArray: Uint8Array;
        try {
          const audioBytes = atob(data.data);
          audioArray = new Uint8Array(audioBytes.length);
          for (let i = 0; i < audioBytes.length; i++) {
            audioArray[i] = audioBytes.charCodeAt(i);
          }
          console.log('‚úÖ Decoded audio data, size:', audioArray.length, 'bytes');
        } catch (error) {
          console.error('‚ùå Error decoding base64 audio:', error);
          return;
        }

        // Create a temporary file URI for the audio
        // Note: Backend sends PCM audio at 24kHz, mono
        // We need to create a playable format
        // For now, let's try to play it directly using expo-av
        
        // Stop any currently playing sound
        if (soundRef.current) {
          try {
            await soundRef.current.unloadAsync();
          } catch (e) {
            // Ignore errors
          }
        }

        // Convert PCM to a format expo-av can play
        // This is a simplified approach - in production you'd want proper PCM handling
        // For now, we'll create a WAV file from the PCM data
        const sampleRate = 24000; // Backend uses 24kHz for output
        const numChannels = 1;
        const bitsPerSample = 16;
        
        // Create WAV header
        const wavHeader = new ArrayBuffer(44);
        const view = new DataView(wavHeader);
        
        // RIFF header
        view.setUint8(0, 'R'.charCodeAt(0));
        view.setUint8(1, 'I'.charCodeAt(0));
        view.setUint8(2, 'F'.charCodeAt(0));
        view.setUint8(3, 'F'.charCodeAt(0));
        view.setUint32(4, 36 + audioArray.length, true);
        view.setUint8(8, 'W'.charCodeAt(0));
        view.setUint8(9, 'A'.charCodeAt(0));
        view.setUint8(10, 'V'.charCodeAt(0));
        view.setUint8(11, 'E'.charCodeAt(0));
        
        // fmt chunk
        view.setUint8(12, 'f'.charCodeAt(0));
        view.setUint8(13, 'm'.charCodeAt(0));
        view.setUint8(14, 't'.charCodeAt(0));
        view.setUint8(15, ' '.charCodeAt(0));
        view.setUint32(16, 16, true); // fmt chunk size
        view.setUint16(20, 1, true); // audio format (PCM)
        view.setUint16(22, numChannels, true);
        view.setUint32(24, sampleRate, true);
        view.setUint32(28, sampleRate * numChannels * bitsPerSample / 8, true); // byte rate
        view.setUint16(32, numChannels * bitsPerSample / 8, true); // block align
        view.setUint16(34, bitsPerSample, true);
        
        // data chunk
        view.setUint8(36, 'd'.charCodeAt(0));
        view.setUint8(37, 'a'.charCodeAt(0));
        view.setUint8(38, 't'.charCodeAt(0));
        view.setUint8(39, 'a'.charCodeAt(0));
        view.setUint32(40, audioArray.length, true);
        
        // Combine header and audio data
        const wavData = new Uint8Array(wavHeader.byteLength + audioArray.length);
        wavData.set(new Uint8Array(wavHeader), 0);
        wavData.set(audioArray, wavHeader.byteLength);
        
        // Convert to base64 data URI
        const base64 = btoa(String.fromCharCode(...wavData));
        const dataUri = `data:audio/wav;base64,${base64}`;
        
        // Create and play sound
        const { sound } = await Audio.Sound.createAsync(
          { uri: dataUri },
          { shouldPlay: true, volume: 1.0 }
        );
        soundRef.current = sound;
        
        // Clean up when done
        sound.setOnPlaybackStatusUpdate((status) => {
          if (!status.isLoaded) return;
          if (status.didJustFinish) {
            sound.unloadAsync();
            soundRef.current = null;
          }
        });
      } catch (error) {
        console.error('Error playing audio response:', error);
      }
    };

    socket.on('audio_data', handleAudioData);

    return () => {
      socket.off('audio_data', handleAudioData);
    };
  }, [socket, sessionActive]);

  const colors = Colors[colorScheme ?? 'light'];

  if (!permission) {
    return (
      <ThemedView style={styles.container}>
        <ThemedText style={styles.loadingText}>Requesting camera permission...</ThemedText>
      </ThemedView>
    );
  }

  if (!permission.granted) {
    return (
      <ThemedView style={styles.container}>
        <ThemedText style={styles.permissionText}>Camera permission is required</ThemedText>
        <TouchableOpacity
          style={[styles.permissionButton, { backgroundColor: colors.tint }]}
          onPress={requestPermission}>
          <ThemedText style={styles.permissionButtonText}>Grant Permission</ThemedText>
        </TouchableOpacity>
      </ThemedView>
    );
  }

  return (
    <ThemedView style={styles.container}>
      <CameraView
        ref={cameraRef}
        style={styles.camera}
        facing={facing}
        mode="picture"
      />

      {/* Minimal connection indicator */}
      <View style={styles.connectionIndicator}>
        <View
          style={[
            styles.connectionDot,
            { backgroundColor: connected ? '#10b981' : '#ef4444' },
          ]}
        />
        <ThemedText style={styles.connectionText}>
          {connected ? 'Connected' : 'Connecting...'}
        </ThemedText>
      </View>

      <View style={styles.controls}>
        <TouchableOpacity
          style={[
            styles.button,
            styles.startButton,
            (!connected || sessionActive) && styles.buttonDisabled,
            { backgroundColor: connected && !sessionActive ? '#10b981' : '#6b7280' },
          ]}
          onPress={handleStartSession}
          disabled={!connected || sessionActive}>
          <ThemedText style={styles.buttonText}>Start</ThemedText>
        </TouchableOpacity>

        <TouchableOpacity
          style={[
            styles.button,
            styles.stopButton,
            !sessionActive && styles.buttonDisabled,
            { backgroundColor: sessionActive ? '#ef4444' : '#6b7280' },
          ]}
          onPress={handleStopSession}
          disabled={!sessionActive}>
          <ThemedText style={styles.buttonText}>Stop</ThemedText>
        </TouchableOpacity>
      </View>
    </ThemedView>
  );
}

const styles = StyleSheet.create({
  container: {
    flex: 1,
    backgroundColor: '#000',
  },
  camera: {
    flex: 1,
  },
  controls: {
    position: 'absolute',
    bottom: 0,
    left: 0,
    right: 0,
    flexDirection: 'row',
    justifyContent: 'center',
    alignItems: 'center',
    paddingBottom: Platform.OS === 'ios' ? 40 : 20,
    paddingHorizontal: 20,
    gap: 20,
  },
  button: {
    flex: 1,
    maxWidth: 150,
    paddingVertical: 16,
    paddingHorizontal: 32,
    borderRadius: 12,
    alignItems: 'center',
    justifyContent: 'center',
  },
  startButton: {
    backgroundColor: '#10b981',
  },
  stopButton: {
    backgroundColor: '#ef4444',
  },
  buttonDisabled: {
    opacity: 0.5,
  },
  buttonText: {
    color: '#fff',
    fontSize: 18,
    fontWeight: '600',
  },
  loadingText: {
    color: '#fff',
    fontSize: 16,
    textAlign: 'center',
    marginTop: 100,
  },
  permissionText: {
    color: '#fff',
    fontSize: 18,
    textAlign: 'center',
    marginBottom: 20,
    marginTop: 100,
  },
  permissionButton: {
    paddingVertical: 16,
    paddingHorizontal: 32,
    borderRadius: 12,
    alignSelf: 'center',
  },
  permissionButtonText: {
    color: '#fff',
    fontSize: 18,
    fontWeight: '600',
  },
  connectionIndicator: {
    position: 'absolute',
    top: Platform.OS === 'ios' ? 60 : 40,
    left: 20,
    flexDirection: 'row',
    alignItems: 'center',
    gap: 8,
    backgroundColor: 'rgba(0, 0, 0, 0.5)',
    paddingHorizontal: 12,
    paddingVertical: 6,
    borderRadius: 20,
  },
  connectionDot: {
    width: 8,
    height: 8,
    borderRadius: 4,
  },
  connectionText: {
    color: '#fff',
    fontSize: 12,
    fontWeight: '500',
  },
});

```

```typescriptreact
import React, { useState, useRef, useEffect } from 'react';
import { StyleSheet, View, TouchableOpacity, Platform, Alert } from 'react-native';
import { CameraView, CameraType, useCameraPermissions } from 'expo-camera';
import { Audio } from 'expo-av';
import { ThemedText } from '@/components/themed-text';
import { ThemedView } from '@/components/themed-view';
import { useSocket } from '@/contexts/SocketContext';
import { useColorScheme } from '@/hooks/use-color-scheme';
import { Colors } from '@/constants/theme';

export default function HomeScreen() {
  const colorScheme = useColorScheme();
  const { connected, sessionActive, startSession, stopSession, sendAudio, socket } = useSocket();
  const [facing, setFacing] = useState<CameraType>('back');
  const [permission, requestPermission] = useCameraPermissions();
  const [audioPermission, setAudioPermission] = useState<boolean>(false);
  const cameraRef = useRef<CameraView>(null);
  const recordingRef = useRef<Audio.Recording | null>(null);
  const soundRef = useRef<Audio.Sound | null>(null);
  const audioIntervalRef = useRef<ReturnType<typeof setInterval> | null>(null);

  useEffect(() => {
    // Request camera permission on mount
    if (!permission?.granted) {
      requestPermission();
    }

    // Request audio permission
    Audio.requestPermissionsAsync().then(({ status }) => {
      setAudioPermission(status === 'granted');
    });

    // Configure audio mode
    Audio.setAudioModeAsync({
      allowsRecordingIOS: true,
      playsInSilentModeIOS: true,
    });

    return () => {
      // Cleanup
      if (recordingRef.current) {
        recordingRef.current.stopAndUnloadAsync();
      }
      if (soundRef.current) {
        soundRef.current.unloadAsync();
      }
      if (audioIntervalRef.current) {
        clearInterval(audioIntervalRef.current);
      }
    };
  }, [permission, requestPermission]);

  const startAudioRecording = async () => {
    try {
      if (!audioPermission) {
        const { status } = await Audio.requestPermissionsAsync();
        if (status !== 'granted') {
          console.warn('Microphone permission not granted');
          return;
        }
        setAudioPermission(true);
      }

      // Start recording with status callback to send audio chunks
      const { recording } = await Audio.Recording.createAsync(
        Audio.RecordingOptionsPresets.LOW_QUALITY,
        async (status) => {
          // This callback is called periodically during recording
          if (status.isRecording && status.durationMillis && status.durationMillis > 0) {
            // Every 200ms, try to get and send audio data
            // Note: This is a workaround since expo-av doesn't provide direct PCM access
            // In production, you'd want to use a library that provides raw audio chunks
          }
        },
        200 // Update every 200ms
      );
      recordingRef.current = recording;
      console.log('‚úÖ Audio recording started');

      // Periodically send audio chunks (simplified approach)
      // Note: expo-av records to compressed format, so this won't work perfectly
      // For real-time audio, you'd need expo-audio or a native module
      audioIntervalRef.current = setInterval(async () => {
        if (recordingRef.current && sessionActive) {
          try {
            // Get recording status
            const status = await recordingRef.current.getStatusAsync();
            if (status.isRecording && status.durationMillis) {
              // For now, we can't easily extract PCM chunks from expo-av
              // The backend will work with text messages or when PyAudio is fixed
              // This is a placeholder for future implementation
            }
          } catch (error) {
            console.error('Error getting recording status:', error);
          }
        }
      }, 200); // Check every 200ms
    } catch (error) {
      console.error('Failed to start recording:', error);
    }
  };

  const stopAudioRecording = async () => {
    try {
      if (audioIntervalRef.current) {
        clearInterval(audioIntervalRef.current);
        audioIntervalRef.current = null;
      }

      if (recordingRef.current) {
        await recordingRef.current.stopAndUnloadAsync();
        recordingRef.current = null;
      }
    } catch (error) {
      console.error('Failed to stop recording:', error);
    }
  };

  const handleStartSession = async () => {
    if (!connected) {
      Alert.alert('Not Connected', 'Please wait for connection to server');
      return;
    }
    if (!permission?.granted) {
      Alert.alert('Camera Permission', 'Please grant camera permission to start session');
      requestPermission();
      return;
    }
    startSession();
    // Start audio recording when session starts
    setTimeout(() => {
      startAudioRecording();
    }, 500);
  };

  const handleStopSession = async () => {
    await stopAudioRecording();
    stopSession();
  };

  // Handle audio responses from backend
  useEffect(() => {
    if (!socket || !sessionActive) return;

    const handleAudioData = async (data: { data: string }) => {
      try {
        console.log('üîä Audio data received, length:', data.data?.length || 0);
        
        if (!data.data || data.data.length === 0) {
          console.warn('Empty audio data received');
          return;
        }

        // Decode base64 audio data
        let audioArray: Uint8Array;
        try {
          const audioBytes = atob(data.data);
          audioArray = new Uint8Array(audioBytes.length);
          for (let i = 0; i < audioBytes.length; i++) {
            audioArray[i] = audioBytes.charCodeAt(i);
          }
          console.log('‚úÖ Decoded audio data, size:', audioArray.length, 'bytes');
        } catch (error) {
          console.error('‚ùå Error decoding base64 audio:', error);
          return;
        }

        // Create a temporary file URI for the audio
        // Note: Backend sends PCM audio at 24kHz, mono
        // We need to create a playable format
        // For now, let's try to play it directly using expo-av
        
        // Stop any currently playing sound
        if (soundRef.current) {
          try {
            await soundRef.current.unloadAsync();
          } catch (e) {
            // Ignore errors
          }
        }

        // Convert PCM to a format expo-av can play
        // This is a simplified approach - in production you'd want proper PCM handling
        // For now, we'll create a WAV file from the PCM data
        const sampleRate = 24000; // Backend uses 24kHz for output
        const numChannels = 1;
        const bitsPerSample = 16;
        
        // Create WAV header
        const wavHeader = new ArrayBuffer(44);
        const view = new DataView(wavHeader);
        
        // RIFF header
        view.setUint8(0, 'R'.charCodeAt(0));
        view.setUint8(1, 'I'.charCodeAt(0));
        view.setUint8(2, 'F'.charCodeAt(0));
        view.setUint8(3, 'F'.charCodeAt(0));
        view.setUint32(4, 36 + audioArray.length, true);
        view.setUint8(8, 'W'.charCodeAt(0));
        view.setUint8(9, 'A'.charCodeAt(0));
        view.setUint8(10, 'V'.charCodeAt(0));
        view.setUint8(11, 'E'.charCodeAt(0));
        
        // fmt chunk
        view.setUint8(12, 'f'.charCodeAt(0));
        view.setUint8(13, 'm'.charCodeAt(0));
        view.setUint8(14, 't'.charCodeAt(0));
        view.setUint8(15, ' '.charCodeAt(0));
        view.setUint32(16, 16, true); // fmt chunk size
        view.setUint16(20, 1, true); // audio format (PCM)
        view.setUint16(22, numChannels, true);
        view.setUint32(24, sampleRate, true);
        view.setUint32(28, sampleRate * numChannels * bitsPerSample / 8, true); // byte rate
        view.setUint16(32, numChannels * bitsPerSample / 8, true); // block align
        view.setUint16(34, bitsPerSample, true);
        
        // data chunk
        view.setUint8(36, 'd'.charCodeAt(0));
        view.setUint8(37, 'a'.charCodeAt(0));
        view.setUint8(38, 't'.charCodeAt(0));
        view.setUint8(39, 'a'.charCodeAt(0));
        view.setUint32(40, audioArray.length, true);
        
        // Combine header and audio data
        const wavData = new Uint8Array(wavHeader.byteLength + audioArray.length);
        wavData.set(new Uint8Array(wavHeader), 0);
        wavData.set(audioArray, wavHeader.byteLength);
        
        // Convert to base64 data URI
        let base64: string;
        try {
          // Use a more efficient method for large arrays
          const binaryString = Array.from(wavData)
            .map(byte => String.fromCharCode(byte))
            .join('');
          base64 = btoa(binaryString);
        } catch (error) {
          console.error('‚ùå Error encoding to base64:', error);
          return;
        }
        
        const dataUri = `data:audio/wav;base64,${base64}`;
        console.log('‚úÖ Created WAV data URI, length:', dataUri.length);
        
        // Stop any currently playing sound
        if (soundRef.current) {
          try {
            const status = await soundRef.current.getStatusAsync();
            if (status.isLoaded) {
              await soundRef.current.stopAsync();
            }
            await soundRef.current.unloadAsync();
          } catch (e) {
            // Ignore errors
          }
          soundRef.current = null;
        }
        
        // Create and play sound
        console.log('üéµ Creating audio sound...');
        const { sound } = await Audio.Sound.createAsync(
          { uri: dataUri },
          { 
            shouldPlay: true, 
            volume: 1.0,
            isMuted: false,
          }
        );
        soundRef.current = sound;
        console.log('‚úÖ Audio sound created and playing');
        
        // Clean up when done
        sound.setOnPlaybackStatusUpdate((status) => {
          if (!status.isLoaded) return;
          if (status.didJustFinish) {
            console.log('‚úÖ Audio playback finished');
            sound.unloadAsync();
            soundRef.current = null;
          }
        });
      } catch (error) {
        console.error('Error playing audio response:', error);
      }
    };

    socket.on('audio_data', handleAudioData);

    return () => {
      socket.off('audio_data', handleAudioData);
    };
  }, [socket, sessionActive]);

  const colors = Colors[colorScheme ?? 'light'];

  if (!permission) {
    return (
      <ThemedView style={styles.container}>
        <ThemedText style={styles.loadingText}>Requesting camera permission...</ThemedText>
      </ThemedView>
    );
  }

  if (!permission.granted) {
    return (
      <ThemedView style={styles.container}>
        <ThemedText style={styles.permissionText}>Camera permission is required</ThemedText>
        <TouchableOpacity
          style={[styles.permissionButton, { backgroundColor: colors.tint }]}
          onPress={requestPermission}>
          <ThemedText style={styles.permissionButtonText}>Grant Permission</ThemedText>
        </TouchableOpacity>
      </ThemedView>
    );
  }

  return (
    <ThemedView style={styles.container}>
      <CameraView
        ref={cameraRef}
        style={styles.camera}
        facing={facing}
        mode="picture"
      />

      {/* Minimal connection indicator */}
      <View style={styles.connectionIndicator}>
        <View
          style={[
            styles.connectionDot,
            { backgroundColor: connected ? '#10b981' : '#ef4444' },
          ]}
        />
        <ThemedText style={styles.connectionText}>
          {connected ? 'Connected' : 'Connecting...'}
        </ThemedText>
      </View>

      <View style={styles.controls}>
        <TouchableOpacity
          style={[
            styles.button,
            styles.startButton,
            (!connected || sessionActive) && styles.buttonDisabled,
            { backgroundColor: connected && !sessionActive ? '#10b981' : '#6b7280' },
          ]}
          onPress={handleStartSession}
          disabled={!connected || sessionActive}>
          <ThemedText style={styles.buttonText}>Start</ThemedText>
        </TouchableOpacity>

        <TouchableOpacity
          style={[
            styles.button,
            styles.stopButton,
            !sessionActive && styles.buttonDisabled,
            { backgroundColor: sessionActive ? '#ef4444' : '#6b7280' },
          ]}
          onPress={handleStopSession}
          disabled={!sessionActive}>
          <ThemedText style={styles.buttonText}>Stop</ThemedText>
        </TouchableOpacity>
      </View>
    </ThemedView>
  );
}

const styles = StyleSheet.create({
  container: {
    flex: 1,
    backgroundColor: '#000',
  },
  camera: {
    flex: 1,
  },
  controls: {
    position: 'absolute',
    bottom: 0,
    left: 0,
    right: 0,
    flexDirection: 'row',
    justifyContent: 'center',
    alignItems: 'center',
    paddingBottom: Platform.OS === 'ios' ? 40 : 20,
    paddingHorizontal: 20,
    gap: 20,
  },
  button: {
    flex: 1,
    maxWidth: 150,
    paddingVertical: 16,
    paddingHorizontal: 32,
    borderRadius: 12,
    alignItems: 'center',
    justifyContent: 'center',
  },
  startButton: {
    backgroundColor: '#10b981',
  },
  stopButton: {
    backgroundColor: '#ef4444',
  },
  buttonDisabled: {
    opacity: 0.5,
  },
  buttonText: {
    color: '#fff',
    fontSize: 18,
    fontWeight: '600',
  },
  loadingText: {
    color: '#fff',
    fontSize: 16,
    textAlign: 'center',
    marginTop: 100,
  },
  permissionText: {
    color: '#fff',
    fontSize: 18,
    textAlign: 'center',
    marginBottom: 20,
    marginTop: 100,
  },
  permissionButton: {
    paddingVertical: 16,
    paddingHorizontal: 32,
    borderRadius: 12,
    alignSelf: 'center',
  },
  permissionButtonText: {
    color: '#fff',
    fontSize: 18,
    fontWeight: '600',
  },
  connectionIndicator: {
    position: 'absolute',
    top: Platform.OS === 'ios' ? 60 : 40,
    left: 20,
    flexDirection: 'row',
    alignItems: 'center',
    gap: 8,
    backgroundColor: 'rgba(0, 0, 0, 0.5)',
    paddingHorizontal: 12,
    paddingVertical: 6,
    borderRadius: 20,
  },
  connectionDot: {
    width: 8,
    height: 8,
    borderRadius: 4,
  },
  connectionText: {
    color: '#fff',
    fontSize: 12,
    fontWeight: '500',
  },
});

```


No audio input reaches Gemini (PyAudio isn't working and the mobile app isn't sending audio), so it won't generate responses. Adding a text message option to trigger responses and verify audio playback:


```typescriptreact
import React, { createContext, useContext, useEffect, useState } from 'react';
import { io, Socket } from 'socket.io-client';
import Constants from 'expo-constants';

interface SocketContextType {
  socket: Socket | null;
  connected: boolean;
  sessionActive: boolean;
  startSession: () => void;
  stopSession: () => void;
  sendAudio: (audioData: string) => void;
  sendMessage: (text: string) => void;
  onAudioData?: (data: { data: string }) => void;
}

const SocketContext = createContext<SocketContextType | undefined>(undefined);

// Get backend URL from environment variable or Constants, with fallback
const getBackendUrl = () => {
  // Try process.env first (works in Expo SDK 49+)
  if (process.env.EXPO_PUBLIC_BACKEND_URL) {
    return process.env.EXPO_PUBLIC_BACKEND_URL;
  }
  // Try Constants.expoConfig.extra (for app.json config)
  if (Constants.expoConfig?.extra?.backendUrl) {
    return Constants.expoConfig.extra.backendUrl;
  }
  // Default fallback
  return 'http://localhost:5001';
};

const BACKEND_URL = getBackendUrl();

export function SocketProvider({ children }: { children: React.ReactNode }) {
  const [socket, setSocket] = useState<Socket | null>(null);
  const [connected, setConnected] = useState(false);
  const [sessionActive, setSessionActive] = useState(false);
  const [audioDataCallback, setAudioDataCallback] = useState<((data: { data: string }) => void) | undefined>();

  useEffect(() => {
    // Log the backend URL being used
    console.log('Connecting to backend at:', BACKEND_URL);

    // Initialize socket connection
    const newSocket = io(BACKEND_URL, {
      transports: ['websocket', 'polling'],
      reconnection: true,
      reconnectionDelay: 1000,
      reconnectionAttempts: 5,
      timeout: 10000,
    });

    // Connection events
    newSocket.on('connect', () => {
      console.log('‚úÖ Socket.IO connected successfully');
      setConnected(true);
    });

    newSocket.on('disconnect', (reason) => {
      console.log('‚ùå Socket.IO disconnected:', reason);
      setConnected(false);
      setSessionActive(false);
    });

    newSocket.on('connect_error', (error) => {
      console.error('‚ùå Socket.IO connection error:', error.message);
      setConnected(false);
    });

    newSocket.on('reconnect', (attemptNumber) => {
      console.log('üîÑ Socket.IO reconnected after', attemptNumber, 'attempts');
      setConnected(true);
    });

    newSocket.on('reconnect_attempt', () => {
      console.log('üîÑ Attempting to reconnect...');
    });

    newSocket.on('reconnect_error', (error) => {
      console.error('‚ùå Socket.IO reconnect error:', error.message);
    });

    newSocket.on('reconnect_failed', () => {
      console.error('‚ùå Socket.IO reconnection failed');
    });

    // Session events
    newSocket.on('session_started', (data) => {
      console.log('‚úÖ Session started:', data);
      setSessionActive(true);
    });

    newSocket.on('session_stopped', (data) => {
      console.log('‚èπÔ∏è Session stopped:', data);
      setSessionActive(false);
    });

    // Handle other events with logging
    newSocket.on('connected', (data) => {
      console.log('üì° Server connected event:', data);
    });

    newSocket.on('text_response', (data) => {
      console.log('üìù Text response received:', data.text);
    });

    newSocket.on('audio_data', async (data) => {
      console.log('üîä Audio data received');
      // Call the callback if set
      if (audioDataCallback) {
        audioDataCallback(data);
      }
    });

    newSocket.on('info', (data) => {
      console.log('‚ÑπÔ∏è Info:', data.message);
    });

    newSocket.on('error', (data) => {
      console.error('‚ùå Error:', data.message);
    });

    setSocket(newSocket);

    return () => {
      console.log('üßπ Cleaning up socket connection');
      newSocket.close();
    };
  }, []);

  const startSession = () => {
    if (socket && connected) {
      socket.emit('start_session', { mode: 'camera' });
    }
  };

  const stopSession = () => {
    if (socket && sessionActive) {
      socket.emit('stop_session');
    }
  };

  const sendAudio = (audioData: string) => {
    if (socket && sessionActive && audioData) {
      socket.emit('send_audio', { data: audioData });
    }
  };

  const setOnAudioData = (callback: ((data: { data: string }) => void) | undefined) => {
    setAudioDataCallback(() => callback);
  };

  return (
    <SocketContext.Provider
      value={{
        socket,
        connected,
        sessionActive,
        startSession,
        stopSession,
        sendAudio,
        onAudioData: audioDataCallback,
      }}>
      {children}
    </SocketContext.Provider>
  );
}

export function useSocket() {
  const context = useContext(SocketContext);
  if (context === undefined) {
    throw new Error('useSocket must be used within a SocketProvider');
  }
  return context;
}


```

```typescriptreact
import React, { createContext, useContext, useEffect, useState } from 'react';
import { io, Socket } from 'socket.io-client';
import Constants from 'expo-constants';

interface SocketContextType {
  socket: Socket | null;
  connected: boolean;
  sessionActive: boolean;
  startSession: () => void;
  stopSession: () => void;
  sendAudio: (audioData: string) => void;
  sendMessage: (text: string) => void;
  onAudioData?: (data: { data: string }) => void;
}

const SocketContext = createContext<SocketContextType | undefined>(undefined);

// Get backend URL from environment variable or Constants, with fallback
const getBackendUrl = () => {
  // Try process.env first (works in Expo SDK 49+)
  if (process.env.EXPO_PUBLIC_BACKEND_URL) {
    return process.env.EXPO_PUBLIC_BACKEND_URL;
  }
  // Try Constants.expoConfig.extra (for app.json config)
  if (Constants.expoConfig?.extra?.backendUrl) {
    return Constants.expoConfig.extra.backendUrl;
  }
  // Default fallback
  return 'http://localhost:5001';
};

const BACKEND_URL = getBackendUrl();

export function SocketProvider({ children }: { children: React.ReactNode }) {
  const [socket, setSocket] = useState<Socket | null>(null);
  const [connected, setConnected] = useState(false);
  const [sessionActive, setSessionActive] = useState(false);
  const [audioDataCallback, setAudioDataCallback] = useState<((data: { data: string }) => void) | undefined>();

  useEffect(() => {
    // Log the backend URL being used
    console.log('Connecting to backend at:', BACKEND_URL);

    // Initialize socket connection
    const newSocket = io(BACKEND_URL, {
      transports: ['websocket', 'polling'],
      reconnection: true,
      reconnectionDelay: 1000,
      reconnectionAttempts: 5,
      timeout: 10000,
    });

    // Connection events
    newSocket.on('connect', () => {
      console.log('‚úÖ Socket.IO connected successfully');
      setConnected(true);
    });

    newSocket.on('disconnect', (reason) => {
      console.log('‚ùå Socket.IO disconnected:', reason);
      setConnected(false);
      setSessionActive(false);
    });

    newSocket.on('connect_error', (error) => {
      console.error('‚ùå Socket.IO connection error:', error.message);
      setConnected(false);
    });

    newSocket.on('reconnect', (attemptNumber) => {
      console.log('üîÑ Socket.IO reconnected after', attemptNumber, 'attempts');
      setConnected(true);
    });

    newSocket.on('reconnect_attempt', () => {
      console.log('üîÑ Attempting to reconnect...');
    });

    newSocket.on('reconnect_error', (error) => {
      console.error('‚ùå Socket.IO reconnect error:', error.message);
    });

    newSocket.on('reconnect_failed', () => {
      console.error('‚ùå Socket.IO reconnection failed');
    });

    // Session events
    newSocket.on('session_started', (data) => {
      console.log('‚úÖ Session started:', data);
      setSessionActive(true);
    });

    newSocket.on('session_stopped', (data) => {
      console.log('‚èπÔ∏è Session stopped:', data);
      setSessionActive(false);
    });

    // Handle other events with logging
    newSocket.on('connected', (data) => {
      console.log('üì° Server connected event:', data);
    });

    newSocket.on('text_response', (data) => {
      console.log('üìù Text response received:', data.text);
    });

    newSocket.on('audio_data', async (data) => {
      console.log('üîä Audio data received, size:', data.data?.length || 0, 'bytes');
      // Call the callback if set
      if (audioDataCallback) {
        audioDataCallback(data);
      }
    });

    newSocket.on('info', (data) => {
      console.log('‚ÑπÔ∏è Info:', data.message);
    });

    newSocket.on('error', (data) => {
      console.error('‚ùå Error:', data.message);
    });

    setSocket(newSocket);

    return () => {
      console.log('üßπ Cleaning up socket connection');
      newSocket.close();
    };
  }, []);

  const startSession = () => {
    if (socket && connected) {
      socket.emit('start_session', { mode: 'camera' });
    }
  };

  const stopSession = () => {
    if (socket && sessionActive) {
      socket.emit('stop_session');
    }
  };

  const sendAudio = (audioData: string) => {
    if (socket && sessionActive && audioData) {
      socket.emit('send_audio', { data: audioData });
    }
  };

  const setOnAudioData = (callback: ((data: { data: string }) => void) | undefined) => {
    setAudioDataCallback(() => callback);
  };

  return (
    <SocketContext.Provider
      value={{
        socket,
        connected,
        sessionActive,
        startSession,
        stopSession,
        sendAudio,
        onAudioData: audioDataCallback,
      }}>
      {children}
    </SocketContext.Provider>
  );
}

export function useSocket() {
  const context = useContext(SocketContext);
  if (context === undefined) {
    throw new Error('useSocket must be used within a SocketProvider');
  }
  return context;
}


```

```typescriptreact
import React, { createContext, useContext, useEffect, useState } from 'react';
import { io, Socket } from 'socket.io-client';
import Constants from 'expo-constants';

interface SocketContextType {
  socket: Socket | null;
  connected: boolean;
  sessionActive: boolean;
  startSession: () => void;
  stopSession: () => void;
  sendAudio: (audioData: string) => void;
  sendMessage: (text: string) => void;
  onAudioData?: (data: { data: string }) => void;
}

const SocketContext = createContext<SocketContextType | undefined>(undefined);

// Get backend URL from environment variable or Constants, with fallback
const getBackendUrl = () => {
  // Try process.env first (works in Expo SDK 49+)
  if (process.env.EXPO_PUBLIC_BACKEND_URL) {
    return process.env.EXPO_PUBLIC_BACKEND_URL;
  }
  // Try Constants.expoConfig.extra (for app.json config)
  if (Constants.expoConfig?.extra?.backendUrl) {
    return Constants.expoConfig.extra.backendUrl;
  }
  // Default fallback
  return 'http://localhost:5001';
};

const BACKEND_URL = getBackendUrl();

export function SocketProvider({ children }: { children: React.ReactNode }) {
  const [socket, setSocket] = useState<Socket | null>(null);
  const [connected, setConnected] = useState(false);
  const [sessionActive, setSessionActive] = useState(false);
  const [audioDataCallback, setAudioDataCallback] = useState<((data: { data: string }) => void) | undefined>();

  useEffect(() => {
    // Log the backend URL being used
    console.log('Connecting to backend at:', BACKEND_URL);

    // Initialize socket connection
    const newSocket = io(BACKEND_URL, {
      transports: ['websocket', 'polling'],
      reconnection: true,
      reconnectionDelay: 1000,
      reconnectionAttempts: 5,
      timeout: 10000,
    });

    // Connection events
    newSocket.on('connect', () => {
      console.log('‚úÖ Socket.IO connected successfully');
      setConnected(true);
    });

    newSocket.on('disconnect', (reason) => {
      console.log('‚ùå Socket.IO disconnected:', reason);
      setConnected(false);
      setSessionActive(false);
    });

    newSocket.on('connect_error', (error) => {
      console.error('‚ùå Socket.IO connection error:', error.message);
      setConnected(false);
    });

    newSocket.on('reconnect', (attemptNumber) => {
      console.log('üîÑ Socket.IO reconnected after', attemptNumber, 'attempts');
      setConnected(true);
    });

    newSocket.on('reconnect_attempt', () => {
      console.log('üîÑ Attempting to reconnect...');
    });

    newSocket.on('reconnect_error', (error) => {
      console.error('‚ùå Socket.IO reconnect error:', error.message);
    });

    newSocket.on('reconnect_failed', () => {
      console.error('‚ùå Socket.IO reconnection failed');
    });

    // Session events
    newSocket.on('session_started', (data) => {
      console.log('‚úÖ Session started:', data);
      setSessionActive(true);
    });

    newSocket.on('session_stopped', (data) => {
      console.log('‚èπÔ∏è Session stopped:', data);
      setSessionActive(false);
    });

    // Handle other events with logging
    newSocket.on('connected', (data) => {
      console.log('üì° Server connected event:', data);
    });

    newSocket.on('text_response', (data) => {
      console.log('üìù Text response received:', data.text);
    });

    newSocket.on('audio_data', async (data) => {
      console.log('üîä Audio data received, size:', data.data?.length || 0, 'bytes');
      // Call the callback if set
      if (audioDataCallback) {
        audioDataCallback(data);
      }
    });

    newSocket.on('info', (data) => {
      console.log('‚ÑπÔ∏è Info:', data.message);
    });

    newSocket.on('error', (data) => {
      console.error('‚ùå Error:', data.message);
    });

    setSocket(newSocket);

    return () => {
      console.log('üßπ Cleaning up socket connection');
      newSocket.close();
    };
  }, []);

  const startSession = () => {
    if (socket && connected) {
      socket.emit('start_session', { mode: 'camera' });
    }
  };

  const stopSession = () => {
    if (socket && sessionActive) {
      socket.emit('stop_session');
    }
  };

  const sendAudio = (audioData: string) => {
    if (socket && sessionActive && audioData) {
      socket.emit('send_audio', { data: audioData });
    }
  };

  const sendMessage = (text: string) => {
    if (socket && sessionActive && text.trim()) {
      console.log('üì§ Sending text message:', text);
      socket.emit('send_message', { text: text.trim() });
    }
  };

  const setOnAudioData = (callback: ((data: { data: string }) => void) | undefined) => {
    setAudioDataCallback(() => callback);
  };

  return (
    <SocketContext.Provider
      value={{
        socket,
        connected,
        sessionActive,
        startSession,
        stopSession,
        sendAudio,
        onAudioData: audioDataCallback,
      }}>
      {children}
    </SocketContext.Provider>
  );
}

export function useSocket() {
  const context = useContext(SocketContext);
  if (context === undefined) {
    throw new Error('useSocket must be used within a SocketProvider');
  }
  return context;
}


```

```typescriptreact
import React, { createContext, useContext, useEffect, useState } from 'react';
import { io, Socket } from 'socket.io-client';
import Constants from 'expo-constants';

interface SocketContextType {
  socket: Socket | null;
  connected: boolean;
  sessionActive: boolean;
  startSession: () => void;
  stopSession: () => void;
  sendAudio: (audioData: string) => void;
  sendMessage: (text: string) => void;
  onAudioData?: (data: { data: string }) => void;
}

const SocketContext = createContext<SocketContextType | undefined>(undefined);

// Get backend URL from environment variable or Constants, with fallback
const getBackendUrl = () => {
  // Try process.env first (works in Expo SDK 49+)
  if (process.env.EXPO_PUBLIC_BACKEND_URL) {
    return process.env.EXPO_PUBLIC_BACKEND_URL;
  }
  // Try Constants.expoConfig.extra (for app.json config)
  if (Constants.expoConfig?.extra?.backendUrl) {
    return Constants.expoConfig.extra.backendUrl;
  }
  // Default fallback
  return 'http://localhost:5001';
};

const BACKEND_URL = getBackendUrl();

export function SocketProvider({ children }: { children: React.ReactNode }) {
  const [socket, setSocket] = useState<Socket | null>(null);
  const [connected, setConnected] = useState(false);
  const [sessionActive, setSessionActive] = useState(false);
  const [audioDataCallback, setAudioDataCallback] = useState<((data: { data: string }) => void) | undefined>();

  useEffect(() => {
    // Log the backend URL being used
    console.log('Connecting to backend at:', BACKEND_URL);

    // Initialize socket connection
    const newSocket = io(BACKEND_URL, {
      transports: ['websocket', 'polling'],
      reconnection: true,
      reconnectionDelay: 1000,
      reconnectionAttempts: 5,
      timeout: 10000,
    });

    // Connection events
    newSocket.on('connect', () => {
      console.log('‚úÖ Socket.IO connected successfully');
      setConnected(true);
    });

    newSocket.on('disconnect', (reason) => {
      console.log('‚ùå Socket.IO disconnected:', reason);
      setConnected(false);
      setSessionActive(false);
    });

    newSocket.on('connect_error', (error) => {
      console.error('‚ùå Socket.IO connection error:', error.message);
      setConnected(false);
    });

    newSocket.on('reconnect', (attemptNumber) => {
      console.log('üîÑ Socket.IO reconnected after', attemptNumber, 'attempts');
      setConnected(true);
    });

    newSocket.on('reconnect_attempt', () => {
      console.log('üîÑ Attempting to reconnect...');
    });

    newSocket.on('reconnect_error', (error) => {
      console.error('‚ùå Socket.IO reconnect error:', error.message);
    });

    newSocket.on('reconnect_failed', () => {
      console.error('‚ùå Socket.IO reconnection failed');
    });

    // Session events
    newSocket.on('session_started', (data) => {
      console.log('‚úÖ Session started:', data);
      setSessionActive(true);
    });

    newSocket.on('session_stopped', (data) => {
      console.log('‚èπÔ∏è Session stopped:', data);
      setSessionActive(false);
    });

    // Handle other events with logging
    newSocket.on('connected', (data) => {
      console.log('üì° Server connected event:', data);
    });

    newSocket.on('text_response', (data) => {
      console.log('üìù Text response received:', data.text);
    });

    newSocket.on('audio_data', async (data) => {
      console.log('üîä Audio data received, size:', data.data?.length || 0, 'bytes');
      // Call the callback if set
      if (audioDataCallback) {
        audioDataCallback(data);
      }
    });

    newSocket.on('info', (data) => {
      console.log('‚ÑπÔ∏è Info:', data.message);
    });

    newSocket.on('error', (data) => {
      console.error('‚ùå Error:', data.message);
    });

    setSocket(newSocket);

    return () => {
      console.log('üßπ Cleaning up socket connection');
      newSocket.close();
    };
  }, []);

  const startSession = () => {
    if (socket && connected) {
      socket.emit('start_session', { mode: 'camera' });
    }
  };

  const stopSession = () => {
    if (socket && sessionActive) {
      socket.emit('stop_session');
    }
  };

  const sendAudio = (audioData: string) => {
    if (socket && sessionActive && audioData) {
      socket.emit('send_audio', { data: audioData });
    }
  };

  const sendMessage = (text: string) => {
    if (socket && sessionActive && text.trim()) {
      console.log('üì§ Sending text message:', text);
      socket.emit('send_message', { text: text.trim() });
    }
  };

  const setOnAudioData = (callback: ((data: { data: string }) => void) | undefined) => {
    setAudioDataCallback(() => callback);
  };

  return (
    <SocketContext.Provider
      value={{
        socket,
        connected,
        sessionActive,
        startSession,
        stopSession,
        sendAudio,
        sendMessage,
        onAudioData: audioDataCallback,
      }}>
      {children}
    </SocketContext.Provider>
  );
}

export function useSocket() {
  const context = useContext(SocketContext);
  if (context === undefined) {
    throw new Error('useSocket must be used within a SocketProvider');
  }
  return context;
}


```

```typescriptreact
import React, { useState, useRef, useEffect } from 'react';
import { StyleSheet, View, TouchableOpacity, Platform, Alert } from 'react-native';
import { CameraView, CameraType, useCameraPermissions } from 'expo-camera';
import { Audio } from 'expo-av';
import { ThemedText } from '@/components/themed-text';
import { ThemedView } from '@/components/themed-view';
import { useSocket } from '@/contexts/SocketContext';
import { useColorScheme } from '@/hooks/use-color-scheme';
import { Colors } from '@/constants/theme';

export default function HomeScreen() {
  const colorScheme = useColorScheme();
  const { connected, sessionActive, startSession, stopSession, sendAudio, sendMessage, socket } = useSocket();
  const [showTextInput, setShowTextInput] = useState(false);
  const [textInput, setTextInput] = useState('');
  const [facing, setFacing] = useState<CameraType>('back');
  const [permission, requestPermission] = useCameraPermissions();
  const [audioPermission, setAudioPermission] = useState<boolean>(false);
  const cameraRef = useRef<CameraView>(null);
  const recordingRef = useRef<Audio.Recording | null>(null);
  const soundRef = useRef<Audio.Sound | null>(null);
  const audioIntervalRef = useRef<ReturnType<typeof setInterval> | null>(null);

  useEffect(() => {
    // Request camera permission on mount
    if (!permission?.granted) {
      requestPermission();
    }

    // Request audio permission
    Audio.requestPermissionsAsync().then(({ status }) => {
      setAudioPermission(status === 'granted');
    });

    // Configure audio mode
    Audio.setAudioModeAsync({
      allowsRecordingIOS: true,
      playsInSilentModeIOS: true,
    });

    return () => {
      // Cleanup
      if (recordingRef.current) {
        recordingRef.current.stopAndUnloadAsync();
      }
      if (soundRef.current) {
        soundRef.current.unloadAsync();
      }
      if (audioIntervalRef.current) {
        clearInterval(audioIntervalRef.current);
      }
    };
  }, [permission, requestPermission]);

  const startAudioRecording = async () => {
    try {
      if (!audioPermission) {
        const { status } = await Audio.requestPermissionsAsync();
        if (status !== 'granted') {
          console.warn('Microphone permission not granted');
          return;
        }
        setAudioPermission(true);
      }

      // Start recording with status callback to send audio chunks
      const { recording } = await Audio.Recording.createAsync(
        Audio.RecordingOptionsPresets.LOW_QUALITY,
        async (status) => {
          // This callback is called periodically during recording
          if (status.isRecording && status.durationMillis && status.durationMillis > 0) {
            // Every 200ms, try to get and send audio data
            // Note: This is a workaround since expo-av doesn't provide direct PCM access
            // In production, you'd want to use a library that provides raw audio chunks
          }
        },
        200 // Update every 200ms
      );
      recordingRef.current = recording;
      console.log('‚úÖ Audio recording started');

      // Periodically send audio chunks (simplified approach)
      // Note: expo-av records to compressed format, so this won't work perfectly
      // For real-time audio, you'd need expo-audio or a native module
      audioIntervalRef.current = setInterval(async () => {
        if (recordingRef.current && sessionActive) {
          try {
            // Get recording status
            const status = await recordingRef.current.getStatusAsync();
            if (status.isRecording && status.durationMillis) {
              // For now, we can't easily extract PCM chunks from expo-av
              // The backend will work with text messages or when PyAudio is fixed
              // This is a placeholder for future implementation
            }
          } catch (error) {
            console.error('Error getting recording status:', error);
          }
        }
      }, 200); // Check every 200ms
    } catch (error) {
      console.error('Failed to start recording:', error);
    }
  };

  const stopAudioRecording = async () => {
    try {
      if (audioIntervalRef.current) {
        clearInterval(audioIntervalRef.current);
        audioIntervalRef.current = null;
      }

      if (recordingRef.current) {
        await recordingRef.current.stopAndUnloadAsync();
        recordingRef.current = null;
      }
    } catch (error) {
      console.error('Failed to stop recording:', error);
    }
  };

  const handleStartSession = async () => {
    if (!connected) {
      Alert.alert('Not Connected', 'Please wait for connection to server');
      return;
    }
    if (!permission?.granted) {
      Alert.alert('Camera Permission', 'Please grant camera permission to start session');
      requestPermission();
      return;
    }
    startSession();
    // Start audio recording when session starts
    setTimeout(() => {
      startAudioRecording();
    }, 500);
  };

  const handleStopSession = async () => {
    await stopAudioRecording();
    stopSession();
  };

  // Handle audio responses from backend
  useEffect(() => {
    if (!socket || !sessionActive) return;

    const handleAudioData = async (data: { data: string }) => {
      try {
        console.log('üîä Audio data received, length:', data.data?.length || 0);
        
        if (!data.data || data.data.length === 0) {
          console.warn('Empty audio data received');
          return;
        }

        // Decode base64 audio data
        let audioArray: Uint8Array;
        try {
          const audioBytes = atob(data.data);
          audioArray = new Uint8Array(audioBytes.length);
          for (let i = 0; i < audioBytes.length; i++) {
            audioArray[i] = audioBytes.charCodeAt(i);
          }
          console.log('‚úÖ Decoded audio data, size:', audioArray.length, 'bytes');
        } catch (error) {
          console.error('‚ùå Error decoding base64 audio:', error);
          return;
        }

        // Create a temporary file URI for the audio
        // Note: Backend sends PCM audio at 24kHz, mono
        // We need to create a playable format
        // For now, let's try to play it directly using expo-av
        
        // Stop any currently playing sound
        if (soundRef.current) {
          try {
            await soundRef.current.unloadAsync();
          } catch (e) {
            // Ignore errors
          }
        }

        // Convert PCM to a format expo-av can play
        // This is a simplified approach - in production you'd want proper PCM handling
        // For now, we'll create a WAV file from the PCM data
        const sampleRate = 24000; // Backend uses 24kHz for output
        const numChannels = 1;
        const bitsPerSample = 16;
        
        // Create WAV header
        const wavHeader = new ArrayBuffer(44);
        const view = new DataView(wavHeader);
        
        // RIFF header
        view.setUint8(0, 'R'.charCodeAt(0));
        view.setUint8(1, 'I'.charCodeAt(0));
        view.setUint8(2, 'F'.charCodeAt(0));
        view.setUint8(3, 'F'.charCodeAt(0));
        view.setUint32(4, 36 + audioArray.length, true);
        view.setUint8(8, 'W'.charCodeAt(0));
        view.setUint8(9, 'A'.charCodeAt(0));
        view.setUint8(10, 'V'.charCodeAt(0));
        view.setUint8(11, 'E'.charCodeAt(0));
        
        // fmt chunk
        view.setUint8(12, 'f'.charCodeAt(0));
        view.setUint8(13, 'm'.charCodeAt(0));
        view.setUint8(14, 't'.charCodeAt(0));
        view.setUint8(15, ' '.charCodeAt(0));
        view.setUint32(16, 16, true); // fmt chunk size
        view.setUint16(20, 1, true); // audio format (PCM)
        view.setUint16(22, numChannels, true);
        view.setUint32(24, sampleRate, true);
        view.setUint32(28, sampleRate * numChannels * bitsPerSample / 8, true); // byte rate
        view.setUint16(32, numChannels * bitsPerSample / 8, true); // block align
        view.setUint16(34, bitsPerSample, true);
        
        // data chunk
        view.setUint8(36, 'd'.charCodeAt(0));
        view.setUint8(37, 'a'.charCodeAt(0));
        view.setUint8(38, 't'.charCodeAt(0));
        view.setUint8(39, 'a'.charCodeAt(0));
        view.setUint32(40, audioArray.length, true);
        
        // Combine header and audio data
        const wavData = new Uint8Array(wavHeader.byteLength + audioArray.length);
        wavData.set(new Uint8Array(wavHeader), 0);
        wavData.set(audioArray, wavHeader.byteLength);
        
        // Convert to base64 data URI
        let base64: string;
        try {
          // Use a more efficient method for large arrays
          const binaryString = Array.from(wavData)
            .map(byte => String.fromCharCode(byte))
            .join('');
          base64 = btoa(binaryString);
        } catch (error) {
          console.error('‚ùå Error encoding to base64:', error);
          return;
        }
        
        const dataUri = `data:audio/wav;base64,${base64}`;
        console.log('‚úÖ Created WAV data URI, length:', dataUri.length);
        
        // Stop any currently playing sound
        if (soundRef.current) {
          try {
            const status = await soundRef.current.getStatusAsync();
            if (status.isLoaded) {
              await soundRef.current.stopAsync();
            }
            await soundRef.current.unloadAsync();
          } catch (e) {
            // Ignore errors
          }
          soundRef.current = null;
        }
        
        // Create and play sound
        console.log('üéµ Creating audio sound...');
        const { sound } = await Audio.Sound.createAsync(
          { uri: dataUri },
          { 
            shouldPlay: true, 
            volume: 1.0,
            isMuted: false,
          }
        );
        soundRef.current = sound;
        console.log('‚úÖ Audio sound created and playing');
        
        // Clean up when done
        sound.setOnPlaybackStatusUpdate((status) => {
          if (!status.isLoaded) return;
          if (status.didJustFinish) {
            console.log('‚úÖ Audio playback finished');
            sound.unloadAsync();
            soundRef.current = null;
          }
        });
      } catch (error) {
        console.error('Error playing audio response:', error);
      }
    };

    socket.on('audio_data', handleAudioData);

    return () => {
      socket.off('audio_data', handleAudioData);
    };
  }, [socket, sessionActive]);

  const colors = Colors[colorScheme ?? 'light'];

  if (!permission) {
    return (
      <ThemedView style={styles.container}>
        <ThemedText style={styles.loadingText}>Requesting camera permission...</ThemedText>
      </ThemedView>
    );
  }

  if (!permission.granted) {
    return (
      <ThemedView style={styles.container}>
        <ThemedText style={styles.permissionText}>Camera permission is required</ThemedText>
        <TouchableOpacity
          style={[styles.permissionButton, { backgroundColor: colors.tint }]}
          onPress={requestPermission}>
          <ThemedText style={styles.permissionButtonText}>Grant Permission</ThemedText>
        </TouchableOpacity>
      </ThemedView>
    );
  }

  return (
    <ThemedView style={styles.container}>
      <CameraView
        ref={cameraRef}
        style={styles.camera}
        facing={facing}
        mode="picture"
      />

      {/* Minimal connection indicator */}
      <View style={styles.connectionIndicator}>
        <View
          style={[
            styles.connectionDot,
            { backgroundColor: connected ? '#10b981' : '#ef4444' },
          ]}
        />
        <ThemedText style={styles.connectionText}>
          {connected ? 'Connected' : 'Connecting...'}
        </ThemedText>
      </View>

      <View style={styles.controls}>
        <TouchableOpacity
          style={[
            styles.button,
            styles.startButton,
            (!connected || sessionActive) && styles.buttonDisabled,
            { backgroundColor: connected && !sessionActive ? '#10b981' : '#6b7280' },
          ]}
          onPress={handleStartSession}
          disabled={!connected || sessionActive}>
          <ThemedText style={styles.buttonText}>Start</ThemedText>
        </TouchableOpacity>

        <TouchableOpacity
          style={[
            styles.button,
            styles.stopButton,
            !sessionActive && styles.buttonDisabled,
            { backgroundColor: sessionActive ? '#ef4444' : '#6b7280' },
          ]}
          onPress={handleStopSession}
          disabled={!sessionActive}>
          <ThemedText style={styles.buttonText}>Stop</ThemedText>
        </TouchableOpacity>
      </View>
    </ThemedView>
  );
}

const styles = StyleSheet.create({
  container: {
    flex: 1,
    backgroundColor: '#000',
  },
  camera: {
    flex: 1,
  },
  controls: {
    position: 'absolute',
    bottom: 0,
    left: 0,
    right: 0,
    flexDirection: 'row',
    justifyContent: 'center',
    alignItems: 'center',
    paddingBottom: Platform.OS === 'ios' ? 40 : 20,
    paddingHorizontal: 20,
    gap: 20,
  },
  button: {
    flex: 1,
    maxWidth: 150,
    paddingVertical: 16,
    paddingHorizontal: 32,
    borderRadius: 12,
    alignItems: 'center',
    justifyContent: 'center',
  },
  startButton: {
    backgroundColor: '#10b981',
  },
  stopButton: {
    backgroundColor: '#ef4444',
  },
  buttonDisabled: {
    opacity: 0.5,
  },
  buttonText: {
    color: '#fff',
    fontSize: 18,
    fontWeight: '600',
  },
  loadingText: {
    color: '#fff',
    fontSize: 16,
    textAlign: 'center',
    marginTop: 100,
  },
  permissionText: {
    color: '#fff',
    fontSize: 18,
    textAlign: 'center',
    marginBottom: 20,
    marginTop: 100,
  },
  permissionButton: {
    paddingVertical: 16,
    paddingHorizontal: 32,
    borderRadius: 12,
    alignSelf: 'center',
  },
  permissionButtonText: {
    color: '#fff',
    fontSize: 18,
    fontWeight: '600',
  },
  connectionIndicator: {
    position: 'absolute',
    top: Platform.OS === 'ios' ? 60 : 40,
    left: 20,
    flexDirection: 'row',
    alignItems: 'center',
    gap: 8,
    backgroundColor: 'rgba(0, 0, 0, 0.5)',
    paddingHorizontal: 12,
    paddingVertical: 6,
    borderRadius: 20,
  },
  connectionDot: {
    width: 8,
    height: 8,
    borderRadius: 4,
  },
  connectionText: {
    color: '#fff',
    fontSize: 12,
    fontWeight: '500',
  },
});

```

```typescriptreact
import React, { useState, useRef, useEffect } from 'react';
import { StyleSheet, View, TouchableOpacity, Platform, Alert } from 'react-native';
import { CameraView, CameraType, useCameraPermissions } from 'expo-camera';
import { Audio } from 'expo-av';
import { ThemedText } from '@/components/themed-text';
import { ThemedView } from '@/components/themed-view';
import { useSocket } from '@/contexts/SocketContext';
import { useColorScheme } from '@/hooks/use-color-scheme';
import { Colors } from '@/constants/theme';

export default function HomeScreen() {
  const colorScheme = useColorScheme();
  const { connected, sessionActive, startSession, stopSession, sendAudio, sendMessage, socket } = useSocket();
  const [showTextInput, setShowTextInput] = useState(false);
  const [textInput, setTextInput] = useState('');
  const [facing, setFacing] = useState<CameraType>('back');
  const [permission, requestPermission] = useCameraPermissions();
  const [audioPermission, setAudioPermission] = useState<boolean>(false);
  const cameraRef = useRef<CameraView>(null);
  const recordingRef = useRef<Audio.Recording | null>(null);
  const soundRef = useRef<Audio.Sound | null>(null);
  const audioIntervalRef = useRef<ReturnType<typeof setInterval> | null>(null);

  useEffect(() => {
    // Request camera permission on mount
    if (!permission?.granted) {
      requestPermission();
    }

    // Request audio permission
    Audio.requestPermissionsAsync().then(({ status }) => {
      setAudioPermission(status === 'granted');
    });

    // Configure audio mode
    Audio.setAudioModeAsync({
      allowsRecordingIOS: true,
      playsInSilentModeIOS: true,
    });

    return () => {
      // Cleanup
      if (recordingRef.current) {
        recordingRef.current.stopAndUnloadAsync();
      }
      if (soundRef.current) {
        soundRef.current.unloadAsync();
      }
      if (audioIntervalRef.current) {
        clearInterval(audioIntervalRef.current);
      }
    };
  }, [permission, requestPermission]);

  const startAudioRecording = async () => {
    try {
      if (!audioPermission) {
        const { status } = await Audio.requestPermissionsAsync();
        if (status !== 'granted') {
          console.warn('Microphone permission not granted');
          return;
        }
        setAudioPermission(true);
      }

      // Start recording with status callback to send audio chunks
      const { recording } = await Audio.Recording.createAsync(
        Audio.RecordingOptionsPresets.LOW_QUALITY,
        async (status) => {
          // This callback is called periodically during recording
          if (status.isRecording && status.durationMillis && status.durationMillis > 0) {
            // Every 200ms, try to get and send audio data
            // Note: This is a workaround since expo-av doesn't provide direct PCM access
            // In production, you'd want to use a library that provides raw audio chunks
          }
        },
        200 // Update every 200ms
      );
      recordingRef.current = recording;
      console.log('‚úÖ Audio recording started');

      // Periodically send audio chunks (simplified approach)
      // Note: expo-av records to compressed format, so this won't work perfectly
      // For real-time audio, you'd need expo-audio or a native module
      audioIntervalRef.current = setInterval(async () => {
        if (recordingRef.current && sessionActive) {
          try {
            // Get recording status
            const status = await recordingRef.current.getStatusAsync();
            if (status.isRecording && status.durationMillis) {
              // For now, we can't easily extract PCM chunks from expo-av
              // The backend will work with text messages or when PyAudio is fixed
              // This is a placeholder for future implementation
            }
          } catch (error) {
            console.error('Error getting recording status:', error);
          }
        }
      }, 200); // Check every 200ms
    } catch (error) {
      console.error('Failed to start recording:', error);
    }
  };

  const stopAudioRecording = async () => {
    try {
      if (audioIntervalRef.current) {
        clearInterval(audioIntervalRef.current);
        audioIntervalRef.current = null;
      }

      if (recordingRef.current) {
        await recordingRef.current.stopAndUnloadAsync();
        recordingRef.current = null;
      }
    } catch (error) {
      console.error('Failed to stop recording:', error);
    }
  };

  const handleStartSession = async () => {
    if (!connected) {
      Alert.alert('Not Connected', 'Please wait for connection to server');
      return;
    }
    if (!permission?.granted) {
      Alert.alert('Camera Permission', 'Please grant camera permission to start session');
      requestPermission();
      return;
    }
    startSession();
    // Start audio recording when session starts
    setTimeout(() => {
      startAudioRecording();
    }, 500);
  };

  const handleStopSession = async () => {
    await stopAudioRecording();
    stopSession();
  };

  // Handle audio responses from backend
  useEffect(() => {
    if (!socket || !sessionActive) return;

    const handleAudioData = async (data: { data: string }) => {
      try {
        console.log('üîä Audio data received, length:', data.data?.length || 0);
        
        if (!data.data || data.data.length === 0) {
          console.warn('Empty audio data received');
          return;
        }

        // Decode base64 audio data
        let audioArray: Uint8Array;
        try {
          const audioBytes = atob(data.data);
          audioArray = new Uint8Array(audioBytes.length);
          for (let i = 0; i < audioBytes.length; i++) {
            audioArray[i] = audioBytes.charCodeAt(i);
          }
          console.log('‚úÖ Decoded audio data, size:', audioArray.length, 'bytes');
        } catch (error) {
          console.error('‚ùå Error decoding base64 audio:', error);
          return;
        }

        // Create a temporary file URI for the audio
        // Note: Backend sends PCM audio at 24kHz, mono
        // We need to create a playable format
        // For now, let's try to play it directly using expo-av
        
        // Stop any currently playing sound
        if (soundRef.current) {
          try {
            await soundRef.current.unloadAsync();
          } catch (e) {
            // Ignore errors
          }
        }

        // Convert PCM to a format expo-av can play
        // This is a simplified approach - in production you'd want proper PCM handling
        // For now, we'll create a WAV file from the PCM data
        const sampleRate = 24000; // Backend uses 24kHz for output
        const numChannels = 1;
        const bitsPerSample = 16;
        
        // Create WAV header
        const wavHeader = new ArrayBuffer(44);
        const view = new DataView(wavHeader);
        
        // RIFF header
        view.setUint8(0, 'R'.charCodeAt(0));
        view.setUint8(1, 'I'.charCodeAt(0));
        view.setUint8(2, 'F'.charCodeAt(0));
        view.setUint8(3, 'F'.charCodeAt(0));
        view.setUint32(4, 36 + audioArray.length, true);
        view.setUint8(8, 'W'.charCodeAt(0));
        view.setUint8(9, 'A'.charCodeAt(0));
        view.setUint8(10, 'V'.charCodeAt(0));
        view.setUint8(11, 'E'.charCodeAt(0));
        
        // fmt chunk
        view.setUint8(12, 'f'.charCodeAt(0));
        view.setUint8(13, 'm'.charCodeAt(0));
        view.setUint8(14, 't'.charCodeAt(0));
        view.setUint8(15, ' '.charCodeAt(0));
        view.setUint32(16, 16, true); // fmt chunk size
        view.setUint16(20, 1, true); // audio format (PCM)
        view.setUint16(22, numChannels, true);
        view.setUint32(24, sampleRate, true);
        view.setUint32(28, sampleRate * numChannels * bitsPerSample / 8, true); // byte rate
        view.setUint16(32, numChannels * bitsPerSample / 8, true); // block align
        view.setUint16(34, bitsPerSample, true);
        
        // data chunk
        view.setUint8(36, 'd'.charCodeAt(0));
        view.setUint8(37, 'a'.charCodeAt(0));
        view.setUint8(38, 't'.charCodeAt(0));
        view.setUint8(39, 'a'.charCodeAt(0));
        view.setUint32(40, audioArray.length, true);
        
        // Combine header and audio data
        const wavData = new Uint8Array(wavHeader.byteLength + audioArray.length);
        wavData.set(new Uint8Array(wavHeader), 0);
        wavData.set(audioArray, wavHeader.byteLength);
        
        // Convert to base64 data URI
        let base64: string;
        try {
          // Use a more efficient method for large arrays
          const binaryString = Array.from(wavData)
            .map(byte => String.fromCharCode(byte))
            .join('');
          base64 = btoa(binaryString);
        } catch (error) {
          console.error('‚ùå Error encoding to base64:', error);
          return;
        }
        
        const dataUri = `data:audio/wav;base64,${base64}`;
        console.log('‚úÖ Created WAV data URI, length:', dataUri.length);
        
        // Stop any currently playing sound
        if (soundRef.current) {
          try {
            const status = await soundRef.current.getStatusAsync();
            if (status.isLoaded) {
              await soundRef.current.stopAsync();
            }
            await soundRef.current.unloadAsync();
          } catch (e) {
            // Ignore errors
          }
          soundRef.current = null;
        }
        
        // Create and play sound
        console.log('üéµ Creating audio sound...');
        const { sound } = await Audio.Sound.createAsync(
          { uri: dataUri },
          { 
            shouldPlay: true, 
            volume: 1.0,
            isMuted: false,
          }
        );
        soundRef.current = sound;
        console.log('‚úÖ Audio sound created and playing');
        
        // Clean up when done
        sound.setOnPlaybackStatusUpdate((status) => {
          if (!status.isLoaded) return;
          if (status.didJustFinish) {
            console.log('‚úÖ Audio playback finished');
            sound.unloadAsync();
            soundRef.current = null;
          }
        });
      } catch (error) {
        console.error('Error playing audio response:', error);
      }
    };

    socket.on('audio_data', handleAudioData);

    return () => {
      socket.off('audio_data', handleAudioData);
    };
  }, [socket, sessionActive]);

  const colors = Colors[colorScheme ?? 'light'];

  if (!permission) {
    return (
      <ThemedView style={styles.container}>
        <ThemedText style={styles.loadingText}>Requesting camera permission...</ThemedText>
      </ThemedView>
    );
  }

  if (!permission.granted) {
    return (
      <ThemedView style={styles.container}>
        <ThemedText style={styles.permissionText}>Camera permission is required</ThemedText>
        <TouchableOpacity
          style={[styles.permissionButton, { backgroundColor: colors.tint }]}
          onPress={requestPermission}>
          <ThemedText style={styles.permissionButtonText}>Grant Permission</ThemedText>
        </TouchableOpacity>
      </ThemedView>
    );
  }

  return (
    <ThemedView style={styles.container}>
      <CameraView
        ref={cameraRef}
        style={styles.camera}
        facing={facing}
        mode="picture"
      />

      {/* Minimal connection indicator */}
      <View style={styles.connectionIndicator}>
        <View
          style={[
            styles.connectionDot,
            { backgroundColor: connected ? '#10b981' : '#ef4444' },
          ]}
        />
        <ThemedText style={styles.connectionText}>
          {connected ? 'Connected' : 'Connecting...'}
        </ThemedText>
      </View>

      <View style={styles.controls}>
        {sessionActive && (
          <TouchableOpacity
            style={styles.textButton}
            onPress={() => setShowTextInput(!showTextInput)}>
            <ThemedText style={styles.textButtonText}>üí¨</ThemedText>
          </TouchableOpacity>
        )}
        
        <TouchableOpacity
          style={[
            styles.button,
            styles.startButton,
            (!connected || sessionActive) && styles.buttonDisabled,
            { backgroundColor: connected && !sessionActive ? '#10b981' : '#6b7280' },
          ]}
          onPress={handleStartSession}
          disabled={!connected || sessionActive}>
          <ThemedText style={styles.buttonText}>Start</ThemedText>
        </TouchableOpacity>

        <TouchableOpacity
          style={[
            styles.button,
            styles.stopButton,
            !sessionActive && styles.buttonDisabled,
            { backgroundColor: sessionActive ? '#ef4444' : '#6b7280' },
          ]}
          onPress={handleStopSession}
          disabled={!sessionActive}>
          <ThemedText style={styles.buttonText}>Stop</ThemedText>
        </TouchableOpacity>
      </View>

      {showTextInput && sessionActive && (
        <View style={styles.textInputContainer}>
          <TextInput
            style={styles.textInput}
            placeholder="Type a message to test audio..."
            placeholderTextColor="#999"
            value={textInput}
            onChangeText={setTextInput}
            onSubmitEditing={() => {
              if (textInput.trim()) {
                sendMessage(textInput);
                setTextInput('');
                setShowTextInput(false);
              }
            }}
            autoFocus
          />
          <TouchableOpacity
            style={styles.sendTextButton}
            onPress={() => {
              if (textInput.trim()) {
                sendMessage(textInput);
                setTextInput('');
                setShowTextInput(false);
              }
            }}>
            <ThemedText style={styles.sendTextButtonText}>Send</ThemedText>
          </TouchableOpacity>
        </View>
      )}
    </ThemedView>
  );
}

const styles = StyleSheet.create({
  container: {
    flex: 1,
    backgroundColor: '#000',
  },
  camera: {
    flex: 1,
  },
  controls: {
    position: 'absolute',
    bottom: 0,
    left: 0,
    right: 0,
    flexDirection: 'row',
    justifyContent: 'center',
    alignItems: 'center',
    paddingBottom: Platform.OS === 'ios' ? 40 : 20,
    paddingHorizontal: 20,
    gap: 20,
  },
  button: {
    flex: 1,
    maxWidth: 150,
    paddingVertical: 16,
    paddingHorizontal: 32,
    borderRadius: 12,
    alignItems: 'center',
    justifyContent: 'center',
  },
  startButton: {
    backgroundColor: '#10b981',
  },
  stopButton: {
    backgroundColor: '#ef4444',
  },
  buttonDisabled: {
    opacity: 0.5,
  },
  buttonText: {
    color: '#fff',
    fontSize: 18,
    fontWeight: '600',
  },
  loadingText: {
    color: '#fff',
    fontSize: 16,
    textAlign: 'center',
    marginTop: 100,
  },
  permissionText: {
    color: '#fff',
    fontSize: 18,
    textAlign: 'center',
    marginBottom: 20,
    marginTop: 100,
  },
  permissionButton: {
    paddingVertical: 16,
    paddingHorizontal: 32,
    borderRadius: 12,
    alignSelf: 'center',
  },
  permissionButtonText: {
    color: '#fff',
    fontSize: 18,
    fontWeight: '600',
  },
  connectionIndicator: {
    position: 'absolute',
    top: Platform.OS === 'ios' ? 60 : 40,
    left: 20,
    flexDirection: 'row',
    alignItems: 'center',
    gap: 8,
    backgroundColor: 'rgba(0, 0, 0, 0.5)',
    paddingHorizontal: 12,
    paddingVertical: 6,
    borderRadius: 20,
  },
  connectionDot: {
    width: 8,
    height: 8,
    borderRadius: 4,
  },
  connectionText: {
    color: '#fff',
    fontSize: 12,
    fontWeight: '500',
  },
});

```

```typescriptreact
import React, { useState, useRef, useEffect } from 'react';
import { StyleSheet, View, TouchableOpacity, Platform, Alert, TextInput } from 'react-native';
import { CameraView, CameraType, useCameraPermissions } from 'expo-camera';
import { Audio } from 'expo-av';
import { ThemedText } from '@/components/themed-text';
import { ThemedView } from '@/components/themed-view';
import { useSocket } from '@/contexts/SocketContext';
import { useColorScheme } from '@/hooks/use-color-scheme';
import { Colors } from '@/constants/theme';

export default function HomeScreen() {
  const colorScheme = useColorScheme();
  const { connected, sessionActive, startSession, stopSession, sendAudio, sendMessage, socket } = useSocket();
  const [showTextInput, setShowTextInput] = useState(false);
  const [textInput, setTextInput] = useState('');
  const [facing, setFacing] = useState<CameraType>('back');
  const [permission, requestPermission] = useCameraPermissions();
  const [audioPermission, setAudioPermission] = useState<boolean>(false);
  const cameraRef = useRef<CameraView>(null);
  const recordingRef = useRef<Audio.Recording | null>(null);
  const soundRef = useRef<Audio.Sound | null>(null);
  const audioIntervalRef = useRef<ReturnType<typeof setInterval> | null>(null);

  useEffect(() => {
    // Request camera permission on mount
    if (!permission?.granted) {
      requestPermission();
    }

    // Request audio permission
    Audio.requestPermissionsAsync().then(({ status }) => {
      setAudioPermission(status === 'granted');
    });

    // Configure audio mode
    Audio.setAudioModeAsync({
      allowsRecordingIOS: true,
      playsInSilentModeIOS: true,
    });

    return () => {
      // Cleanup
      if (recordingRef.current) {
        recordingRef.current.stopAndUnloadAsync();
      }
      if (soundRef.current) {
        soundRef.current.unloadAsync();
      }
      if (audioIntervalRef.current) {
        clearInterval(audioIntervalRef.current);
      }
    };
  }, [permission, requestPermission]);

  const startAudioRecording = async () => {
    try {
      if (!audioPermission) {
        const { status } = await Audio.requestPermissionsAsync();
        if (status !== 'granted') {
          console.warn('Microphone permission not granted');
          return;
        }
        setAudioPermission(true);
      }

      // Start recording with status callback to send audio chunks
      const { recording } = await Audio.Recording.createAsync(
        Audio.RecordingOptionsPresets.LOW_QUALITY,
        async (status) => {
          // This callback is called periodically during recording
          if (status.isRecording && status.durationMillis && status.durationMillis > 0) {
            // Every 200ms, try to get and send audio data
            // Note: This is a workaround since expo-av doesn't provide direct PCM access
            // In production, you'd want to use a library that provides raw audio chunks
          }
        },
        200 // Update every 200ms
      );
      recordingRef.current = recording;
      console.log('‚úÖ Audio recording started');

      // Periodically send audio chunks (simplified approach)
      // Note: expo-av records to compressed format, so this won't work perfectly
      // For real-time audio, you'd need expo-audio or a native module
      audioIntervalRef.current = setInterval(async () => {
        if (recordingRef.current && sessionActive) {
          try {
            // Get recording status
            const status = await recordingRef.current.getStatusAsync();
            if (status.isRecording && status.durationMillis) {
              // For now, we can't easily extract PCM chunks from expo-av
              // The backend will work with text messages or when PyAudio is fixed
              // This is a placeholder for future implementation
            }
          } catch (error) {
            console.error('Error getting recording status:', error);
          }
        }
      }, 200); // Check every 200ms
    } catch (error) {
      console.error('Failed to start recording:', error);
    }
  };

  const stopAudioRecording = async () => {
    try {
      if (audioIntervalRef.current) {
        clearInterval(audioIntervalRef.current);
        audioIntervalRef.current = null;
      }

      if (recordingRef.current) {
        await recordingRef.current.stopAndUnloadAsync();
        recordingRef.current = null;
      }
    } catch (error) {
      console.error('Failed to stop recording:', error);
    }
  };

  const handleStartSession = async () => {
    if (!connected) {
      Alert.alert('Not Connected', 'Please wait for connection to server');
      return;
    }
    if (!permission?.granted) {
      Alert.alert('Camera Permission', 'Please grant camera permission to start session');
      requestPermission();
      return;
    }
    startSession();
    // Start audio recording when session starts
    setTimeout(() => {
      startAudioRecording();
    }, 500);
  };

  const handleStopSession = async () => {
    await stopAudioRecording();
    stopSession();
  };

  // Handle audio responses from backend
  useEffect(() => {
    if (!socket || !sessionActive) return;

    const handleAudioData = async (data: { data: string }) => {
      try {
        console.log('üîä Audio data received, length:', data.data?.length || 0);
        
        if (!data.data || data.data.length === 0) {
          console.warn('Empty audio data received');
          return;
        }

        // Decode base64 audio data
        let audioArray: Uint8Array;
        try {
          const audioBytes = atob(data.data);
          audioArray = new Uint8Array(audioBytes.length);
          for (let i = 0; i < audioBytes.length; i++) {
            audioArray[i] = audioBytes.charCodeAt(i);
          }
          console.log('‚úÖ Decoded audio data, size:', audioArray.length, 'bytes');
        } catch (error) {
          console.error('‚ùå Error decoding base64 audio:', error);
          return;
        }

        // Create a temporary file URI for the audio
        // Note: Backend sends PCM audio at 24kHz, mono
        // We need to create a playable format
        // For now, let's try to play it directly using expo-av
        
        // Stop any currently playing sound
        if (soundRef.current) {
          try {
            await soundRef.current.unloadAsync();
          } catch (e) {
            // Ignore errors
          }
        }

        // Convert PCM to a format expo-av can play
        // This is a simplified approach - in production you'd want proper PCM handling
        // For now, we'll create a WAV file from the PCM data
        const sampleRate = 24000; // Backend uses 24kHz for output
        const numChannels = 1;
        const bitsPerSample = 16;
        
        // Create WAV header
        const wavHeader = new ArrayBuffer(44);
        const view = new DataView(wavHeader);
        
        // RIFF header
        view.setUint8(0, 'R'.charCodeAt(0));
        view.setUint8(1, 'I'.charCodeAt(0));
        view.setUint8(2, 'F'.charCodeAt(0));
        view.setUint8(3, 'F'.charCodeAt(0));
        view.setUint32(4, 36 + audioArray.length, true);
        view.setUint8(8, 'W'.charCodeAt(0));
        view.setUint8(9, 'A'.charCodeAt(0));
        view.setUint8(10, 'V'.charCodeAt(0));
        view.setUint8(11, 'E'.charCodeAt(0));
        
        // fmt chunk
        view.setUint8(12, 'f'.charCodeAt(0));
        view.setUint8(13, 'm'.charCodeAt(0));
        view.setUint8(14, 't'.charCodeAt(0));
        view.setUint8(15, ' '.charCodeAt(0));
        view.setUint32(16, 16, true); // fmt chunk size
        view.setUint16(20, 1, true); // audio format (PCM)
        view.setUint16(22, numChannels, true);
        view.setUint32(24, sampleRate, true);
        view.setUint32(28, sampleRate * numChannels * bitsPerSample / 8, true); // byte rate
        view.setUint16(32, numChannels * bitsPerSample / 8, true); // block align
        view.setUint16(34, bitsPerSample, true);
        
        // data chunk
        view.setUint8(36, 'd'.charCodeAt(0));
        view.setUint8(37, 'a'.charCodeAt(0));
        view.setUint8(38, 't'.charCodeAt(0));
        view.setUint8(39, 'a'.charCodeAt(0));
        view.setUint32(40, audioArray.length, true);
        
        // Combine header and audio data
        const wavData = new Uint8Array(wavHeader.byteLength + audioArray.length);
        wavData.set(new Uint8Array(wavHeader), 0);
        wavData.set(audioArray, wavHeader.byteLength);
        
        // Convert to base64 data URI
        let base64: string;
        try {
          // Use a more efficient method for large arrays
          const binaryString = Array.from(wavData)
            .map(byte => String.fromCharCode(byte))
            .join('');
          base64 = btoa(binaryString);
        } catch (error) {
          console.error('‚ùå Error encoding to base64:', error);
          return;
        }
        
        const dataUri = `data:audio/wav;base64,${base64}`;
        console.log('‚úÖ Created WAV data URI, length:', dataUri.length);
        
        // Stop any currently playing sound
        if (soundRef.current) {
          try {
            const status = await soundRef.current.getStatusAsync();
            if (status.isLoaded) {
              await soundRef.current.stopAsync();
            }
            await soundRef.current.unloadAsync();
          } catch (e) {
            // Ignore errors
          }
          soundRef.current = null;
        }
        
        // Create and play sound
        console.log('üéµ Creating audio sound...');
        const { sound } = await Audio.Sound.createAsync(
          { uri: dataUri },
          { 
            shouldPlay: true, 
            volume: 1.0,
            isMuted: false,
          }
        );
        soundRef.current = sound;
        console.log('‚úÖ Audio sound created and playing');
        
        // Clean up when done
        sound.setOnPlaybackStatusUpdate((status) => {
          if (!status.isLoaded) return;
          if (status.didJustFinish) {
            console.log('‚úÖ Audio playback finished');
            sound.unloadAsync();
            soundRef.current = null;
          }
        });
      } catch (error) {
        console.error('Error playing audio response:', error);
      }
    };

    socket.on('audio_data', handleAudioData);

    return () => {
      socket.off('audio_data', handleAudioData);
    };
  }, [socket, sessionActive]);

  const colors = Colors[colorScheme ?? 'light'];

  if (!permission) {
    return (
      <ThemedView style={styles.container}>
        <ThemedText style={styles.loadingText}>Requesting camera permission...</ThemedText>
      </ThemedView>
    );
  }

  if (!permission.granted) {
    return (
      <ThemedView style={styles.container}>
        <ThemedText style={styles.permissionText}>Camera permission is required</ThemedText>
        <TouchableOpacity
          style={[styles.permissionButton, { backgroundColor: colors.tint }]}
          onPress={requestPermission}>
          <ThemedText style={styles.permissionButtonText}>Grant Permission</ThemedText>
        </TouchableOpacity>
      </ThemedView>
    );
  }

  return (
    <ThemedView style={styles.container}>
      <CameraView
        ref={cameraRef}
        style={styles.camera}
        facing={facing}
        mode="picture"
      />

      {/* Minimal connection indicator */}
      <View style={styles.connectionIndicator}>
        <View
          style={[
            styles.connectionDot,
            { backgroundColor: connected ? '#10b981' : '#ef4444' },
          ]}
        />
        <ThemedText style={styles.connectionText}>
          {connected ? 'Connected' : 'Connecting...'}
        </ThemedText>
      </View>

      <View style={styles.controls}>
        {sessionActive && (
          <TouchableOpacity
            style={styles.textButton}
            onPress={() => setShowTextInput(!showTextInput)}>
            <ThemedText style={styles.textButtonText}>üí¨</ThemedText>
          </TouchableOpacity>
        )}
        
        <TouchableOpacity
          style={[
            styles.button,
            styles.startButton,
            (!connected || sessionActive) && styles.buttonDisabled,
            { backgroundColor: connected && !sessionActive ? '#10b981' : '#6b7280' },
          ]}
          onPress={handleStartSession}
          disabled={!connected || sessionActive}>
          <ThemedText style={styles.buttonText}>Start</ThemedText>
        </TouchableOpacity>

        <TouchableOpacity
          style={[
            styles.button,
            styles.stopButton,
            !sessionActive && styles.buttonDisabled,
            { backgroundColor: sessionActive ? '#ef4444' : '#6b7280' },
          ]}
          onPress={handleStopSession}
          disabled={!sessionActive}>
          <ThemedText style={styles.buttonText}>Stop</ThemedText>
        </TouchableOpacity>
      </View>

      {showTextInput && sessionActive && (
        <View style={styles.textInputContainer}>
          <TextInput
            style={styles.textInput}
            placeholder="Type a message to test audio..."
            placeholderTextColor="#999"
            value={textInput}
            onChangeText={setTextInput}
            onSubmitEditing={() => {
              if (textInput.trim()) {
                sendMessage(textInput);
                setTextInput('');
                setShowTextInput(false);
              }
            }}
            autoFocus
          />
          <TouchableOpacity
            style={styles.sendTextButton}
            onPress={() => {
              if (textInput.trim()) {
                sendMessage(textInput);
                setTextInput('');
                setShowTextInput(false);
              }
            }}>
            <ThemedText style={styles.sendTextButtonText}>Send</ThemedText>
          </TouchableOpacity>
        </View>
      )}
    </ThemedView>
  );
}

const styles = StyleSheet.create({
  container: {
    flex: 1,
    backgroundColor: '#000',
  },
  camera: {
    flex: 1,
  },
  controls: {
    position: 'absolute',
    bottom: 0,
    left: 0,
    right: 0,
    flexDirection: 'row',
    justifyContent: 'center',
    alignItems: 'center',
    paddingBottom: Platform.OS === 'ios' ? 40 : 20,
    paddingHorizontal: 20,
    gap: 20,
  },
  button: {
    flex: 1,
    maxWidth: 150,
    paddingVertical: 16,
    paddingHorizontal: 32,
    borderRadius: 12,
    alignItems: 'center',
    justifyContent: 'center',
  },
  startButton: {
    backgroundColor: '#10b981',
  },
  stopButton: {
    backgroundColor: '#ef4444',
  },
  buttonDisabled: {
    opacity: 0.5,
  },
  buttonText: {
    color: '#fff',
    fontSize: 18,
    fontWeight: '600',
  },
  loadingText: {
    color: '#fff',
    fontSize: 16,
    textAlign: 'center',
    marginTop: 100,
  },
  permissionText: {
    color: '#fff',
    fontSize: 18,
    textAlign: 'center',
    marginBottom: 20,
    marginTop: 100,
  },
  permissionButton: {
    paddingVertical: 16,
    paddingHorizontal: 32,
    borderRadius: 12,
    alignSelf: 'center',
  },
  permissionButtonText: {
    color: '#fff',
    fontSize: 18,
    fontWeight: '600',
  },
  connectionIndicator: {
    position: 'absolute',
    top: Platform.OS === 'ios' ? 60 : 40,
    left: 20,
    flexDirection: 'row',
    alignItems: 'center',
    gap: 8,
    backgroundColor: 'rgba(0, 0, 0, 0.5)',
    paddingHorizontal: 12,
    paddingVertical: 6,
    borderRadius: 20,
  },
  connectionDot: {
    width: 8,
    height: 8,
    borderRadius: 4,
  },
  connectionText: {
    color: '#fff',
    fontSize: 12,
    fontWeight: '500',
  },
});

```

```typescriptreact
import React, { useState, useRef, useEffect } from 'react';
import { StyleSheet, View, TouchableOpacity, Platform, Alert, TextInput } from 'react-native';
import { CameraView, CameraType, useCameraPermissions } from 'expo-camera';
import { Audio } from 'expo-av';
import { ThemedText } from '@/components/themed-text';
import { ThemedView } from '@/components/themed-view';
import { useSocket } from '@/contexts/SocketContext';
import { useColorScheme } from '@/hooks/use-color-scheme';
import { Colors } from '@/constants/theme';

export default function HomeScreen() {
  const colorScheme = useColorScheme();
  const { connected, sessionActive, startSession, stopSession, sendAudio, sendMessage, socket } = useSocket();
  const [showTextInput, setShowTextInput] = useState(false);
  const [textInput, setTextInput] = useState('');
  const [facing, setFacing] = useState<CameraType>('back');
  const [permission, requestPermission] = useCameraPermissions();
  const [audioPermission, setAudioPermission] = useState<boolean>(false);
  const cameraRef = useRef<CameraView>(null);
  const recordingRef = useRef<Audio.Recording | null>(null);
  const soundRef = useRef<Audio.Sound | null>(null);
  const audioIntervalRef = useRef<ReturnType<typeof setInterval> | null>(null);

  useEffect(() => {
    // Request camera permission on mount
    if (!permission?.granted) {
      requestPermission();
    }

    // Request audio permission
    Audio.requestPermissionsAsync().then(({ status }) => {
      setAudioPermission(status === 'granted');
    });

    // Configure audio mode
    Audio.setAudioModeAsync({
      allowsRecordingIOS: true,
      playsInSilentModeIOS: true,
    });

    return () => {
      // Cleanup
      if (recordingRef.current) {
        recordingRef.current.stopAndUnloadAsync();
      }
      if (soundRef.current) {
        soundRef.current.unloadAsync();
      }
      if (audioIntervalRef.current) {
        clearInterval(audioIntervalRef.current);
      }
    };
  }, [permission, requestPermission]);

  const startAudioRecording = async () => {
    try {
      if (!audioPermission) {
        const { status } = await Audio.requestPermissionsAsync();
        if (status !== 'granted') {
          console.warn('Microphone permission not granted');
          return;
        }
        setAudioPermission(true);
      }

      // Start recording with status callback to send audio chunks
      const { recording } = await Audio.Recording.createAsync(
        Audio.RecordingOptionsPresets.LOW_QUALITY,
        async (status) => {
          // This callback is called periodically during recording
          if (status.isRecording && status.durationMillis && status.durationMillis > 0) {
            // Every 200ms, try to get and send audio data
            // Note: This is a workaround since expo-av doesn't provide direct PCM access
            // In production, you'd want to use a library that provides raw audio chunks
          }
        },
        200 // Update every 200ms
      );
      recordingRef.current = recording;
      console.log('‚úÖ Audio recording started');

      // Periodically send audio chunks (simplified approach)
      // Note: expo-av records to compressed format, so this won't work perfectly
      // For real-time audio, you'd need expo-audio or a native module
      audioIntervalRef.current = setInterval(async () => {
        if (recordingRef.current && sessionActive) {
          try {
            // Get recording status
            const status = await recordingRef.current.getStatusAsync();
            if (status.isRecording && status.durationMillis) {
              // For now, we can't easily extract PCM chunks from expo-av
              // The backend will work with text messages or when PyAudio is fixed
              // This is a placeholder for future implementation
            }
          } catch (error) {
            console.error('Error getting recording status:', error);
          }
        }
      }, 200); // Check every 200ms
    } catch (error) {
      console.error('Failed to start recording:', error);
    }
  };

  const stopAudioRecording = async () => {
    try {
      if (audioIntervalRef.current) {
        clearInterval(audioIntervalRef.current);
        audioIntervalRef.current = null;
      }

      if (recordingRef.current) {
        await recordingRef.current.stopAndUnloadAsync();
        recordingRef.current = null;
      }
    } catch (error) {
      console.error('Failed to stop recording:', error);
    }
  };

  const handleStartSession = async () => {
    if (!connected) {
      Alert.alert('Not Connected', 'Please wait for connection to server');
      return;
    }
    if (!permission?.granted) {
      Alert.alert('Camera Permission', 'Please grant camera permission to start session');
      requestPermission();
      return;
    }
    startSession();
    // Start audio recording when session starts
    setTimeout(() => {
      startAudioRecording();
    }, 500);
  };

  const handleStopSession = async () => {
    await stopAudioRecording();
    stopSession();
  };

  // Handle audio responses from backend
  useEffect(() => {
    if (!socket || !sessionActive) return;

    const handleAudioData = async (data: { data: string }) => {
      try {
        console.log('üîä Audio data received, length:', data.data?.length || 0);
        
        if (!data.data || data.data.length === 0) {
          console.warn('Empty audio data received');
          return;
        }

        // Decode base64 audio data
        let audioArray: Uint8Array;
        try {
          const audioBytes = atob(data.data);
          audioArray = new Uint8Array(audioBytes.length);
          for (let i = 0; i < audioBytes.length; i++) {
            audioArray[i] = audioBytes.charCodeAt(i);
          }
          console.log('‚úÖ Decoded audio data, size:', audioArray.length, 'bytes');
        } catch (error) {
          console.error('‚ùå Error decoding base64 audio:', error);
          return;
        }

        // Create a temporary file URI for the audio
        // Note: Backend sends PCM audio at 24kHz, mono
        // We need to create a playable format
        // For now, let's try to play it directly using expo-av
        
        // Stop any currently playing sound
        if (soundRef.current) {
          try {
            await soundRef.current.unloadAsync();
          } catch (e) {
            // Ignore errors
          }
        }

        // Convert PCM to a format expo-av can play
        // This is a simplified approach - in production you'd want proper PCM handling
        // For now, we'll create a WAV file from the PCM data
        const sampleRate = 24000; // Backend uses 24kHz for output
        const numChannels = 1;
        const bitsPerSample = 16;
        
        // Create WAV header
        const wavHeader = new ArrayBuffer(44);
        const view = new DataView(wavHeader);
        
        // RIFF header
        view.setUint8(0, 'R'.charCodeAt(0));
        view.setUint8(1, 'I'.charCodeAt(0));
        view.setUint8(2, 'F'.charCodeAt(0));
        view.setUint8(3, 'F'.charCodeAt(0));
        view.setUint32(4, 36 + audioArray.length, true);
        view.setUint8(8, 'W'.charCodeAt(0));
        view.setUint8(9, 'A'.charCodeAt(0));
        view.setUint8(10, 'V'.charCodeAt(0));
        view.setUint8(11, 'E'.charCodeAt(0));
        
        // fmt chunk
        view.setUint8(12, 'f'.charCodeAt(0));
        view.setUint8(13, 'm'.charCodeAt(0));
        view.setUint8(14, 't'.charCodeAt(0));
        view.setUint8(15, ' '.charCodeAt(0));
        view.setUint32(16, 16, true); // fmt chunk size
        view.setUint16(20, 1, true); // audio format (PCM)
        view.setUint16(22, numChannels, true);
        view.setUint32(24, sampleRate, true);
        view.setUint32(28, sampleRate * numChannels * bitsPerSample / 8, true); // byte rate
        view.setUint16(32, numChannels * bitsPerSample / 8, true); // block align
        view.setUint16(34, bitsPerSample, true);
        
        // data chunk
        view.setUint8(36, 'd'.charCodeAt(0));
        view.setUint8(37, 'a'.charCodeAt(0));
        view.setUint8(38, 't'.charCodeAt(0));
        view.setUint8(39, 'a'.charCodeAt(0));
        view.setUint32(40, audioArray.length, true);
        
        // Combine header and audio data
        const wavData = new Uint8Array(wavHeader.byteLength + audioArray.length);
        wavData.set(new Uint8Array(wavHeader), 0);
        wavData.set(audioArray, wavHeader.byteLength);
        
        // Convert to base64 data URI
        let base64: string;
        try {
          // Use a more efficient method for large arrays
          const binaryString = Array.from(wavData)
            .map(byte => String.fromCharCode(byte))
            .join('');
          base64 = btoa(binaryString);
        } catch (error) {
          console.error('‚ùå Error encoding to base64:', error);
          return;
        }
        
        const dataUri = `data:audio/wav;base64,${base64}`;
        console.log('‚úÖ Created WAV data URI, length:', dataUri.length);
        
        // Stop any currently playing sound
        if (soundRef.current) {
          try {
            const status = await soundRef.current.getStatusAsync();
            if (status.isLoaded) {
              await soundRef.current.stopAsync();
            }
            await soundRef.current.unloadAsync();
          } catch (e) {
            // Ignore errors
          }
          soundRef.current = null;
        }
        
        // Create and play sound
        console.log('üéµ Creating audio sound...');
        const { sound } = await Audio.Sound.createAsync(
          { uri: dataUri },
          { 
            shouldPlay: true, 
            volume: 1.0,
            isMuted: false,
          }
        );
        soundRef.current = sound;
        console.log('‚úÖ Audio sound created and playing');
        
        // Clean up when done
        sound.setOnPlaybackStatusUpdate((status) => {
          if (!status.isLoaded) return;
          if (status.didJustFinish) {
            console.log('‚úÖ Audio playback finished');
            sound.unloadAsync();
            soundRef.current = null;
          }
        });
      } catch (error) {
        console.error('Error playing audio response:', error);
      }
    };

    socket.on('audio_data', handleAudioData);

    return () => {
      socket.off('audio_data', handleAudioData);
    };
  }, [socket, sessionActive]);

  const colors = Colors[colorScheme ?? 'light'];

  if (!permission) {
    return (
      <ThemedView style={styles.container}>
        <ThemedText style={styles.loadingText}>Requesting camera permission...</ThemedText>
      </ThemedView>
    );
  }

  if (!permission.granted) {
    return (
      <ThemedView style={styles.container}>
        <ThemedText style={styles.permissionText}>Camera permission is required</ThemedText>
        <TouchableOpacity
          style={[styles.permissionButton, { backgroundColor: colors.tint }]}
          onPress={requestPermission}>
          <ThemedText style={styles.permissionButtonText}>Grant Permission</ThemedText>
        </TouchableOpacity>
      </ThemedView>
    );
  }

  return (
    <ThemedView style={styles.container}>
      <CameraView
        ref={cameraRef}
        style={styles.camera}
        facing={facing}
        mode="picture"
      />

      {/* Minimal connection indicator */}
      <View style={styles.connectionIndicator}>
        <View
          style={[
            styles.connectionDot,
            { backgroundColor: connected ? '#10b981' : '#ef4444' },
          ]}
        />
        <ThemedText style={styles.connectionText}>
          {connected ? 'Connected' : 'Connecting...'}
        </ThemedText>
      </View>

      <View style={styles.controls}>
        {sessionActive && (
          <TouchableOpacity
            style={styles.textButton}
            onPress={() => setShowTextInput(!showTextInput)}>
            <ThemedText style={styles.textButtonText}>üí¨</ThemedText>
          </TouchableOpacity>
        )}
        
        <TouchableOpacity
          style={[
            styles.button,
            styles.startButton,
            (!connected || sessionActive) && styles.buttonDisabled,
            { backgroundColor: connected && !sessionActive ? '#10b981' : '#6b7280' },
          ]}
          onPress={handleStartSession}
          disabled={!connected || sessionActive}>
          <ThemedText style={styles.buttonText}>Start</ThemedText>
        </TouchableOpacity>

        <TouchableOpacity
          style={[
            styles.button,
            styles.stopButton,
            !sessionActive && styles.buttonDisabled,
            { backgroundColor: sessionActive ? '#ef4444' : '#6b7280' },
          ]}
          onPress={handleStopSession}
          disabled={!sessionActive}>
          <ThemedText style={styles.buttonText}>Stop</ThemedText>
        </TouchableOpacity>
      </View>

      {showTextInput && sessionActive && (
        <View style={styles.textInputContainer}>
          <TextInput
            style={styles.textInput}
            placeholder="Type a message to test audio..."
            placeholderTextColor="#999"
            value={textInput}
            onChangeText={setTextInput}
            onSubmitEditing={() => {
              if (textInput.trim()) {
                sendMessage(textInput);
                setTextInput('');
                setShowTextInput(false);
              }
            }}
            autoFocus
          />
          <TouchableOpacity
            style={styles.sendTextButton}
            onPress={() => {
              if (textInput.trim()) {
                sendMessage(textInput);
                setTextInput('');
                setShowTextInput(false);
              }
            }}>
            <ThemedText style={styles.sendTextButtonText}>Send</ThemedText>
          </TouchableOpacity>
        </View>
      )}
    </ThemedView>
  );
}

const styles = StyleSheet.create({
  container: {
    flex: 1,
    backgroundColor: '#000',
  },
  camera: {
    flex: 1,
  },
  controls: {
    position: 'absolute',
    bottom: 0,
    left: 0,
    right: 0,
    flexDirection: 'row',
    justifyContent: 'center',
    alignItems: 'center',
    paddingBottom: Platform.OS === 'ios' ? 40 : 20,
    paddingHorizontal: 20,
    gap: 20,
  },
  button: {
    flex: 1,
    maxWidth: 150,
    paddingVertical: 16,
    paddingHorizontal: 32,
    borderRadius: 12,
    alignItems: 'center',
    justifyContent: 'center',
  },
  startButton: {
    backgroundColor: '#10b981',
  },
  stopButton: {
    backgroundColor: '#ef4444',
  },
  buttonDisabled: {
    opacity: 0.5,
  },
  buttonText: {
    color: '#fff',
    fontSize: 18,
    fontWeight: '600',
  },
  loadingText: {
    color: '#fff',
    fontSize: 16,
    textAlign: 'center',
    marginTop: 100,
  },
  permissionText: {
    color: '#fff',
    fontSize: 18,
    textAlign: 'center',
    marginBottom: 20,
    marginTop: 100,
  },
  permissionButton: {
    paddingVertical: 16,
    paddingHorizontal: 32,
    borderRadius: 12,
    alignSelf: 'center',
  },
  permissionButtonText: {
    color: '#fff',
    fontSize: 18,
    fontWeight: '600',
  },
  connectionIndicator: {
    position: 'absolute',
    top: Platform.OS === 'ios' ? 60 : 40,
    left: 20,
    flexDirection: 'row',
    alignItems: 'center',
    gap: 8,
    backgroundColor: 'rgba(0, 0, 0, 0.5)',
    paddingHorizontal: 12,
    paddingVertical: 6,
    borderRadius: 20,
  },
  connectionDot: {
    width: 8,
    height: 8,
    borderRadius: 4,
  },
  connectionText: {
    color: '#fff',
    fontSize: 12,
    fontWeight: '500',
  },
  textButton: {
    position: 'absolute',
    top: -60,
    right: 20,
    width: 50,
    height: 50,
    borderRadius: 25,
    backgroundColor: 'rgba(0, 0, 0, 0.7)',
    alignItems: 'center',
    justifyContent: 'center',
  },
  textButtonText: {
    fontSize: 24,
  },
  textInputContainer: {
    position: 'absolute',
    bottom: 100,
    left: 20,
    right: 20,
    flexDirection: 'row',
    gap: 10,
    backgroundColor: 'rgba(0, 0, 0, 0.8)',
    padding: 12,
    borderRadius: 12,
  },
  textInput: {
    flex: 1,
    backgroundColor: '#fff',
    padding: 12,
    borderRadius: 8,
    color: '#000',
    fontSize: 16,
  },
  sendTextButton: {
    backgroundColor: '#667eea',
    paddingHorizontal: 20,
    paddingVertical: 12,
    borderRadius: 8,
    justifyContent: 'center',
  },
  sendTextButtonText: {
    color: '#fff',
    fontSize: 16,
    fontWeight: '600',
  },
});

```


Made these updates:

1. **Added text message input** ‚Äî Tap the üí¨ button when a session is active to send a text message and test audio output.
2. **Improved audio playback logging** ‚Äî Console logs show when audio is received and played.
3. **Fixed audio playback** ‚Äî Improved error handling and base64 encoding.

## To Test Audio Output:

1. Start a session (tap "Start").
2. Tap the üí¨ button that appears above the Start/Stop buttons.
3. Type a message (e.g., "Hello, can you hear me?") and send it.
4. Check the console logs ‚Äî you should see:
   - `üì§ Sending text message: ...`
   - `üîä Audio data received, size: ... bytes`
   - `‚úÖ Decoded audio data, size: ... bytes`
   - `üéµ Creating audio sound...`
   - `‚úÖ Audio sound created and playing`

If you see these logs but no audio plays, the issue is likely with the WAV conversion or playback. If you don't see `üîä Audio data received`, Gemini isn't sending audio (possibly because there's no audio input).

**Next steps:**
- Check the console logs when you send a text message.
- Share what you see ‚Äî this will help identify where the issue is.

The text message feature lets you test audio output even without working microphone input.

