# Building an iOS app with Google Live API
_Exported on 14/11/2025 at 23:45:52 CET from Cursor (1.7.44)_

---

**User**

I am attempting to build an ios app that uses the Google Live API to process and understand a live video stream from the camera on the phone. The idea is that you point it at a shelf in a store and ask it e.g. "where is the gluten free bread?". The point is to make it answer you e.g. "third shelf from the top, top right corner, green bag". The user input will be primarily voice, but also written if needed. 

The UI is supposed to be the simplest thing here, nothing fancy. Just one button that will activate the video stream, and give the user the option to ask the question they desire. The output will also be a vocal response providing the user clear, actionable spatial descriptions I have a working prototype achieved using python and a html file for displaying it locally on my computer. It is the transfer from this to the app I need help with. 

I currently have a working prototype that works when I boot with "python app.py" in @WebInterface/ . This is just a prototype and is not to be used in the development directly, but it is a reference point to see how the protoype works and how we interact with the model, how the video streaming works etc. @main.py is directly picked from the documentation and is the main driving force for the application. You must mainly look at this. 


The plan
Choose: Keep your existing Python Flask + Socket.IO backend that already speaks the Gemini Live API, and make the iOS app a thin, real-time capture & playback client that connects to your server over Socket.IO.

Why this path?
* It preserves your current Live API setup and configuration exactly as you’ve prototyped it in Python.
* No API keys on the device; your server stays the only place that talks to Gemini.
* iOS just sends audio + video to your server and plays the model’s audio back. Fastest to ship.
For the Live API details you’re already aligned with (send 16-bit PCM, 16 kHz, mono; receive 24 kHz PCM), keep those as-is. (Google AI for Developers)

What to install (dependencies)
Xcode (iOS side)
* I have installed the socketio dependency
No other third-party iOS libraries are required. You’ll use Apple frameworks for capture and audio:
* AVFoundation for camera (AVCaptureSession). (Apple Developer)
* AVFAudio for mic and playback (AVAudioSession with .playAndRecord and .voiceChat mode). (Apple Developer)
Server (Python side)
* Keep your current stack. You’ll only add two simple Socket.IO events to accept video frames and audio chunks from the phone (details below). Your existing Live API integration, model selection, and response modalities remain unchanged.

Prep the Python server (minimal changes)
Your current web UI only sends start/stop + text, while the server itself captures video/audio locally. For the phone to “see the shelf,” switch to client-sourced capture:
1. Keep these events as-is (you already use them):
* start_streaming { mode: "camera" | "screen" | "none" }
* stop_streaming
* send_message { message: string } Server continues to forward text to the Live API and mark end-of-turn. (Your server also keeps emitting status, ai_response, and audio_data back to clients.)
1. Add two new inbound events from iOS (no code here, just what to implement):
* video_frame { mime_type: "image/jpeg", data: <base64> } → server forwards to session.send(input={mime_type, data}). Throttle server-side to ~1–3 fps if needed. Live API handles ~1 fps well for spatial grounding. (Google Cloud)
* audio_chunk { data: <base64>, sampleRate: 16000 } → server forwards to session.send(input={mime_type:"audio/pcm", data:<bytes>}). Keep chunks around 20–60 ms worth of PCM for smoothness. (Your current Live config already outputs 24 kHz PCM—don’t change that.) (Google AI for Developers)
1. Stop server-side capture when the client is connected In your “web mode” loop, don’t spawn the local camera/mic capture tasks. Only receive from the phone and forward to the Live API.
2. Transport security
* Use WSS (TLS) for mobile testing/production to satisfy iOS ATS. For quick device tests against localhost, either tunnel with a TLS tool (e.g., ngrok) or add a temporary ATS exception (remove before shipping). (Apple Developer)
That’s it on the backend: simple event pass-throughs that call the same Live API methods you already use.

Configure the iOS project (from a fresh Xcode app)
1. Add the package
* Xcode → File → Add Package Dependencies → socketio/socket.io-client-swift → add to your target. (GitHub) (THIS IS DONE)
1. Info.plist privacy strings Add these keys with clear end-user reasons (App Store will reject otherwise):
* NSCameraUsageDescription
* NSMicrophoneUsageDescription (Apple Developer)
1. (Dev only) ATS exception if needed If you must connect to ws:// or http:// during local testing, add a temporary ATS exception. Prefer wss:// and remove the exception before TestFlight/App Store. (Apple Developer)
2. Background modes (optional for later) If you want audio to continue when locked, enable the Audio, AirPlay, and Picture in Picture background mode. (You can skip this for first build.)

Wire the one-button flow
When user taps “Start”:
1. Open Socket.IO to your server (wss://your-server/socket.io if possible). On connect:
    * Emit start_streaming { mode: "camera" }.
    * The server opens/joins the Live API session and begins awaiting frames and audio. (No local capture on server now.)
2. Configure audio for capture + playback
    * Set AVAudioSession category to .playAndRecord and mode to .voiceChat to get system AEC and voice processing tuned for two-way voice. (Apple Developer)
    * Start an AVAudioEngine with an input tap that yields audio PCM buffers.
    * Convert to mono, 16 kHz, 16-bit PCM frames of ~20–60 ms; base64 the bytes and emit audio_chunk as they arrive. (Matches Live API spec.) (Google AI for Developers)
3. Configure camera
    * Start an AVCaptureSession with a modest preset (e.g., 640×480); keep latency and battery low. (Apple Developer)
    * Sample frames sparsely (e.g., 1–3 fps), JPEG-encode, base64 and emit video_frame. Limit size to ~≤1024 px on the longer edge, exactly like your Python thumbnailing. (Live API guidance indicates low fps is fine.) (Google Cloud)
4. Handle server → app messages
    * On audio_data { data: base64, sampleRate: 24000 }: enqueue into an audio player node; iOS will resample to device rate automatically. This is 24 kHz PCM from the model. (Google AI for Developers)
    * On ai_response { text }: surface as on-screen captions; also handy for debugging what the model “thinks.”
5. User’s voice vs. typed prompts
    * If the user speaks continuously, just keep sending audio_chunk; the model’s native-audio Live variants are designed for conversational turn-taking.
    * If the user types, emit your existing send_message { message } to do an explicit end-of-turn text prompt (your server already forwards this).
When user taps “Stop”:
* Emit stop_streaming, close the capture sessions, and disconnect the socket.

Message/event schema to reuse (and extend)
You already have:
* Client → Server: start_streaming, stop_streaming, send_message
* Server → Client: status, ai_response { text }, audio_data { data, sampleRate: 24000 }
Add (client-sourced capture):
* Client → Server:
    * video_frame { mime_type: "image/jpeg", data: <base64> }
    * audio_chunk { data: <base64>, sampleRate: 16000 }
On the server side, these two just forward to session.send(...) exactly as your Python does today for locally captured frames and audio. (No model changes; same Live API payloads.) (Google AI for Developers)

Testing (quick but thorough)
1. Simulator to localhost doesn’t use your Mac camera—use a real device over LAN or via a TLS tunnel (ngrok) so the phone can connect and send frames. Prefer wss:// for realism. (Apple Developer)
2. Mic/camera prompts appear the first time. If they don’t: double-check your Info.plist purpose strings. (Apple Developer)
3. Confirm audio round-trip: speak → server forwards → Live API responds → you hear the bot at 24 kHz PCM. (Google AI for Developers)
4. Confirm video effect: point at products; ask location; verify spatial directions are grounded in the current frame.

---

**Cursor**

### Plan in Pseudocode

```
GOAL: Use existing Python Socket.IO + Gemini Live backend; build iOS client that streams mic/camera to server and plays Live API audio.

BACKEND (Python, WebInterface/app.py + main.py reuse)
1. On start_streaming:
   - create WebAudioLoop but disable local capture tasks (no cv2/sounddevice).
   - store client session id for routing.
   - emit status → “ready for client media”.
2. Add socket handlers:
   - @socketio.on('video_frame'): decode base64 → bytes; enqueue into AudioLoop.out_queue.
   - @socketio.on('audio_chunk'): decode base64 PCM → bytes; enqueue into out_queue (audio path).
3. Modify AudioLoop:
   - expose async method accept_external_frame(frame_dict) pushes to self.out_queue.
   - expose async accept_external_audio(audio_bytes, sample_rate) -> convert to expected dict {mime_type:"audio/pcm", data:..., sampleRate?}
   - skip launching get_frames/get_screen/listen_audio tasks when operating in “client_input” mode.
   - ensure send_realtime() handles both frame/audio dicts arriving.
4. Maintain send_text for text prompts (queued from client).
5. Continue receive_audio() and play_audio() but remove local playback; instead emit via Socket.IO.
6. Ensure cleanup on stop/disconnect; cancel session, clear queues.

FRONTEND (iOS SwiftUI)
1. Info.plist: add NSCameraUsageDescription, NSMicrophoneUsageDescription (and temporary ATS exception if needed).
2. Define Socket manager:
   - import SocketIO
   - configure manager with server URL, `.log(false)`.
   - handle connect → emit start_streaming {mode:"camera"}.
   - handle disconnect, errors, ai_response, audio_data events.
3. Camera capture:
   - Create CameraCapture class w/ AVCaptureSession (preset .vga640x480).
   - Use AVCaptureVideoDataOutput; set delegate queue.
   - For each frame (target 1–3 fps):
       * throttle by timestamp.
       * convert CMSampleBuffer → JPEG (AVCaptureStillImage? use CIImage + CIContext).
       * resize to <=1024 px.
       * base64 encode.
       * emit video_frame via socket.
4. Audio capture:
   - Configure AVAudioSession category .playAndRecord, mode .voiceChat, options .defaultToSpeaker.
   - Setup AVAudioEngine.
   - Input node tap -> format 16k mono:
       * attach AVAudioConverter to resample to 16 kHz, interleave 16-bit PCM.
       * chunk ~20–40 ms (320–640 samples) into Data.
       * base64 encode, emit audio_chunk with sampleRate 16000.
5. Audio playback:
   - Create AudioPlayer class using AVAudioEngine + player node.
   - On audio_data event:
       * base64 decode to Data.
       * convert PCM 24 kHz → AVAudioPCMBuffer (16-bit) with AVAudioConverter.
       * schedule buffer on player node, start engine if not running.
6. UI:
   - Simple SwiftUI view with:
       * status text (connection/AI response).
       * text field for optional typed prompt.
       * main button toggling Start/Stop.
   - On Start:
       * Connect socket.
       * Request camera/mic authorization; start capture engines on success.
   - On Stop:
       * emit stop_streaming.
       * stop capture, disconnect socket.
7. Error handling:
   - Observe socket ‘error’ events; update UI.
   - Handle permission denials gracefully.
8. Testing steps:
   - Connect to secure endpoint (wss or ATS exception).
   - Confirm frame throttle, audio chunk sizes, reconnection cleanup.

CONFIRM: Do you want me to implement these backend updates and the new SwiftUI client now?
```

```plaintext
GOAL: Use existing Python Socket.IO + Gemini Live backend; build iOS client that streams mic/camera to server and plays Live API audio.

BACKEND (Python, WebInterface/app.py + main.py reuse)
1. On start_streaming:
   - create WebAudioLoop but disable local capture tasks (no cv2/sounddevice).
   - store client session id for routing.
   - emit status → “ready for client media”.
2. Add socket handlers:
   - @socketio.on('video_frame'): decode base64 → bytes; enqueue into AudioLoop.out_queue.
   - @socketio.on('audio_chunk'): decode base64 PCM → bytes; enqueue into out_queue (audio path).
3. Modify AudioLoop:
   - expose async method accept_external_frame(frame_dict) pushes to self.out_queue.
   - expose async accept_external_audio(audio_bytes, sample_rate) -> convert to expected dict {mime_type:"audio/pcm", data:..., sampleRate?}
   - skip launching get_frames/get_screen/listen_audio tasks when operating in “client_input” mode.
   - ensure send_realtime() handles both frame/audio dicts arriving.
4. Maintain send_text for text prompts (queued from client).
5. Continue receive_audio() and play_audio() but remove local playback; instead emit via Socket.IO.
6. Ensure cleanup on stop/disconnect; cancel session, clear queues.

FRONTEND (iOS SwiftUI)
1. Info.plist: add NSCameraUsageDescription, NSMicrophoneUsageDescription (and temporary ATS exception if needed).
2. Define Socket manager:
   - import SocketIO
   - configure manager with server URL, `.log(false)`.
   - handle connect → emit start_streaming {mode:"camera"}.
   - handle disconnect, errors, ai_response, audio_data events.
3. Camera capture:
   - Create CameraCapture class w/ AVCaptureSession (preset .vga640x480).
   - Use AVCaptureVideoDataOutput; set delegate queue.
   - For each frame (target 1–3 fps):
       * throttle by timestamp.
       * convert CMSampleBuffer → JPEG (AVCaptureStillImage? use CIImage + CIContext).
       * resize to <=1024 px.
       * base64 encode.
       * emit video_frame via socket.
4. Audio capture:
   - Configure AVAudioSession category .playAndRecord, mode .voiceChat, options .defaultToSpeaker.
   - Setup AVAudioEngine.
   - Input node tap -> format 16k mono:
       * attach AVAudioConverter to resample to 16 kHz, interleave 16-bit PCM.
       * chunk ~20–40 ms (320–640 samples) into Data.
       * base64 encode, emit audio_chunk with sampleRate 16000.
5. Audio playback:
   - Create AudioPlayer class using AVAudioEngine + player node.
   - On audio_data event:
       * base64 decode to Data.
       * convert PCM 24 kHz → AVAudioPCMBuffer (16-bit) with AVAudioConverter.
       * schedule buffer on player node, start engine if not running.
6. UI:
   - Simple SwiftUI view with:
       * status text (connection/AI response).
       * text field for optional typed prompt.
       * main button toggling Start/Stop.
   - On Start:
       * Connect socket.
       * Request camera/mic authorization; start capture engines on success.
   - On Stop:
       * emit stop_streaming.
       * stop capture, disconnect socket.
7. Error handling:
   - Observe socket ‘error’ events; update UI.
   - Handle permission denials gracefully.
8. Testing steps:
   - Connect to secure endpoint (wss or ATS exception).
   - Confirm frame throttle, audio chunk sizes, reconnection cleanup.

CONFIRM: Do you want me to implement these backend updates and the new SwiftUI client now?
```

---

**User**

Yes, I confirm. Continue with implementation

---

**Cursor**

```python
*** Begin Patch
*** Update File: /Users/eirvav/PythonProjects/A-EYE/WebInterface/main.py
@@
-class AudioLoop:
-    def __init__(self, video_mode=DEFAULT_MODE):
-        self.video_mode = video_mode
-
-        self.audio_in_queue = None
-        self.out_queue = None
-
-        self.session = None
-
-        self.send_text_task = None
-        self.receive_audio_task = None
-        self.play_audio_task = None
-        
-        # Audio streams for sounddevice
-        self.input_stream = None
-        self.output_stream = None
+class AudioLoop:
+    def __init__(
+        self,
+        video_mode=DEFAULT_MODE,
+        *,
+        external_media: bool = False,
+        enable_local_playback: bool = True,
+        cli_text_input: bool = True,
+    ):
+        self.video_mode = video_mode
+        self.external_media = external_media
+        self.enable_local_playback = enable_local_playback
+        self.cli_text_input = cli_text_input
+
+        self.audio_in_queue = None
+        self.out_queue = None
+        self.text_queue = None
+
+        self.session = None
+
+        self.send_text_task = None
+        self.receive_audio_task = None
+        self.play_audio_task = None
+
+        # Audio streams for sounddevice (only used when capturing locally)
+        self.input_stream = None
+        self.output_stream = None
+
+        # Internal flag for graceful shutdown
+        self._active = True
@@
     async def send_text(self):
-        while True:
-            text = await asyncio.to_thread(
-                input,
-                "message > ",
-            )
-            if text.lower() == "q":
-                break
-            await self.session.send(input=text or ".", end_of_turn=True)
+        while self._active:
+            text = await asyncio.to_thread(
+                input,
+                "message > ",
+            )
+            if text.lower() == "q":
+                self._active = False
+                break
+            await self.session.send(input=text or ".", end_of_turn=True)
@@
     async def send_realtime(self):
         while True:
             msg = await self.out_queue.get()
             await self.session.send(input=msg)
+
+    async def queue_realtime_input(self, payload):
+        if not self.out_queue:
+            return
+        if self.out_queue.full():
+            try:
+                self.out_queue.get_nowait()
+            except asyncio.QueueEmpty:
+                pass
+        await self.out_queue.put(payload)
 
     async def listen_audio(self):
         # Create a queue for audio data
         audio_queue = asyncio.Queue()
@@
-                await self.out_queue.put(audio_data)
+                if self.out_queue.full():
+                    try:
+                        self.out_queue.get_nowait()
+                    except asyncio.QueueEmpty:
+                        pass
+                await self.out_queue.put(audio_data)
         finally:
             if self.input_stream:
                 self.input_stream.stop()
                 self.input_stream.close()
@@
-            async with (
-                client.aio.live.connect(model=MODEL, config=CONFIG) as session,
-                asyncio.TaskGroup() as tg,
-            ):
+            async with (
+                client.aio.live.connect(model=MODEL, config=CONFIG) as session,
+                asyncio.TaskGroup() as tg,
+            ):
+                loop = asyncio.get_running_loop()
+                self.session = session
+
+                if self.enable_local_playback:
+                    self.audio_in_queue = asyncio.Queue(maxsize=50)
+                else:
+                    self.audio_in_queue = asyncio.Queue(maxsize=50)
+
+                self.out_queue = asyncio.Queue(maxsize=10)
+
+                if self.cli_text_input:
+                    send_text_task = tg.create_task(self.send_text())
+                else:
+                    send_text_task = None
+
+                tg.create_task(self.send_realtime())
+
+                if not self.external_media:
+                    tg.create_task(self.listen_audio())
+                    if self.video_mode == "camera":
+                        tg.create_task(self.get_frames())
+                    elif self.video_mode == "screen":
+                        tg.create_task(self.get_screen())
+
+                tg.create_task(self.receive_audio())
+
+                if self.enable_local_playback:
+                    tg.create_task(self.play_audio())
+
+                if send_text_task is not None:
+                    await send_text_task
+                    raise asyncio.CancelledError("User requested exit")
+                else:
+                    await asyncio.Future()
-
-                self.session = session
-
-                self.audio_in_queue = asyncio.Queue()
-                self.out_queue = asyncio.Queue(maxsize=5)
-
-                send_text_task = tg.create_task(self.send_text())
-                tg.create_task(self.send_realtime())
-                tg.create_task(self.listen_audio())
-                if self.video_mode == "camera":
-                    tg.create_task(self.get_frames())
-                elif self.video_mode == "screen":
-                    tg.create_task(self.get_screen())
-
-                tg.create_task(self.receive_audio())
-                tg.create_task(self.play_audio())
-
-                await send_text_task
-                raise asyncio.CancelledError("User requested exit")
 
         except asyncio.CancelledError:
             pass
*** End Patch
```

```python
"""
## Documentation
Quickstart: https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI.py

## Setup

To install the dependencies for this script, run:

```
pip install google-genai opencv-python sounddevice pillow mss python-dotenv numpy

Important: **Use headphones**. This script uses the system default audio
input and output, which often won't include echo cancellation. So to prevent
the model from interrupting itself it is important that you use headphones. 

```
To run the script:
```
python main.py
```

The script takes a video-mode flag `--mode`, this can be "camera", "screen", or "none".
The default is "camera". To share your screen run:

```
python Get_started_LiveAPI.py --mode screen
```
"""

import os
import asyncio
import base64
import io
import traceback
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

import cv2
import sounddevice as sd
import numpy as np
import PIL.Image
import mss

import argparse

from google import genai
from google.genai import types

CHANNELS = 1
SEND_SAMPLE_RATE = 16000
RECEIVE_SAMPLE_RATE = 24000
CHUNK_SIZE = 1024
DTYPE = np.int16

MODEL = "models/gemini-2.5-flash-preview-native-audio-dialog"

DEFAULT_MODE = "camera"

client = genai.Client(
    http_options={"api_version": "v1beta"},
    api_key=os.environ.get("GEMINI_API_KEY"),
)


CONFIG = types.LiveConnectConfig(
    response_modalities=[
        "AUDIO",
    ],
    media_resolution="MEDIA_RESOLUTION_MEDIUM",
    speech_config=types.SpeechConfig(
        voice_config=types.VoiceConfig(
            prebuilt_voice_config=types.PrebuiltVoiceConfig(voice_name="Zephyr")
        )
    ),
    context_window_compression=types.ContextWindowCompressionConfig(
        trigger_tokens=25600,
        sliding_window=types.SlidingWindow(target_tokens=12800),
    ),
)

# Audio device will be selected in the AudioLoop class


class AudioLoop:
    def __init__(
        self,
        video_mode=DEFAULT_MODE,
        *,
        external_media: bool = False,
        enable_local_playback: bool = True,
        enable_cli_text: bool = True,
    ):
        self.video_mode = video_mode
        self.external_media = external_media
        self.enable_local_playback = enable_local_playback
        self.enable_cli_text = enable_cli_text

        self.audio_in_queue = None
        self.out_queue = None
        self.text_queue = None

        self.session = None

        self.send_text_task = None
        self.receive_audio_task = None
        self.play_audio_task = None

        # Audio streams for sounddevice (only used when capturing locally)
        self.input_stream = None
        self.output_stream = None

        self._active = True

    async def send_text(self):
        while self._active:
            text = await asyncio.to_thread(
                input,
                "message > ",
            )
            if text.lower() == "q":
                self._active = False
                break
            await self.session.send(input=text or ".", end_of_turn=True)

    def _get_frame(self, cap):
        # Read the frameq
        ret, frame = cap.read()
        # Check if the frame was read successfully
        if not ret:
            return None
        # Fix: Convert BGR to RGB color space
        # OpenCV captures in BGR but PIL expects RGB format
        # This prevents the blue tint in the video feed
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        img = PIL.Image.fromarray(frame_rgb)  # Now using RGB frame
        img.thumbnail([1024, 1024])

        image_io = io.BytesIO()
        img.save(image_io, format="jpeg")
        image_io.seek(0)

        mime_type = "image/jpeg"
        image_bytes = image_io.read()
        return {"mime_type": mime_type, "data": base64.b64encode(image_bytes).decode()}

    async def get_frames(self):
        # This takes about a second, and will block the whole program
        # causing the audio pipeline to overflow if you don't to_thread it.
        cap = await asyncio.to_thread(
            cv2.VideoCapture, 0
        )  # 0 represents the default camera

        while True:
            frame = await asyncio.to_thread(self._get_frame, cap)
            if frame is None:
                break

            await asyncio.sleep(1.0)

            await self.out_queue.put(frame)

        # Release the VideoCapture object
        cap.release()

    def _get_screen(self):
        sct = mss.mss()
        monitor = sct.monitors[0]

        i = sct.grab(monitor)

        mime_type = "image/jpeg"
        image_bytes = mss.tools.to_png(i.rgb, i.size)
        img = PIL.Image.open(io.BytesIO(image_bytes))

        image_io = io.BytesIO()
        img.save(image_io, format="jpeg")
        image_io.seek(0)

        image_bytes = image_io.read()
        return {"mime_type": mime_type, "data": base64.b64encode(image_bytes).decode()}

    async def get_screen(self):

        while True:
            frame = await asyncio.to_thread(self._get_screen)
            if frame is None:
                break

            await asyncio.sleep(1.0)

            await self.out_queue.put(frame)

    async def send_realtime(self):
        while True:
            msg = await self.out_queue.get()
            await self.session.send(input=msg)

    async def queue_realtime_input(self, payload):
        if not self.out_queue:
            return
        if self.out_queue.full():
            try:
                self.out_queue.get_nowait()
            except asyncio.QueueEmpty:
                pass
        await self.out_queue.put(payload)

    async def listen_audio(self):
        # Create a queue for audio data
        audio_queue = asyncio.Queue()
        
        def audio_callback(indata, frames, time, status):
            if status:
                print(f"Audio input status: {status}")
            # Convert float32 to int16 and get raw bytes
            audio_data = (indata[:, 0] * 32767).astype(DTYPE).tobytes()
            try:
                audio_queue.put_nowait({"data": audio_data, "mime_type": "audio/pcm"})
            except asyncio.QueueFull:
                pass  # Drop frames if queue is full
        
        # Start the input stream
        self.input_stream = sd.InputStream(
            samplerate=SEND_SAMPLE_RATE,
            channels=CHANNELS,
            dtype='float32',
            blocksize=CHUNK_SIZE,
            callback=audio_callback
        )
        
        self.input_stream.start()
        
        try:
            while True:
                audio_data = await audio_queue.get()
                if self.out_queue.full():
                    try:
                        self.out_queue.get_nowait()
                    except asyncio.QueueEmpty:
                        pass
                await self.out_queue.put(audio_data)
        finally:
            if self.input_stream:
                self.input_stream.stop()
                self.input_stream.close()

    async def receive_audio(self):
        "Background task to reads from the websocket and write pcm chunks to the output queue"
        while True:
            turn = self.session.receive()
            async for response in turn:
                if data := response.data:
                    self.audio_in_queue.put_nowait(data)
                    continue
                if text := response.text:
                    print(text, end="")

            # If you interrupt the model, it sends a turn_complete.
            # For interruptions to work, we need to stop playback.
            # So empty out the audio queue because it may have loaded
            # much more audio than has played yet.
            while not self.audio_in_queue.empty():
                self.audio_in_queue.get_nowait()

    async def play_audio(self):
        # Create a queue for output audio data
        output_queue = asyncio.Queue()
        
        def audio_callback(outdata, frames, time, status):
            if status:
                print(f"Audio output status: {status}")
            try:
                # Get audio data from queue (non-blocking)
                audio_data = output_queue.get_nowait()
                # Convert bytes to numpy array and reshape
                audio_array = np.frombuffer(audio_data, dtype=DTYPE).astype(np.float32) / 32767.0
                # Ensure we have the right number of frames
                if len(audio_array) >= frames:
                    outdata[:, 0] = audio_array[:frames]
                else:
                    # Pad with zeros if not enough data
                    outdata[:len(audio_array), 0] = audio_array
                    outdata[len(audio_array):, 0] = 0
            except asyncio.QueueEmpty:
                # No audio data available, output silence
                outdata[:, 0] = 0
        
        # Start the output stream
        self.output_stream = sd.OutputStream(
            samplerate=RECEIVE_SAMPLE_RATE,
            channels=CHANNELS,
            dtype='float32',
            blocksize=CHUNK_SIZE,
            callback=audio_callback
        )
        
        self.output_stream.start()
        
        try:
            while True:
                bytestream = await self.audio_in_queue.get()
                await output_queue.put(bytestream)
        finally:
            if self.output_stream:
                self.output_stream.stop()
                self.output_stream.close()

    async def run(self):
        try:
            async with (
                client.aio.live.connect(model=MODEL, config=CONFIG) as session,
                asyncio.TaskGroup() as tg,
            ):
                self.session = session

                self.audio_in_queue = asyncio.Queue(maxsize=50)
                self.out_queue = asyncio.Queue(maxsize=10)

                send_text_task = None
                if self.enable_cli_text:
                    send_text_task = tg.create_task(self.send_text())

                tg.create_task(self.send_realtime())

                if not self.external_media:
                    tg.create_task(self.listen_audio())
                    if self.video_mode == "camera":
                        tg.create_task(self.get_frames())
                    elif self.video_mode == "screen":
                        tg.create_task(self.get_screen())

                tg.create_task(self.receive_audio())

                if self.enable_local_playback:
                    tg.create_task(self.play_audio())

                if send_text_task is not None:
                    await send_text_task
                    raise asyncio.CancelledError("User requested exit")
                else:
                    await asyncio.Future()

        except asyncio.CancelledError:
            pass
        except ExceptionGroup as EG:
            # Close audio streams if they exist
            if hasattr(self, 'input_stream') and self.input_stream:
                self.input_stream.stop()
                self.input_stream.close()
            if hasattr(self, 'output_stream') and self.output_stream:
                self.output_stream.stop()
                self.output_stream.close()
            traceback.print_exception(EG)
        finally:
            # Ensure streams are closed
            if hasattr(self, 'input_stream') and self.input_stream:
                self.input_stream.stop()
                self.input_stream.close()
            if hasattr(self, 'output_stream') and self.output_stream:
                self.output_stream.stop()
                self.output_stream.close()


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--mode",
        type=str,
        default=DEFAULT_MODE,
        help="pixels to stream from",
        choices=["camera", "screen", "none"],
    )
    args = parser.parse_args()
    main = AudioLoop(video_mode=args.mode)
    asyncio.run(main.run())

```

```python
"""
## Documentation
Quickstart: https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI.py

## Setup

To install the dependencies for this script, run:

```
pip install google-genai opencv-python sounddevice pillow mss python-dotenv numpy

Important: **Use headphones**. This script uses the system default audio
input and output, which often won't include echo cancellation. So to prevent
the model from interrupting itself it is important that you use headphones. 

```
To run the script:
```
python main.py
```

The script takes a video-mode flag `--mode`, this can be "camera", "screen", or "none".
The default is "camera". To share your screen run:

```
python Get_started_LiveAPI.py --mode screen
```
"""

import os
import asyncio
import base64
import io
import traceback
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

import cv2
import sounddevice as sd
import numpy as np
import PIL.Image
import mss

import argparse

from google import genai
from google.genai import types

CHANNELS = 1
SEND_SAMPLE_RATE = 16000
RECEIVE_SAMPLE_RATE = 24000
CHUNK_SIZE = 1024
DTYPE = np.int16

MODEL = "models/gemini-2.5-flash-preview-native-audio-dialog"

DEFAULT_MODE = "camera"

client = genai.Client(
    http_options={"api_version": "v1beta"},
    api_key=os.environ.get("GEMINI_API_KEY"),
)


CONFIG = types.LiveConnectConfig(
    response_modalities=[
        "AUDIO",
    ],
    media_resolution="MEDIA_RESOLUTION_MEDIUM",
    speech_config=types.SpeechConfig(
        voice_config=types.VoiceConfig(
            prebuilt_voice_config=types.PrebuiltVoiceConfig(voice_name="Zephyr")
        )
    ),
    context_window_compression=types.ContextWindowCompressionConfig(
        trigger_tokens=25600,
        sliding_window=types.SlidingWindow(target_tokens=12800),
    ),
)

# Audio device will be selected in the AudioLoop class


class AudioLoop:
    def __init__(
        self,
        video_mode=DEFAULT_MODE,
        *,
        external_media: bool = False,
        enable_local_playback: bool = True,
        enable_cli_text: bool = True,
    ):
        self.video_mode = video_mode
        self.external_media = external_media
        self.enable_local_playback = enable_local_playback
        self.enable_cli_text = enable_cli_text

        self.audio_in_queue = None
        self.out_queue = None
        self.text_queue = None

        self.session = None

        self.send_text_task = None
        self.receive_audio_task = None
        self.play_audio_task = None

        # Audio streams for sounddevice (only used when capturing locally)
        self.input_stream = None
        self.output_stream = None

        self._active = True
        self._shutdown_event = asyncio.Event()

        if not self.enable_cli_text:
            self.text_queue = asyncio.Queue()

    async def send_text(self):
        if self.enable_cli_text:
            while self._active:
                text = await asyncio.to_thread(
                    input,
                    "message > ",
                )
                if text.lower() == "q":
                    self._active = False
                    self._shutdown_event.set()
                    break
                await self.session.send(input=text or ".", end_of_turn=True)
        else:
            while self._active:
                try:
                    message = await self.text_queue.get()
                except asyncio.CancelledError:
                    break
                if message is None:
                    self._active = False
                    self._shutdown_event.set()
                    break
                await self.session.send(input=message, end_of_turn=True)

    def _get_frame(self, cap):
        # Read the frameq
        ret, frame = cap.read()
        # Check if the frame was read successfully
        if not ret:
            return None
        # Fix: Convert BGR to RGB color space
        # OpenCV captures in BGR but PIL expects RGB format
        # This prevents the blue tint in the video feed
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        img = PIL.Image.fromarray(frame_rgb)  # Now using RGB frame
        img.thumbnail([1024, 1024])

        image_io = io.BytesIO()
        img.save(image_io, format="jpeg")
        image_io.seek(0)

        mime_type = "image/jpeg"
        image_bytes = image_io.read()
        return {"mime_type": mime_type, "data": base64.b64encode(image_bytes).decode()}

    async def get_frames(self):
        # This takes about a second, and will block the whole program
        # causing the audio pipeline to overflow if you don't to_thread it.
        cap = await asyncio.to_thread(
            cv2.VideoCapture, 0
        )  # 0 represents the default camera

        while True:
            frame = await asyncio.to_thread(self._get_frame, cap)
            if frame is None:
                break

            await asyncio.sleep(1.0)

            await self.out_queue.put(frame)

        # Release the VideoCapture object
        cap.release()

    def _get_screen(self):
        sct = mss.mss()
        monitor = sct.monitors[0]

        i = sct.grab(monitor)

        mime_type = "image/jpeg"
        image_bytes = mss.tools.to_png(i.rgb, i.size)
        img = PIL.Image.open(io.BytesIO(image_bytes))

        image_io = io.BytesIO()
        img.save(image_io, format="jpeg")
        image_io.seek(0)

        image_bytes = image_io.read()
        return {"mime_type": mime_type, "data": base64.b64encode(image_bytes).decode()}

    async def get_screen(self):

        while True:
            frame = await asyncio.to_thread(self._get_screen)
            if frame is None:
                break

            await asyncio.sleep(1.0)

            await self.out_queue.put(frame)

    async def send_realtime(self):
        while True:
            msg = await self.out_queue.get()
            await self.session.send(input=msg)

    async def queue_realtime_input(self, payload):
        if not self.out_queue:
            return
        if self.out_queue.full():
            try:
                self.out_queue.get_nowait()
            except asyncio.QueueEmpty:
                pass
        await self.out_queue.put(payload)

    async def queue_text(self, message: str, *, end_of_turn: bool = True):
        if self.enable_cli_text or not self.text_queue:
            raise RuntimeError("Text queue not available in CLI mode")
        payload = message
        if end_of_turn:
            await self.text_queue.put(payload)
        else:
            await self.session.send(input=payload, end_of_turn=False)

    def stop(self):
        self._active = False
        self._shutdown_event.set()

    async def listen_audio(self):
        # Create a queue for audio data
        audio_queue = asyncio.Queue()
        
        def audio_callback(indata, frames, time, status):
            if status:
                print(f"Audio input status: {status}")
            # Convert float32 to int16 and get raw bytes
            audio_data = (indata[:, 0] * 32767).astype(DTYPE).tobytes()
            try:
                audio_queue.put_nowait({"data": audio_data, "mime_type": "audio/pcm"})
            except asyncio.QueueFull:
                pass  # Drop frames if queue is full
        
        # Start the input stream
        self.input_stream = sd.InputStream(
            samplerate=SEND_SAMPLE_RATE,
            channels=CHANNELS,
            dtype='float32',
            blocksize=CHUNK_SIZE,
            callback=audio_callback
        )
        
        self.input_stream.start()
        
        try:
            while True:
                audio_data = await audio_queue.get()
                if self.out_queue.full():
                    try:
                        self.out_queue.get_nowait()
                    except asyncio.QueueEmpty:
                        pass
                await self.out_queue.put(audio_data)
        finally:
            if self.input_stream:
                self.input_stream.stop()
                self.input_stream.close()

    async def receive_audio(self):
        "Background task to reads from the websocket and write pcm chunks to the output queue"
        while True:
            turn = self.session.receive()
            async for response in turn:
                if data := response.data:
                    self.audio_in_queue.put_nowait(data)
                    continue
                if text := response.text:
                    print(text, end="")

            # If you interrupt the model, it sends a turn_complete.
            # For interruptions to work, we need to stop playback.
            # So empty out the audio queue because it may have loaded
            # much more audio than has played yet.
            while not self.audio_in_queue.empty():
                self.audio_in_queue.get_nowait()

    async def play_audio(self):
        # Create a queue for output audio data
        output_queue = asyncio.Queue()
        
        def audio_callback(outdata, frames, time, status):
            if status:
                print(f"Audio output status: {status}")
            try:
                # Get audio data from queue (non-blocking)
                audio_data = output_queue.get_nowait()
                # Convert bytes to numpy array and reshape
                audio_array = np.frombuffer(audio_data, dtype=DTYPE).astype(np.float32) / 32767.0
                # Ensure we have the right number of frames
                if len(audio_array) >= frames:
                    outdata[:, 0] = audio_array[:frames]
                else:
                    # Pad with zeros if not enough data
                    outdata[:len(audio_array), 0] = audio_array
                    outdata[len(audio_array):, 0] = 0
            except asyncio.QueueEmpty:
                # No audio data available, output silence
                outdata[:, 0] = 0
        
        # Start the output stream
        self.output_stream = sd.OutputStream(
            samplerate=RECEIVE_SAMPLE_RATE,
            channels=CHANNELS,
            dtype='float32',
            blocksize=CHUNK_SIZE,
            callback=audio_callback
        )
        
        self.output_stream.start()
        
        try:
            while True:
                bytestream = await self.audio_in_queue.get()
                await output_queue.put(bytestream)
        finally:
            if self.output_stream:
                self.output_stream.stop()
                self.output_stream.close()

    async def run(self):
        try:
            async with (
                client.aio.live.connect(model=MODEL, config=CONFIG) as session,
                asyncio.TaskGroup() as tg,
            ):
                self.session = session

                self.audio_in_queue = asyncio.Queue(maxsize=50)
                self.out_queue = asyncio.Queue(maxsize=10)

                send_text_task = None
                if self.enable_cli_text:
                    send_text_task = tg.create_task(self.send_text())

                tg.create_task(self.send_realtime())

                if not self.external_media:
                    tg.create_task(self.listen_audio())
                    if self.video_mode == "camera":
                        tg.create_task(self.get_frames())
                    elif self.video_mode == "screen":
                        tg.create_task(self.get_screen())

                tg.create_task(self.receive_audio())

                if self.enable_local_playback:
                    tg.create_task(self.play_audio())

                if send_text_task is not None:
                    await send_text_task
                    raise asyncio.CancelledError("User requested exit")
                else:
                    await asyncio.Future()

        except asyncio.CancelledError:
            pass
        except ExceptionGroup as EG:
            # Close audio streams if they exist
            if hasattr(self, 'input_stream') and self.input_stream:
                self.input_stream.stop()
                self.input_stream.close()
            if hasattr(self, 'output_stream') and self.output_stream:
                self.output_stream.stop()
                self.output_stream.close()
            traceback.print_exception(EG)
        finally:
            # Ensure streams are closed
            if hasattr(self, 'input_stream') and self.input_stream:
                self.input_stream.stop()
                self.input_stream.close()
            if hasattr(self, 'output_stream') and self.output_stream:
                self.output_stream.stop()
                self.output_stream.close()


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--mode",
        type=str,
        default=DEFAULT_MODE,
        help="pixels to stream from",
        choices=["camera", "screen", "none"],
    )
    args = parser.parse_args()
    main = AudioLoop(video_mode=args.mode)
    asyncio.run(main.run())

```

```python
*** Begin Patch
*** Update File: /Users/eirvav/PythonProjects/A-EYE/WebInterface/main.py
@@
     async def listen_audio(self):
         # Create a queue for audio data
         audio_queue = asyncio.Queue()
 
         def audio_callback(indata, frames, time, status):
             if status:
                 print(f"Audio input status: {status}")
             # Convert float32 to int16 and get raw bytes
             audio_data = (indata[:, 0] * 32767).astype(DTYPE).tobytes()
             try:
                 audio_queue.put_nowait({"data": audio_data, "mime_type": "audio/pcm"})
             except asyncio.QueueFull:
                 pass  # Drop frames if queue is full
 
         # Start the input stream
         self.input_stream = sd.InputStream(
             samplerate=SEND_SAMPLE_RATE,
             channels=CHANNELS,
             dtype='float32',
             blocksize=CHUNK_SIZE,
             callback=audio_callback
         )
 
         self.input_stream.start()
 
         try:
             while True:
                 audio_data = await audio_queue.get()
                 if self.out_queue.full():
                     try:
                         self.out_queue.get_nowait()
                     except asyncio.QueueEmpty:
                         pass
                 await self.out_queue.put(audio_data)
         finally:
             if self.input_stream:
                 self.input_stream.stop()
                 self.input_stream.close()
@@
     async def run(self):
         try:
             async with (
                 client.aio.live.connect(model=MODEL, config=CONFIG) as session,
                 asyncio.TaskGroup() as tg,
             ):
                 self.session = session
 
                 self.audio_in_queue = asyncio.Queue(maxsize=50)
                 self.out_queue = asyncio.Queue(maxsize=10)
 
                 send_text_task = None
                 if self.enable_cli_text:
                     send_text_task = tg.create_task(self.send_text())
 
                 tg.create_task(self.send_realtime())
 
                 if not self.external_media:
                     tg.create_task(self.listen_audio())
                     if self.video_mode == "camera":
                         tg.create_task(self.get_frames())
                     elif self.video_mode == "screen":
                         tg.create_task(self.get_screen())
 
                 tg.create_task(self.receive_audio())
 
                 if self.enable_local_playback:
                     tg.create_task(self.play_audio())
 
                 if send_text_task is not None:
                     await send_text_task
                     raise asyncio.CancelledError("User requested exit")
                 else:
                     await self._shutdown_event.wait()
 
         except asyncio.CancelledError:
             pass
*** End Patch
```

```python
"""
## Documentation
Quickstart: https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI.py

## Setup

To install the dependencies for this script, run:

```
pip install google-genai opencv-python sounddevice pillow mss python-dotenv numpy

Important: **Use headphones**. This script uses the system default audio
input and output, which often won't include echo cancellation. So to prevent
the model from interrupting itself it is important that you use headphones. 

```
To run the script:
```
python main.py
```

The script takes a video-mode flag `--mode`, this can be "camera", "screen", or "none".
The default is "camera". To share your screen run:

```
python Get_started_LiveAPI.py --mode screen
```
"""

import os
import asyncio
import base64
import io
import traceback
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

import cv2
import sounddevice as sd
import numpy as np
import PIL.Image
import mss

import argparse

from google import genai
from google.genai import types

CHANNELS = 1
SEND_SAMPLE_RATE = 16000
RECEIVE_SAMPLE_RATE = 24000
CHUNK_SIZE = 1024
DTYPE = np.int16

MODEL = "models/gemini-2.5-flash-preview-native-audio-dialog"

DEFAULT_MODE = "camera"

client = genai.Client(
    http_options={"api_version": "v1beta"},
    api_key=os.environ.get("GEMINI_API_KEY"),
)


CONFIG = types.LiveConnectConfig(
    response_modalities=[
        "AUDIO",
    ],
    media_resolution="MEDIA_RESOLUTION_MEDIUM",
    speech_config=types.SpeechConfig(
        voice_config=types.VoiceConfig(
            prebuilt_voice_config=types.PrebuiltVoiceConfig(voice_name="Zephyr")
        )
    ),
    context_window_compression=types.ContextWindowCompressionConfig(
        trigger_tokens=25600,
        sliding_window=types.SlidingWindow(target_tokens=12800),
    ),
)

# Audio device will be selected in the AudioLoop class


class AudioLoop:
    def __init__(
        self,
        video_mode=DEFAULT_MODE,
        *,
        external_media: bool = False,
        enable_local_playback: bool = True,
        enable_cli_text: bool = True,
    ):
        self.video_mode = video_mode
        self.external_media = external_media
        self.enable_local_playback = enable_local_playback
        self.enable_cli_text = enable_cli_text

        self.audio_in_queue = None
        self.out_queue = None
        self.text_queue = None

        self.session = None

        self.send_text_task = None
        self.receive_audio_task = None
        self.play_audio_task = None

        # Audio streams for sounddevice (only used when capturing locally)
        self.input_stream = None
        self.output_stream = None

        self._active = True
        self._shutdown_event = asyncio.Event()

        if not self.enable_cli_text:
            self.text_queue = asyncio.Queue()

    async def send_text(self):
        if self.enable_cli_text:
            while self._active:
                text = await asyncio.to_thread(
                    input,
                    "message > ",
                )
                if text.lower() == "q":
                    self._active = False
                    self._shutdown_event.set()
                    break
                await self.session.send(input=text or ".", end_of_turn=True)
        else:
            while self._active:
                try:
                    message = await self.text_queue.get()
                except asyncio.CancelledError:
                    break
                if message is None:
                    self._active = False
                    self._shutdown_event.set()
                    break
                await self.session.send(input=message, end_of_turn=True)

    def _get_frame(self, cap):
        # Read the frameq
        ret, frame = cap.read()
        # Check if the frame was read successfully
        if not ret:
            return None
        # Fix: Convert BGR to RGB color space
        # OpenCV captures in BGR but PIL expects RGB format
        # This prevents the blue tint in the video feed
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        img = PIL.Image.fromarray(frame_rgb)  # Now using RGB frame
        img.thumbnail([1024, 1024])

        image_io = io.BytesIO()
        img.save(image_io, format="jpeg")
        image_io.seek(0)

        mime_type = "image/jpeg"
        image_bytes = image_io.read()
        return {"mime_type": mime_type, "data": base64.b64encode(image_bytes).decode()}

    async def get_frames(self):
        # This takes about a second, and will block the whole program
        # causing the audio pipeline to overflow if you don't to_thread it.
        cap = await asyncio.to_thread(
            cv2.VideoCapture, 0
        )  # 0 represents the default camera

        while True:
            frame = await asyncio.to_thread(self._get_frame, cap)
            if frame is None:
                break

            await asyncio.sleep(1.0)

            await self.out_queue.put(frame)

        # Release the VideoCapture object
        cap.release()

    def _get_screen(self):
        sct = mss.mss()
        monitor = sct.monitors[0]

        i = sct.grab(monitor)

        mime_type = "image/jpeg"
        image_bytes = mss.tools.to_png(i.rgb, i.size)
        img = PIL.Image.open(io.BytesIO(image_bytes))

        image_io = io.BytesIO()
        img.save(image_io, format="jpeg")
        image_io.seek(0)

        image_bytes = image_io.read()
        return {"mime_type": mime_type, "data": base64.b64encode(image_bytes).decode()}

    async def get_screen(self):

        while True:
            frame = await asyncio.to_thread(self._get_screen)
            if frame is None:
                break

            await asyncio.sleep(1.0)

            await self.out_queue.put(frame)

    async def send_realtime(self):
        while True:
            msg = await self.out_queue.get()
            await self.session.send(input=msg)

    async def queue_realtime_input(self, payload):
        if not self.out_queue:
            return
        if self.out_queue.full():
            try:
                self.out_queue.get_nowait()
            except asyncio.QueueEmpty:
                pass
        await self.out_queue.put(payload)

    async def queue_text(self, message: str, *, end_of_turn: bool = True):
        if self.enable_cli_text or not self.text_queue:
            raise RuntimeError("Text queue not available in CLI mode")
        payload = message
        if end_of_turn:
            await self.text_queue.put(payload)
        else:
            await self.session.send(input=payload, end_of_turn=False)

    def stop(self):
        self._active = False
        self._shutdown_event.set()

    async def listen_audio(self):
        # Create a queue for audio data
        audio_queue = asyncio.Queue()
        
        def audio_callback(indata, frames, time, status):
            if status:
                print(f"Audio input status: {status}")
            # Convert float32 to int16 and get raw bytes
            audio_data = (indata[:, 0] * 32767).astype(DTYPE).tobytes()
            try:
                audio_queue.put_nowait({"data": audio_data, "mime_type": "audio/pcm"})
            except asyncio.QueueFull:
                pass  # Drop frames if queue is full
        
        # Start the input stream
        self.input_stream = sd.InputStream(
            samplerate=SEND_SAMPLE_RATE,
            channels=CHANNELS,
            dtype='float32',
            blocksize=CHUNK_SIZE,
            callback=audio_callback
        )
        
        self.input_stream.start()
        
        try:
            while True:
                audio_data = await audio_queue.get()
                if self.out_queue.full():
                    try:
                        self.out_queue.get_nowait()
                    except asyncio.QueueEmpty:
                        pass
                await self.out_queue.put(audio_data)
        finally:
            if self.input_stream:
                self.input_stream.stop()
                self.input_stream.close()

    async def receive_audio(self):
        "Background task to reads from the websocket and write pcm chunks to the output queue"
        while True:
            turn = self.session.receive()
            async for response in turn:
                if data := response.data:
                    self.audio_in_queue.put_nowait(data)
                    continue
                if text := response.text:
                    print(text, end="")

            # If you interrupt the model, it sends a turn_complete.
            # For interruptions to work, we need to stop playback.
            # So empty out the audio queue because it may have loaded
            # much more audio than has played yet.
            while not self.audio_in_queue.empty():
                self.audio_in_queue.get_nowait()

    async def play_audio(self):
        # Create a queue for output audio data
        output_queue = asyncio.Queue()
        
        def audio_callback(outdata, frames, time, status):
            if status:
                print(f"Audio output status: {status}")
            try:
                # Get audio data from queue (non-blocking)
                audio_data = output_queue.get_nowait()
                # Convert bytes to numpy array and reshape
                audio_array = np.frombuffer(audio_data, dtype=DTYPE).astype(np.float32) / 32767.0
                # Ensure we have the right number of frames
                if len(audio_array) >= frames:
                    outdata[:, 0] = audio_array[:frames]
                else:
                    # Pad with zeros if not enough data
                    outdata[:len(audio_array), 0] = audio_array
                    outdata[len(audio_array):, 0] = 0
            except asyncio.QueueEmpty:
                # No audio data available, output silence
                outdata[:, 0] = 0
        
        # Start the output stream
        self.output_stream = sd.OutputStream(
            samplerate=RECEIVE_SAMPLE_RATE,
            channels=CHANNELS,
            dtype='float32',
            blocksize=CHUNK_SIZE,
            callback=audio_callback
        )
        
        self.output_stream.start()
        
        try:
            while True:
                bytestream = await self.audio_in_queue.get()
                await output_queue.put(bytestream)
        finally:
            if self.output_stream:
                self.output_stream.stop()
                self.output_stream.close()

    async def run(self):
        try:
            async with (
                client.aio.live.connect(model=MODEL, config=CONFIG) as session,
                asyncio.TaskGroup() as tg,
            ):
                self.session = session

                self.audio_in_queue = asyncio.Queue(maxsize=50)
                self.out_queue = asyncio.Queue(maxsize=10)

                send_text_task = None
                if self.enable_cli_text:
                    send_text_task = tg.create_task(self.send_text())

                tg.create_task(self.send_realtime())

                if not self.external_media:
                    tg.create_task(self.listen_audio())
                    if self.video_mode == "camera":
                        tg.create_task(self.get_frames())
                    elif self.video_mode == "screen":
                        tg.create_task(self.get_screen())

                tg.create_task(self.receive_audio())

                if self.enable_local_playback:
                    tg.create_task(self.play_audio())

                if send_text_task is not None:
                    await send_text_task
                    raise asyncio.CancelledError("User requested exit")
                else:
                    await self._shutdown_event.wait()

        except asyncio.CancelledError:
            pass
        except ExceptionGroup as EG:
            # Close audio streams if they exist
            if hasattr(self, 'input_stream') and self.input_stream:
                self.input_stream.stop()
                self.input_stream.close()
            if hasattr(self, 'output_stream') and self.output_stream:
                self.output_stream.stop()
                self.output_stream.close()
            traceback.print_exception(EG)
        finally:
            # Ensure streams are closed
            if hasattr(self, 'input_stream') and self.input_stream:
                self.input_stream.stop()
                self.input_stream.close()
            if hasattr(self, 'output_stream') and self.output_stream:
                self.output_stream.stop()
                self.output_stream.close()


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--mode",
        type=str,
        default=DEFAULT_MODE,
        help="pixels to stream from",
        choices=["camera", "screen", "none"],
    )
    args = parser.parse_args()
    main = AudioLoop(video_mode=args.mode)
    asyncio.run(main.run())

```

```python
*** Begin Patch
*** Update File: /Users/eirvav/PythonProjects/A-EYE/WebInterface/app.py
@@
-from flask_socketio import SocketIO, emit
+from flask_socketio import SocketIO, emit
@@
-# Global variables
-audio_loop = None
-is_streaming = False
+# Global state per Socket.IO session
+active_loops = {}
@@
-    print('Client connected')
-    emit('status', {'message': 'Connected to server'})
+    print('Client connected')
+    emit('status', {'message': 'Connected to server'})
 
 @socketio.on('disconnect')
 def handle_disconnect():
-    print('Client disconnected')
-    global is_streaming, audio_loop
-    if is_streaming and audio_loop:
-        is_streaming = False
+    sid = request.sid
+    print(f'Client disconnected: {sid}')
+    loop_entry = active_loops.pop(sid, None)
+    if loop_entry:
+        loop, controller = loop_entry
+        controller.stop()
 
 @socketio.on('start_streaming')
 def handle_start_streaming(data):
-    global audio_loop, is_streaming
-    
-    if is_streaming:
-        emit('error', {'message': 'Already streaming'})
+    sid = request.sid
+
+    if sid in active_loops:
+        emit('error', {'message': 'Already streaming'})
         return
-    
+    
     mode = data.get('mode', 'camera')
     
     try:
-        is_streaming = True
-        audio_loop = WebAudioLoop(mode=mode)
+        audio_loop = WebAudioLoop(mode=mode, socket_sid=sid)
 
         # Start the audio loop in a separate thread
         def run_audio_loop():
             asyncio.run(audio_loop.run())
 
         thread = threading.Thread(target=run_audio_loop)
         thread.daemon = True
         thread.start()
-        
-        emit('status', {'message': f'Started streaming with {mode} mode'})
-        
+        active_loops[sid] = (audio_loop, audio_loop.controller)
+
+        emit('status', {'message': f'Started streaming with {mode} mode'})
+
     except Exception as e:
-        is_streaming = False
         emit('error', {'message': f'Failed to start streaming: {str(e)}'})
 
 @socketio.on('stop_streaming')
 def handle_stop_streaming():
-    global is_streaming, audio_loop
-    
-    if not is_streaming:
-        emit('error', {'message': 'Not currently streaming'})
+    sid = request.sid
+
+    loop_entry = active_loops.pop(sid, None)
+    if not loop_entry:
+        emit('error', {'message': 'Not currently streaming'})
         return
     
     try:
-        is_streaming = False
-        if audio_loop:
-            # This will be handled by the audio loop's cleanup
-            pass
-        emit('status', {'message': 'Stopped streaming'})
-        
+        loop, controller = loop_entry
+        controller.stop()
+        emit('status', {'message': 'Stopped streaming'})
+
     except Exception as e:
         emit('error', {'message': f'Failed to stop streaming: {str(e)}'})
@@
-    global audio_loop
-    
-    if not is_streaming or not audio_loop:
-        emit('error', {'message': 'Not currently streaming'})
+    sid = request.sid
+    loop_entry = active_loops.get(sid)
+    if not loop_entry:
+        emit('error', {'message': 'Not currently streaming'})
         return
-    
+
     message = data.get('message', '')
     if message:
         # Add message to the audio loop's queue
         try:
-            asyncio.run_coroutine_threadsafe(
-                audio_loop.send_text_message(message), 
-                audio_loop.loop
-            )
+            loop, controller = loop_entry
+            controller.enqueue_text(message)
             emit('status', {'message': f'Sent message: {message}'})
         except Exception as e:
             emit('error', {'message': f'Failed to send message: {str(e)}'})
+
+
+@socketio.on('video_frame')
+def handle_video_frame(data):
+    sid = request.sid
+    loop_entry = active_loops.get(sid)
+    if not loop_entry:
+        emit('error', {'message': 'Not currently streaming'})
+        return
+
+    frame_data = data.get('data')
+    mime_type = data.get('mime_type', 'image/jpeg')
+    if not frame_data:
+        emit('error', {'message': 'Missing frame data'})
+        return
+
+    try:
+        loop, controller = loop_entry
+        controller.enqueue_frame(mime_type, frame_data)
+    except Exception as exc:
+        emit('error', {'message': f'Failed to queue frame: {exc}'})
+
+
+@socketio.on('audio_chunk')
+def handle_audio_chunk(data):
+    sid = request.sid
+    loop_entry = active_loops.get(sid)
+    if not loop_entry:
+        emit('error', {'message': 'Not currently streaming'})
+        return
+
+    chunk = data.get('data')
+    sample_rate = data.get('sampleRate', SEND_SAMPLE_RATE)
+    if not chunk:
+        emit('error', {'message': 'Missing audio data'})
+        return
+
+    try:
+        loop, controller = loop_entry
+        controller.enqueue_audio(chunk, sample_rate)
+    except Exception as exc:
+        emit('error', {'message': f'Failed to queue audio: {exc}'})
@@
-class WebAudioLoop(AudioLoop):
-    """Extended AudioLoop class for web interface"""
-    
-    def __init__(self, mode="camera"):
-        super().__init__(video_mode=mode)
-        self.loop = None
-        self.text_queue = asyncio.Queue()
-    
-    async def send_text_message(self, message):
-        """Send a text message to the AI"""
-        if self.session:
-            await self.session.send(input=message, end_of_turn=True)
-    
-    async def send_text(self):
-        """Override to handle web-based text input"""
-        while True:
-            try:
-                # Wait for messages from the web interface
-                message = await self.text_queue.get()
-                if message.lower() == "q":
-                    break
-                await self.session.send(input=message, end_of_turn=True)
-            except Exception as e:
-                print(f"Error in send_text: {e}")
-                break
+class LoopController:
+    """Helper class to interact with AudioLoop from Flask thread."""
+
+    def __init__(self, loop: AudioLoop, aio_loop: asyncio.AbstractEventLoop):
+        self.loop = loop
+        self.aio_loop = aio_loop
+
+    def enqueue_text(self, message: str):
+        if not self.loop.text_queue:
+            raise RuntimeError("Text input is not enabled")
+        asyncio.run_coroutine_threadsafe(
+            self.loop.queue_text(message),
+            self.aio_loop,
+        )
+
+    def enqueue_frame(self, mime_type: str, base64_data: str):
+        payload = {
+            "mime_type": mime_type,
+            "data": base64_data,
+        }
+        asyncio.run_coroutine_threadsafe(
+            self.loop.queue_realtime_input(payload),
+            self.aio_loop,
+        )
+
+    def enqueue_audio(self, base64_data: str, sample_rate: int):
+        audio_bytes = base64.b64decode(base64_data)
+        payload = {
+            "data": audio_bytes,
+            "mime_type": "audio/pcm",
+            "sample_rate": sample_rate,
+        }
+        asyncio.run_coroutine_threadsafe(
+            self.loop.queue_realtime_input(payload),
+            self.aio_loop,
+        )
+
+    def stop(self):
+        asyncio.run_coroutine_threadsafe(self._async_stop(), self.aio_loop)
+
+    async def _async_stop(self):
+        self.loop.stop()
+        await asyncio.sleep(0)
+
+
+class WebAudioLoop(AudioLoop):
+    """Extended AudioLoop class for Socket.IO bridge"""
+
+    def __init__(self, mode="camera", *, socket_sid: str):
+        super().__init__(
+            video_mode=mode,
+            external_media=True,
+            enable_local_playback=False,
+            enable_cli_text=False,
+        )
+        self.socket_sid = socket_sid
+        self.loop = None
+        self.controller = None
@@
     async def receive_audio(self):
         """Override to emit audio data to web clients"""
         while True:
@@
-                        socketio.emit('audio_data', {
+                        socketio.emit('audio_data', {
                             'data': base64.b64encode(data).decode(),
                             'sampleRate': 24000
-                        })
+                        }, room=self.socket_sid)
                         continue
                     if text := response.text:
                         print(text, end="")
                         # Emit text to web clients
-                        socketio.emit('ai_response', {'text': text})
+                        socketio.emit('ai_response', {'text': text}, room=self.socket_sid)
@@
-        self.loop = asyncio.get_event_loop()
+        self.loop = asyncio.get_event_loop()
+        self.controller = LoopController(self, self.loop)
 
         try:
             async with (
                 client.aio.live.connect(model=MODEL, config=CONFIG) as session,
                 asyncio.TaskGroup() as tg,
             ):
                 self.session = session
 
-                self.audio_in_queue = asyncio.Queue()
-                self.out_queue = asyncio.Queue(maxsize=5)
-
-                tg.create_task(self.send_text())
-                tg.create_task(self.send_realtime())
-                tg.create_task(self.listen_audio())
-                
-                if self.video_mode == "camera":
-                    tg.create_task(self.get_frames())
-                elif self.video_mode == "screen":
-                    tg.create_task(self.get_screen())
-
-                tg.create_task(self.receive_audio())
-                tg.create_task(self.play_audio())
-
-                # Wait until streaming is stopped
-                while is_streaming:
-                    await asyncio.sleep(0.1)
-                
-                raise asyncio.CancelledError("Streaming stopped")
+                self.audio_in_queue = asyncio.Queue(maxsize=200)
+                self.out_queue = asyncio.Queue(maxsize=20)
+
+                tg.create_task(self.send_text())
+                tg.create_task(self.send_realtime())
+                tg.create_task(self.receive_audio())
 
             ...
*** End Patch
```

```python
"""
Web frontend for the Gemini AI Audio/Video streaming application
"""

import os
import asyncio
import base64
import io
import json
import traceback
from dotenv import load_dotenv
from flask import Flask, render_template, request, jsonify
from flask_socketio import SocketIO, emit
import cv2
import sounddevice as sd
import numpy as np
import PIL.Image
import threading
import time

# Load environment variables from .env file
load_dotenv()

from google import genai
from google.genai import types

# Import the AudioLoop class from main.py
from main import AudioLoop, CONFIG, MODEL, client

app = Flask(__name__)
app.config['SECRET_KEY'] = 'your-secret-key-here'
socketio = SocketIO(app, cors_allowed_origins="*")

# Global variables
active_loops = {}

@app.route('/')
def index():
    return render_template('index.html')

@socketio.on('connect')
def handle_connect():
    print('Client connected')
    emit('status', {'message': 'Connected to server'})

@socketio.on('disconnect')
def handle_disconnect():
    sid = request.sid
    print(f'Client disconnected: {sid}')
    loop_entry = active_loops.pop(sid, None)
    if loop_entry:
        loop_entry.stop()

@socketio.on('start_streaming')
def handle_start_streaming(data):
    sid = request.sid

    if sid in active_loops:
        emit('error', {'message': 'Already streaming'})
        return
    
    mode = data.get('mode', 'camera')
    
    try:
        controller = LoopController(mode=mode, sid=sid)
        controller.start()
        active_loops[sid] = controller
        emit('status', {'message': f'Started streaming with {mode} mode'})

    except Exception as e:
        emit('error', {'message': f'Failed to start streaming: {str(e)}'})

@socketio.on('stop_streaming')
def handle_stop_streaming():
    global is_streaming, audio_loop
    
    sid = request.sid
    controller = active_loops.pop(sid, None)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return
    
    try:
        controller.stop()
        emit('status', {'message': 'Stopped streaming'})
        
    except Exception as e:
        emit('error', {'message': f'Failed to stop streaming: {str(e)}'})

@socketio.on('send_message')
def handle_send_message(data):
    sid = request.sid
    controller = active_loops.get(sid)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return
    
    message = data.get('message', '')
    if message:
        # Add message to the audio loop's queue
        try:
            controller.enqueue_text(message)
            emit('status', {'message': f'Sent message: {message}'})
        except Exception as e:
            emit('error', {'message': f'Failed to send message: {str(e)}'})


@socketio.on('video_frame')
def handle_video_frame(data):
    sid = request.sid
    controller = active_loops.get(sid)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return

    frame_data = data.get('data')
    mime_type = data.get('mime_type', 'image/jpeg')
    if not frame_data:
        emit('error', {'message': 'Missing frame data'})
        return

    try:
        controller.enqueue_frame(mime_type, frame_data)
    except Exception as exc:
        emit('error', {'message': f'Failed to queue frame: {exc}'})


@socketio.on('audio_chunk')
def handle_audio_chunk(data):
    sid = request.sid
    controller = active_loops.get(sid)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return

    chunk = data.get('data')
    sample_rate = data.get('sampleRate', SEND_SAMPLE_RATE)
    if not chunk:
        emit('error', {'message': 'Missing audio data'})
        return

    try:
        controller.enqueue_audio(chunk, sample_rate)
    except Exception as exc:
        emit('error', {'message': f'Failed to queue audio: {exc}'})


class LoopController:
    """Thread-safe controller bridging Flask events to AudioLoop."""

    def __init__(self, mode: str, sid: str):
        self.mode = mode
        self.sid = sid
        self.thread = None
        self.loop = None
        self.audio_loop: WebAudioLoop | None = None

    def start(self):
        def runner():
            asyncio.run(self._run())

        self.thread = threading.Thread(target=runner, daemon=True)
        self.thread.start()

    def stop(self):
        if self.audio_loop:
            self.audio_loop.stop()
        if self.loop:
            asyncio.run_coroutine_threadsafe(asyncio.sleep(0), self.loop)
        if self.thread:
            self.thread.join(timeout=1)

    def enqueue_text(self, message: str):
        if not self.audio_loop or not self.audio_loop.text_queue:
            raise RuntimeError("Text queue unavailable")
        asyncio.run_coroutine_threadsafe(
            self.audio_loop.queue_text(message),
            self.loop,
        )

    def enqueue_frame(self, mime_type: str, base64_data: str):
        payload = {"mime_type": mime_type, "data": base64_data}
        asyncio.run_coroutine_threadsafe(
            self.audio_loop.queue_realtime_input(payload),
            self.loop,
        )

    def enqueue_audio(self, base64_data: str, sample_rate: int):
        audio_bytes = base64.b64decode(base64_data)
        payload = {"data": audio_bytes, "mime_type": "audio/pcm", "sample_rate": sample_rate}
        asyncio.run_coroutine_threadsafe(
            self.audio_loop.queue_realtime_input(payload),
            self.loop,
        )

    async def _run(self):
        self.loop = asyncio.get_event_loop()
        self.audio_loop = WebAudioLoop(mode=self.mode, socket_sid=self.sid)
        await self.audio_loop.run()
    def __init__(self, mode="camera", *, socket_sid: str):
        super().__init__(
            video_mode=mode,
            external_media=True,
            enable_local_playback=False,
            enable_cli_text=False,
        )
        self.socket_sid = socket_sid
        self.loop = None
        self.text_queue = asyncio.Queue()
    
    async def send_text_message(self, message):
        """Send a text message to the AI"""
        if self.session:
            await self.session.send(input=message, end_of_turn=True)
    
    async def send_text(self):
        """Override to handle web-based text input"""
        while True:
            try:
                # Wait for messages from the web interface
                message = await self.text_queue.get()
                if message.lower() == "q":
                    break
                await self.session.send(input=message, end_of_turn=True)
            except Exception as e:
                print(f"Error in send_text: {e}")
                break
    
    async def receive_audio(self):
        """Override to emit audio data to web clients"""
        while True:
            try:
                turn = self.session.receive()
                async for response in turn:
                    if data := response.data:
                        self.audio_in_queue.put_nowait(data)
                        # Emit audio data to web clients
                        socketio.emit('audio_data', {
                            'data': base64.b64encode(data).decode(),
                            'sampleRate': 24000
                        })
                        continue
                    if text := response.text:
                        print(text, end="")
                        # Emit text to web clients
                        socketio.emit('ai_response', {'text': text})

                # If you interrupt the model, it sends a turn_complete.
                while not self.audio_in_queue.empty():
                    self.audio_in_queue.get_nowait()
                    
            except Exception as e:
                print(f"Error in receive_audio: {e}")
                break
    
    async def run(self):
        """Override run method to store the event loop"""
        self.loop = asyncio.get_event_loop()
        
        try:
            async with (
                client.aio.live.connect(model=MODEL, config=CONFIG) as session,
                asyncio.TaskGroup() as tg,
            ):
                self.session = session

                self.audio_in_queue = asyncio.Queue()
                self.out_queue = asyncio.Queue(maxsize=5)

                tg.create_task(self.send_text())
                tg.create_task(self.send_realtime())
                tg.create_task(self.listen_audio())
                
                if self.video_mode == "camera":
                    tg.create_task(self.get_frames())
                elif self.video_mode == "screen":
                    tg.create_task(self.get_screen())

                tg.create_task(self.receive_audio())
                tg.create_task(self.play_audio())

                # Wait until streaming is stopped
                while is_streaming:
                    await asyncio.sleep(0.1)
                
                raise asyncio.CancelledError("Streaming stopped")

        except asyncio.CancelledError:
            pass
        except Exception as e:
            print(f"Error in WebAudioLoop.run: {e}")
            traceback.print_exc()
        finally:
            # Cleanup
            if hasattr(self, 'input_stream') and self.input_stream:
                self.input_stream.stop()
                self.input_stream.close()
            if hasattr(self, 'output_stream') and self.output_stream:
                self.output_stream.stop()
                self.output_stream.close()


if __name__ == '__main__':
    print("Starting Gemini AI Web Interface...")
    print("Open your browser and go to http://localhost:8080")
    socketio.run(app, debug=True, host='0.0.0.0', port=8080, allow_unsafe_werkzeug=True)

```

```python
"""
Web frontend for the Gemini AI Audio/Video streaming application
"""

import os
import asyncio
import base64
import io
import json
import traceback
from dotenv import load_dotenv
from flask import Flask, render_template, request, jsonify
from flask_socketio import SocketIO, emit
import cv2
import sounddevice as sd
import numpy as np
import PIL.Image
import threading
import time

# Load environment variables from .env file
load_dotenv()

from google import genai
from google.genai import types

# Import the AudioLoop class from main.py
from main import AudioLoop, CONFIG, MODEL, client

app = Flask(__name__)
app.config['SECRET_KEY'] = 'your-secret-key-here'
socketio = SocketIO(app, cors_allowed_origins="*")

# Global variables
active_loops = {}

@app.route('/')
def index():
    return render_template('index.html')

@socketio.on('connect')
def handle_connect():
    print('Client connected')
    emit('status', {'message': 'Connected to server'})

@socketio.on('disconnect')
def handle_disconnect():
    sid = request.sid
    print(f'Client disconnected: {sid}')
    loop_entry = active_loops.pop(sid, None)
    if loop_entry:
        loop_entry.stop()

@socketio.on('start_streaming')
def handle_start_streaming(data):
    sid = request.sid

    if sid in active_loops:
        emit('error', {'message': 'Already streaming'})
        return
    
    mode = data.get('mode', 'camera')
    
    try:
        controller = LoopController(mode=mode, sid=sid)
        controller.start()
        active_loops[sid] = controller
        emit('status', {'message': f'Started streaming with {mode} mode'})

    except Exception as e:
        emit('error', {'message': f'Failed to start streaming: {str(e)}'})

@socketio.on('stop_streaming')
def handle_stop_streaming():
    global is_streaming, audio_loop
    
    sid = request.sid
    controller = active_loops.pop(sid, None)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return
    
    try:
        controller.stop()
        emit('status', {'message': 'Stopped streaming'})
        
    except Exception as e:
        emit('error', {'message': f'Failed to stop streaming: {str(e)}'})

@socketio.on('send_message')
def handle_send_message(data):
    sid = request.sid
    controller = active_loops.get(sid)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return
    
    message = data.get('message', '')
    if message:
        # Add message to the audio loop's queue
        try:
            controller.enqueue_text(message)
            emit('status', {'message': f'Sent message: {message}'})
        except Exception as e:
            emit('error', {'message': f'Failed to send message: {str(e)}'})


@socketio.on('video_frame')
def handle_video_frame(data):
    sid = request.sid
    controller = active_loops.get(sid)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return

    frame_data = data.get('data')
    mime_type = data.get('mime_type', 'image/jpeg')
    if not frame_data:
        emit('error', {'message': 'Missing frame data'})
        return

    try:
        controller.enqueue_frame(mime_type, frame_data)
    except Exception as exc:
        emit('error', {'message': f'Failed to queue frame: {exc}'})


@socketio.on('audio_chunk')
def handle_audio_chunk(data):
    sid = request.sid
    controller = active_loops.get(sid)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return

    chunk = data.get('data')
    sample_rate = data.get('sampleRate', SEND_SAMPLE_RATE)
    if not chunk:
        emit('error', {'message': 'Missing audio data'})
        return

    try:
        controller.enqueue_audio(chunk, sample_rate)
    except Exception as exc:
        emit('error', {'message': f'Failed to queue audio: {exc}'})


class LoopController:
    """Thread-safe controller bridging Flask events to AudioLoop."""

    def __init__(self, mode: str, sid: str):
        self.mode = mode
        self.sid = sid
        self.thread = None
        self.loop = None
        self.audio_loop: WebAudioLoop | None = None

    def start(self):
        def runner():
            asyncio.run(self._run())

        self.thread = threading.Thread(target=runner, daemon=True)
        self.thread.start()

    def stop(self):
        if self.audio_loop:
            self.audio_loop.stop()
        if self.loop:
            asyncio.run_coroutine_threadsafe(asyncio.sleep(0), self.loop)
        if self.thread:
            self.thread.join(timeout=1)

    def enqueue_text(self, message: str):
        if not self.audio_loop or not self.audio_loop.text_queue:
            raise RuntimeError("Text queue unavailable")
        asyncio.run_coroutine_threadsafe(
            self.audio_loop.queue_text(message),
            self.loop,
        )

    def enqueue_frame(self, mime_type: str, base64_data: str):
        payload = {"mime_type": mime_type, "data": base64_data}
        asyncio.run_coroutine_threadsafe(
            self.audio_loop.queue_realtime_input(payload),
            self.loop,
        )

    def enqueue_audio(self, base64_data: str, sample_rate: int):
        audio_bytes = base64.b64decode(base64_data)
        payload = {"data": audio_bytes, "mime_type": "audio/pcm", "sample_rate": sample_rate}
        asyncio.run_coroutine_threadsafe(
            self.audio_loop.queue_realtime_input(payload),
            self.loop,
        )

    async def _run(self):
        self.loop = asyncio.get_event_loop()
        self.audio_loop = WebAudioLoop(mode=self.mode, socket_sid=self.sid)
        await self.audio_loop.run()
    def __init__(self, mode="camera", *, socket_sid: str):
        super().__init__(
            video_mode=mode,
            external_media=True,
            enable_local_playback=False,
            enable_cli_text=False,
        )
        self.socket_sid = socket_sid
        self.loop = None
        self.text_queue = asyncio.Queue()
    
    async def send_text_message(self, message):
        """Send a text message to the AI"""
        if self.session:
            await self.session.send(input=message, end_of_turn=True)
    
    async def send_text(self):
        """Override to handle web-based text input"""
        while True:
            try:
                # Wait for messages from the web interface
                message = await self.text_queue.get()
                if message.lower() == "q":
                    break
                await self.session.send(input=message, end_of_turn=True)
            except Exception as e:
                print(f"Error in send_text: {e}")
                break
    
    async def receive_audio(self):
        """Override to emit audio data to web clients"""
        while True:
            try:
                turn = self.session.receive()
                async for response in turn:
                    if data := response.data:
                        self.audio_in_queue.put_nowait(data)
                        # Emit audio data to web clients
                        socketio.emit('audio_data', {
                            'data': base64.b64encode(data).decode(),
                            'sampleRate': 24000
                        })
                        continue
                    if text := response.text:
                        print(text, end="")
                        # Emit text to web clients
                        socketio.emit('ai_response', {'text': text})

                # If you interrupt the model, it sends a turn_complete.
                while not self.audio_in_queue.empty():
                    self.audio_in_queue.get_nowait()
                    
            except Exception as e:
                print(f"Error in receive_audio: {e}")
                break
    
    async def run(self):
        """Override run method to store the event loop"""
        self.loop = asyncio.get_event_loop()
        
        try:
            async with (
                client.aio.live.connect(model=MODEL, config=CONFIG) as session,
                asyncio.TaskGroup() as tg,
            ):
                self.session = session

                self.audio_in_queue = asyncio.Queue()
                self.out_queue = asyncio.Queue(maxsize=5)

                tg.create_task(self.send_text())
                tg.create_task(self.send_realtime())
                tg.create_task(self.listen_audio())
                
                if self.video_mode == "camera":
                    tg.create_task(self.get_frames())
                elif self.video_mode == "screen":
                    tg.create_task(self.get_screen())

                tg.create_task(self.receive_audio())
                tg.create_task(self.play_audio())

                # Wait until streaming is stopped
                while is_streaming:
                    await asyncio.sleep(0.1)
                
                raise asyncio.CancelledError("Streaming stopped")

        except asyncio.CancelledError:
            pass
        except Exception as e:
            print(f"Error in WebAudioLoop.run: {e}")
            traceback.print_exc()
        finally:
            # Cleanup
            if hasattr(self, 'input_stream') and self.input_stream:
                self.input_stream.stop()
                self.input_stream.close()
            if hasattr(self, 'output_stream') and self.output_stream:
                self.output_stream.stop()
                self.output_stream.close()


if __name__ == '__main__':
    print("Starting Gemini AI Web Interface...")
    print("Open your browser and go to http://localhost:8080")
    socketio.run(app, debug=True, host='0.0.0.0', port=8080, allow_unsafe_werkzeug=True)

```

```python
"""
## Documentation
Quickstart: https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI.py

## Setup

To install the dependencies for this script, run:

```
pip install google-genai opencv-python sounddevice pillow mss python-dotenv numpy

Important: **Use headphones**. This script uses the system default audio
input and output, which often won't include echo cancellation. So to prevent
the model from interrupting itself it is important that you use headphones. 

```
To run the script:
```
python main.py
```

The script takes a video-mode flag `--mode`, this can be "camera", "screen", or "none".
The default is "camera". To share your screen run:

```
python Get_started_LiveAPI.py --mode screen
```
"""

import os
import asyncio
import base64
import io
import traceback
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

import cv2
import sounddevice as sd
import numpy as np
import PIL.Image
import mss

import argparse

from google import genai
from google.genai import types

CHANNELS = 1
SEND_SAMPLE_RATE = 16000
RECEIVE_SAMPLE_RATE = 24000
CHUNK_SIZE = 1024
DTYPE = np.int16

MODEL = "models/gemini-2.5-flash-preview-native-audio-dialog"

DEFAULT_MODE = "camera"

client = genai.Client(
    http_options={"api_version": "v1beta"},
    api_key=os.environ.get("GEMINI_API_KEY"),
)


CONFIG = types.LiveConnectConfig(
    response_modalities=[
        "AUDIO",
    ],
    media_resolution="MEDIA_RESOLUTION_MEDIUM",
    speech_config=types.SpeechConfig(
        voice_config=types.VoiceConfig(
            prebuilt_voice_config=types.PrebuiltVoiceConfig(voice_name="Zephyr")
        )
    ),
    context_window_compression=types.ContextWindowCompressionConfig(
        trigger_tokens=25600,
        sliding_window=types.SlidingWindow(target_tokens=12800),
    ),
)

# Audio device will be selected in the AudioLoop class


class AudioLoop:
    def __init__(
        self,
        video_mode=DEFAULT_MODE,
        *,
        external_media: bool = False,
        enable_local_playback: bool = True,
        enable_cli_text: bool = True,
    ):
        self.video_mode = video_mode
        self.external_media = external_media
        self.enable_local_playback = enable_local_playback
        self.enable_cli_text = enable_cli_text

        self.session = None

        self.audio_in_queue = None
        self.out_queue = None
        self.text_queue = None

        self.input_stream = None
        self.output_stream = None

        self._shutdown_future = None
        self._loop = None

    async def send_text(self):
        if self.enable_cli_text:
            while True:
                text = await asyncio.to_thread(
                    input,
                    "message > ",
                )
                if text.lower() == "q":
                    await self.shutdown()
                    break
                await self.session.send(input=text or ".", end_of_turn=True)
        else:
            while True:
                message = await self.text_queue.get()
                if message is None:
                    break
                await self.session.send(input=message, end_of_turn=True)

    def _get_frame(self, cap):
        # Read the frameq
        ret, frame = cap.read()
        # Check if the frame was read successfully
        if not ret:
            return None
        # Fix: Convert BGR to RGB color space
        # OpenCV captures in BGR but PIL expects RGB format
        # This prevents the blue tint in the video feed
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        img = PIL.Image.fromarray(frame_rgb)  # Now using RGB frame
        img.thumbnail([1024, 1024])

        image_io = io.BytesIO()
        img.save(image_io, format="jpeg")
        image_io.seek(0)

        mime_type = "image/jpeg"
        image_bytes = image_io.read()
        return {"mime_type": mime_type, "data": base64.b64encode(image_bytes).decode()}

    async def get_frames(self):
        # This takes about a second, and will block the whole program
        # causing the audio pipeline to overflow if you don't to_thread it.
        cap = await asyncio.to_thread(
            cv2.VideoCapture, 0
        )  # 0 represents the default camera

        while True:
            frame = await asyncio.to_thread(self._get_frame, cap)
            if frame is None:
                break

            await asyncio.sleep(1.0)

            await self.out_queue.put(frame)

        # Release the VideoCapture object
        cap.release()

    def _get_screen(self):
        sct = mss.mss()
        monitor = sct.monitors[0]

        i = sct.grab(monitor)

        mime_type = "image/jpeg"
        image_bytes = mss.tools.to_png(i.rgb, i.size)
        img = PIL.Image.open(io.BytesIO(image_bytes))

        image_io = io.BytesIO()
        img.save(image_io, format="jpeg")
        image_io.seek(0)

        image_bytes = image_io.read()
        return {"mime_type": mime_type, "data": base64.b64encode(image_bytes).decode()}

    async def get_screen(self):

        while True:
            frame = await asyncio.to_thread(self._get_screen)
            if frame is None:
                break

            await asyncio.sleep(1.0)

            await self.out_queue.put(frame)

    async def send_realtime(self):
        while True:
            msg = await self.out_queue.get()
            await self.session.send(input=msg)

    async def queue_realtime_input(self, payload):
        if not self.out_queue:
            raise RuntimeError("Realtime queue not ready")
        if self.out_queue.full():
            try:
                self.out_queue.get_nowait()
            except asyncio.QueueEmpty:
                pass
        await self.out_queue.put(payload)

    async def queue_text(self, message: str, *, end_of_turn: bool = True):
        if self.enable_cli_text:
            raise RuntimeError("Text queue not available in CLI mode")
        if not self.text_queue:
            raise RuntimeError("Text queue not ready")
        if end_of_turn:
            await self.text_queue.put(message)
        else:
            await self.session.send(input=message, end_of_turn=False)

    async def listen_audio(self):
        # Create a queue for audio data
        audio_queue = asyncio.Queue()
        
        def audio_callback(indata, frames, time, status):
            if status:
                print(f"Audio input status: {status}")
            # Convert float32 to int16 and get raw bytes
            audio_data = (indata[:, 0] * 32767).astype(DTYPE).tobytes()
            try:
                audio_queue.put_nowait({"data": audio_data, "mime_type": "audio/pcm"})
            except asyncio.QueueFull:
                pass  # Drop frames if queue is full
        
        # Start the input stream
        self.input_stream = sd.InputStream(
            samplerate=SEND_SAMPLE_RATE,
            channels=CHANNELS,
            dtype='float32',
            blocksize=CHUNK_SIZE,
            callback=audio_callback
        )
        
        self.input_stream.start()
        
        try:
            while True:
                audio_data = await audio_queue.get()
                await self.queue_realtime_input(audio_data)
        finally:
            if self.input_stream:
                self.input_stream.stop()
                self.input_stream.close()
                self.input_stream = None

    async def receive_audio(self):
        while True:
            turn = self.session.receive()
            async for response in turn:
                if data := response.data:
                    await self.handle_incoming_audio(data)
                    continue
                if text := response.text:
                    await self.handle_incoming_text(text)
            await self.on_turn_complete()

    async def handle_incoming_audio(self, data: bytes):
        if not self.enable_local_playback or not self.audio_in_queue:
            return
        try:
            self.audio_in_queue.put_nowait(data)
        except asyncio.QueueFull:
            while not self.audio_in_queue.empty():
                self.audio_in_queue.get_nowait()
            try:
                self.audio_in_queue.put_nowait(data)
            except asyncio.QueueFull:
                pass

    async def handle_incoming_text(self, text: str):
        print(text, end="")

    async def on_turn_complete(self):
        if not self.enable_local_playback or not self.audio_in_queue:
            return
        while not self.audio_in_queue.empty():
            self.audio_in_queue.get_nowait()

    async def play_audio(self):
        # Create a queue for output audio data
        output_queue = asyncio.Queue()
        
        def audio_callback(outdata, frames, time, status):
            if status:
                print(f"Audio output status: {status}")
            try:
                # Get audio data from queue (non-blocking)
                audio_data = output_queue.get_nowait()
                # Convert bytes to numpy array and reshape
                audio_array = np.frombuffer(audio_data, dtype=DTYPE).astype(np.float32) / 32767.0
                # Ensure we have the right number of frames
                if len(audio_array) >= frames:
                    outdata[:, 0] = audio_array[:frames]
                else:
                    # Pad with zeros if not enough data
                    outdata[:len(audio_array), 0] = audio_array
                    outdata[len(audio_array):, 0] = 0
            except asyncio.QueueEmpty:
                # No audio data available, output silence
                outdata[:, 0] = 0
        
        # Start the output stream
        self.output_stream = sd.OutputStream(
            samplerate=RECEIVE_SAMPLE_RATE,
            channels=CHANNELS,
            dtype='float32',
            blocksize=CHUNK_SIZE,
            callback=audio_callback
        )
        
        self.output_stream.start()
        
        try:
            while True:
                bytestream = await self.audio_in_queue.get()
                await output_queue.put(bytestream)
        finally:
            if self.output_stream:
                self.output_stream.stop()
                self.output_stream.close()
            if self.output_stream:
                self.output_stream = None

    async def shutdown(self):
        if self._shutdown_future and not self._shutdown_future.done():
            self._shutdown_future.set_result(None)
        if not self.enable_cli_text and self.text_queue:
            await self.text_queue.put(None)
        await self._close_streams()

    async def _close_streams(self):
        if self.input_stream:
            self.input_stream.stop()
            self.input_stream.close()
            self.input_stream = None
        if self.output_stream:
            self.output_stream.stop()
            self.output_stream.close()
            self.output_stream = None

    async def run(self):
        try:
            async with (
                client.aio.live.connect(model=MODEL, config=CONFIG) as session,
                asyncio.TaskGroup() as tg,
            ):
                self.session = session
                loop = asyncio.get_running_loop()
                self._loop = loop
                self.audio_in_queue = asyncio.Queue(maxsize=50)
                self.out_queue = asyncio.Queue(maxsize=10)
                if not self.enable_cli_text:
                    self.text_queue = asyncio.Queue()
                else:
                    self.text_queue = None
                self._shutdown_future = loop.create_future()

                if self.enable_cli_text or self.text_queue is not None:
                    tg.create_task(self.send_text())

                tg.create_task(self.send_realtime())

                if not self.external_media:
                    tg.create_task(self.listen_audio())
                    if self.video_mode == "camera":
                        tg.create_task(self.get_frames())
                    elif self.video_mode == "screen":
                        tg.create_task(self.get_screen())

                tg.create_task(self.receive_audio())

                if self.enable_local_playback:
                    tg.create_task(self.play_audio())

                await self._shutdown_future
                raise asyncio.CancelledError("Shutdown requested")

        except asyncio.CancelledError:
            pass
        except ExceptionGroup as EG:
            traceback.print_exception(EG)
        finally:
            await self._close_streams()


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--mode",
        type=str,
        default=DEFAULT_MODE,
        help="pixels to stream from",
        choices=["camera", "screen", "none"],
    )
    args = parser.parse_args()
    main = AudioLoop(video_mode=args.mode)
    asyncio.run(main.run())

```

```python
"""
## Documentation
Quickstart: https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI.py

## Setup

To install the dependencies for this script, run:

```
pip install google-genai opencv-python sounddevice pillow mss python-dotenv numpy

Important: **Use headphones**. This script uses the system default audio
input and output, which often won't include echo cancellation. So to prevent
the model from interrupting itself it is important that you use headphones. 

```
To run the script:
```
python main.py
```

The script takes a video-mode flag `--mode`, this can be "camera", "screen", or "none".
The default is "camera". To share your screen run:

```
python Get_started_LiveAPI.py --mode screen
```
"""

import os
import asyncio
import base64
import io
import traceback
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

import cv2
import sounddevice as sd
import numpy as np
import PIL.Image
import mss

import argparse

from google import genai
from google.genai import types

CHANNELS = 1
SEND_SAMPLE_RATE = 16000
RECEIVE_SAMPLE_RATE = 24000
CHUNK_SIZE = 1024
DTYPE = np.int16

MODEL = "models/gemini-2.5-flash-preview-native-audio-dialog"

DEFAULT_MODE = "camera"

client = genai.Client(
    http_options={"api_version": "v1beta"},
    api_key=os.environ.get("GEMINI_API_KEY"),
)


CONFIG = types.LiveConnectConfig(
    response_modalities=[
        "AUDIO",
    ],
    media_resolution="MEDIA_RESOLUTION_MEDIUM",
    speech_config=types.SpeechConfig(
        voice_config=types.VoiceConfig(
            prebuilt_voice_config=types.PrebuiltVoiceConfig(voice_name="Zephyr")
        )
    ),
    context_window_compression=types.ContextWindowCompressionConfig(
        trigger_tokens=25600,
        sliding_window=types.SlidingWindow(target_tokens=12800),
    ),
)

# Audio device will be selected in the AudioLoop class


class AudioLoop:
    def __init__(
        self,
        video_mode=DEFAULT_MODE,
        *,
        external_media: bool = False,
        enable_local_playback: bool = True,
        enable_cli_text: bool = True,
    ):
        self.video_mode = video_mode
        self.external_media = external_media
        self.enable_local_playback = enable_local_playback
        self.enable_cli_text = enable_cli_text

        self.session = None

        self.audio_in_queue = None
        self.out_queue = None
        self.text_queue = None

        self.input_stream = None
        self.output_stream = None

        self._shutdown_future = None
        self._loop = None

    async def send_text(self):
        if self.enable_cli_text:
            while True:
                text = await asyncio.to_thread(
                    input,
                    "message > ",
                )
                if text.lower() == "q":
                    await self.shutdown()
                    break
                await self.session.send(input=text or ".", end_of_turn=True)
        else:
            while True:
                message = await self.text_queue.get()
                if message is None:
                    break
                await self.session.send(input=message, end_of_turn=True)

    def _get_frame(self, cap):
        # Read the frameq
        ret, frame = cap.read()
        # Check if the frame was read successfully
        if not ret:
            return None
        # Fix: Convert BGR to RGB color space
        # OpenCV captures in BGR but PIL expects RGB format
        # This prevents the blue tint in the video feed
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        img = PIL.Image.fromarray(frame_rgb)  # Now using RGB frame
        img.thumbnail([1024, 1024])

        image_io = io.BytesIO()
        img.save(image_io, format="jpeg")
        image_io.seek(0)

        mime_type = "image/jpeg"
        image_bytes = image_io.read()
        return {"mime_type": mime_type, "data": base64.b64encode(image_bytes).decode()}

    async def get_frames(self):
        # This takes about a second, and will block the whole program
        # causing the audio pipeline to overflow if you don't to_thread it.
        cap = await asyncio.to_thread(
            cv2.VideoCapture, 0
        )  # 0 represents the default camera

        while True:
            frame = await asyncio.to_thread(self._get_frame, cap)
            if frame is None:
                break

            await asyncio.sleep(1.0)

            await self.out_queue.put(frame)

        # Release the VideoCapture object
        cap.release()

    def _get_screen(self):
        sct = mss.mss()
        monitor = sct.monitors[0]

        i = sct.grab(monitor)

        mime_type = "image/jpeg"
        image_bytes = mss.tools.to_png(i.rgb, i.size)
        img = PIL.Image.open(io.BytesIO(image_bytes))

        image_io = io.BytesIO()
        img.save(image_io, format="jpeg")
        image_io.seek(0)

        image_bytes = image_io.read()
        return {"mime_type": mime_type, "data": base64.b64encode(image_bytes).decode()}

    async def get_screen(self):

        while True:
            frame = await asyncio.to_thread(self._get_screen)
            if frame is None:
                break

            await asyncio.sleep(1.0)

            await self.out_queue.put(frame)

    async def send_realtime(self):
        while True:
            msg = await self.out_queue.get()
            await self.session.send(input=msg)

    async def queue_realtime_input(self, payload):
        if not self.out_queue:
            raise RuntimeError("Realtime queue not ready")
        if self.out_queue.full():
            try:
                self.out_queue.get_nowait()
            except asyncio.QueueEmpty:
                pass
        await self.out_queue.put(payload)

    async def queue_text(self, message: str, *, end_of_turn: bool = True):
        if self.enable_cli_text:
            raise RuntimeError("Text queue not available in CLI mode")
        if not self.text_queue:
            raise RuntimeError("Text queue not ready")
        if end_of_turn:
            await self.text_queue.put(message)
        else:
            await self.session.send(input=message, end_of_turn=False)

    async def listen_audio(self):
        # Create a queue for audio data
        audio_queue = asyncio.Queue()
        
        def audio_callback(indata, frames, time, status):
            if status:
                print(f"Audio input status: {status}")
            # Convert float32 to int16 and get raw bytes
            audio_data = (indata[:, 0] * 32767).astype(DTYPE).tobytes()
            try:
                audio_queue.put_nowait({"data": audio_data, "mime_type": "audio/pcm"})
            except asyncio.QueueFull:
                pass  # Drop frames if queue is full
        
        # Start the input stream
        self.input_stream = sd.InputStream(
            samplerate=SEND_SAMPLE_RATE,
            channels=CHANNELS,
            dtype='float32',
            blocksize=CHUNK_SIZE,
            callback=audio_callback
        )
        
        self.input_stream.start()
        
        try:
            while True:
                audio_data = await audio_queue.get()
            await self.queue_realtime_input(audio_data)
        finally:
            if self.input_stream:
                self.input_stream.stop()
                self.input_stream.close()
                self.input_stream = None

    async def receive_audio(self):
        while True:
            turn = self.session.receive()
            async for response in turn:
                if data := response.data:
                    await self.handle_incoming_audio(response)
                    continue
                if text := response.text:
                    await self.handle_incoming_text(text)
                await self.handle_event(response)
            await self.on_turn_complete()

    async def handle_incoming_audio(self, response):
        if not self.enable_local_playback or not self.audio_in_queue:
            return
        try:
            self.audio_in_queue.put_nowait(response.data)
        except asyncio.QueueFull:
            while not self.audio_in_queue.empty():
                self.audio_in_queue.get_nowait()
            try:
                self.audio_in_queue.put_nowait(response.data)
            except asyncio.QueueFull:
                pass

    async def handle_incoming_text(self, text: str):
        print(text, end="")

    async def handle_event(self, response):
        if hasattr(response, "model_turn") and response.model_turn and response.model_turn.turn_complete:
            await self.on_turn_complete()

    async def on_turn_complete(self):
        if not self.enable_local_playback or not self.audio_in_queue:
            return
        while not self.audio_in_queue.empty():
            self.audio_in_queue.get_nowait()

    async def play_audio(self):
        # Create a queue for output audio data
        output_queue = asyncio.Queue()
        
        def audio_callback(outdata, frames, time, status):
            if status:
                print(f"Audio output status: {status}")
            try:
                # Get audio data from queue (non-blocking)
                audio_data = output_queue.get_nowait()
                # Convert bytes to numpy array and reshape
                audio_array = np.frombuffer(audio_data, dtype=DTYPE).astype(np.float32) / 32767.0
                # Ensure we have the right number of frames
                if len(audio_array) >= frames:
                    outdata[:, 0] = audio_array[:frames]
                else:
                    # Pad with zeros if not enough data
                    outdata[:len(audio_array), 0] = audio_array
                    outdata[len(audio_array):, 0] = 0
            except asyncio.QueueEmpty:
                # No audio data available, output silence
                outdata[:, 0] = 0
        
        # Start the output stream
        self.output_stream = sd.OutputStream(
            samplerate=RECEIVE_SAMPLE_RATE,
            channels=CHANNELS,
            dtype='float32',
            blocksize=CHUNK_SIZE,
            callback=audio_callback
        )
        
        self.output_stream.start()
        
        try:
            while True:
                bytestream = await self.audio_in_queue.get()
                await output_queue.put(bytestream)
        finally:
            if self.output_stream:
                self.output_stream.stop()
                self.output_stream.close()
            if self.output_stream:
                self.output_stream = None

    async def shutdown(self):
        if self._shutdown_future and not self._shutdown_future.done():
            self._shutdown_future.set_result(None)
        if not self.enable_cli_text and self.text_queue:
            await self.text_queue.put(None)
        await self._close_streams()

    async def _close_streams(self):
        if self.input_stream:
            self.input_stream.stop()
            self.input_stream.close()
            self.input_stream = None
        if self.output_stream:
            self.output_stream.stop()
            self.output_stream.close()
            self.output_stream = None

    async def run(self):
        try:
            async with (
                client.aio.live.connect(model=MODEL, config=CONFIG) as session,
                asyncio.TaskGroup() as tg,
            ):
                self.session = session
                loop = asyncio.get_running_loop()
                self._loop = loop
                self.audio_in_queue = asyncio.Queue(maxsize=50)
                self.out_queue = asyncio.Queue(maxsize=10)
                if not self.enable_cli_text:
                    self.text_queue = asyncio.Queue()
                else:
                    self.text_queue = None
                self._shutdown_future = loop.create_future()

                if self.enable_cli_text or self.text_queue is not None:
                    tg.create_task(self.send_text())

                tg.create_task(self.send_realtime())

                if not self.external_media:
                    tg.create_task(self.listen_audio())
                    if self.video_mode == "camera":
                        tg.create_task(self.get_frames())
                    elif self.video_mode == "screen":
                        tg.create_task(self.get_screen())

                tg.create_task(self.receive_audio())

                if self.enable_local_playback:
                    tg.create_task(self.play_audio())

                await self._shutdown_future
                raise asyncio.CancelledError("Shutdown requested")

        except asyncio.CancelledError:
            pass
        except ExceptionGroup as EG:
            traceback.print_exception(EG)
        finally:
            await self._close_streams()


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--mode",
        type=str,
        default=DEFAULT_MODE,
        help="pixels to stream from",
        choices=["camera", "screen", "none"],
    )
    args = parser.parse_args()
    main = AudioLoop(video_mode=args.mode)
    asyncio.run(main.run())

```

```python
"""
## Documentation
Quickstart: https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI.py

## Setup

To install the dependencies for this script, run:

```
pip install google-genai opencv-python sounddevice pillow mss python-dotenv numpy

Important: **Use headphones**. This script uses the system default audio
input and output, which often won't include echo cancellation. So to prevent
the model from interrupting itself it is important that you use headphones. 

```
To run the script:
```
python main.py
```

The script takes a video-mode flag `--mode`, this can be "camera", "screen", or "none".
The default is "camera". To share your screen run:

```
python Get_started_LiveAPI.py --mode screen
```
"""

import os
import asyncio
import base64
import io
import traceback
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

import cv2
import sounddevice as sd
import numpy as np
import PIL.Image
import mss

import argparse

from google import genai
from google.genai import types

CHANNELS = 1
SEND_SAMPLE_RATE = 16000
RECEIVE_SAMPLE_RATE = 24000
CHUNK_SIZE = 1024
DTYPE = np.int16

MODEL = "models/gemini-2.5-flash-preview-native-audio-dialog"

DEFAULT_MODE = "camera"

client = genai.Client(
    http_options={"api_version": "v1beta"},
    api_key=os.environ.get("GEMINI_API_KEY"),
)


CONFIG = types.LiveConnectConfig(
    response_modalities=[
        "AUDIO",
    ],
    media_resolution="MEDIA_RESOLUTION_MEDIUM",
    speech_config=types.SpeechConfig(
        voice_config=types.VoiceConfig(
            prebuilt_voice_config=types.PrebuiltVoiceConfig(voice_name="Zephyr")
        )
    ),
    context_window_compression=types.ContextWindowCompressionConfig(
        trigger_tokens=25600,
        sliding_window=types.SlidingWindow(target_tokens=12800),
    ),
)

# Audio device will be selected in the AudioLoop class


class AudioLoop:
    def __init__(
        self,
        video_mode=DEFAULT_MODE,
        *,
        external_media: bool = False,
        enable_local_playback: bool = True,
        enable_cli_text: bool = True,
    ):
        self.video_mode = video_mode
        self.external_media = external_media
        self.enable_local_playback = enable_local_playback
        self.enable_cli_text = enable_cli_text

        self.session = None

        self.audio_in_queue = None
        self.out_queue = None
        self.text_queue = None

        self.input_stream = None
        self.output_stream = None

        self._shutdown_future = None
        self._loop = None

    async def send_text(self):
        if self.enable_cli_text:
            while True:
                text = await asyncio.to_thread(
                    input,
                    "message > ",
                )
                if text.lower() == "q":
                    await self.shutdown()
                    break
                await self.session.send(input=text or ".", end_of_turn=True)
        else:
            while True:
                message = await self.text_queue.get()
                if message is None:
                    break
                await self.session.send(input=message, end_of_turn=True)

    def _get_frame(self, cap):
        # Read the frameq
        ret, frame = cap.read()
        # Check if the frame was read successfully
        if not ret:
            return None
        # Fix: Convert BGR to RGB color space
        # OpenCV captures in BGR but PIL expects RGB format
        # This prevents the blue tint in the video feed
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        img = PIL.Image.fromarray(frame_rgb)  # Now using RGB frame
        img.thumbnail([1024, 1024])

        image_io = io.BytesIO()
        img.save(image_io, format="jpeg")
        image_io.seek(0)

        mime_type = "image/jpeg"
        image_bytes = image_io.read()
        return {"mime_type": mime_type, "data": base64.b64encode(image_bytes).decode()}

    async def get_frames(self):
        # This takes about a second, and will block the whole program
        # causing the audio pipeline to overflow if you don't to_thread it.
        cap = await asyncio.to_thread(
            cv2.VideoCapture, 0
        )  # 0 represents the default camera

        while True:
            frame = await asyncio.to_thread(self._get_frame, cap)
            if frame is None:
                break

            await asyncio.sleep(1.0)

            await self.out_queue.put(frame)

        # Release the VideoCapture object
        cap.release()

    def _get_screen(self):
        sct = mss.mss()
        monitor = sct.monitors[0]

        i = sct.grab(monitor)

        mime_type = "image/jpeg"
        image_bytes = mss.tools.to_png(i.rgb, i.size)
        img = PIL.Image.open(io.BytesIO(image_bytes))

        image_io = io.BytesIO()
        img.save(image_io, format="jpeg")
        image_io.seek(0)

        image_bytes = image_io.read()
        return {"mime_type": mime_type, "data": base64.b64encode(image_bytes).decode()}

    async def get_screen(self):

        while True:
            frame = await asyncio.to_thread(self._get_screen)
            if frame is None:
                break

            await asyncio.sleep(1.0)

            await self.out_queue.put(frame)

    async def send_realtime(self):
        while True:
            msg = await self.out_queue.get()
            await self.session.send(input=msg)

    async def queue_realtime_input(self, payload):
        if not self.out_queue:
            raise RuntimeError("Realtime queue not ready")
        if self.out_queue.full():
            try:
                self.out_queue.get_nowait()
            except asyncio.QueueEmpty:
                pass
        await self.out_queue.put(payload)

    async def queue_text(self, message: str, *, end_of_turn: bool = True):
        if self.enable_cli_text:
            raise RuntimeError("Text queue not available in CLI mode")
        if not self.text_queue:
            raise RuntimeError("Text queue not ready")
        if end_of_turn:
            await self.text_queue.put(message)
        else:
            await self.session.send(input=message, end_of_turn=False)

    async def listen_audio(self):
        # Create a queue for audio data
        audio_queue = asyncio.Queue()
        
        def audio_callback(indata, frames, time, status):
            if status:
                print(f"Audio input status: {status}")
            # Convert float32 to int16 and get raw bytes
            audio_data = (indata[:, 0] * 32767).astype(DTYPE).tobytes()
            try:
                audio_queue.put_nowait({"data": audio_data, "mime_type": "audio/pcm"})
            except asyncio.QueueFull:
                pass  # Drop frames if queue is full
        
        # Start the input stream
        self.input_stream = sd.InputStream(
            samplerate=SEND_SAMPLE_RATE,
            channels=CHANNELS,
            dtype='float32',
            blocksize=CHUNK_SIZE,
            callback=audio_callback
        )
        
        self.input_stream.start()
        
        try:
            while True:
                audio_data = await audio_queue.get()
                await self.queue_realtime_input(audio_data)
        finally:
            if self.input_stream:
                self.input_stream.stop()
                self.input_stream.close()
                self.input_stream = None

    async def receive_audio(self):
        while True:
            turn = self.session.receive()
            async for response in turn:
                if data := response.data:
                    await self.handle_incoming_audio(response)
                    continue
                if text := response.text:
                    await self.handle_incoming_text(text)
                await self.handle_event(response)
            await self.on_turn_complete()

    async def handle_incoming_audio(self, response):
        if not self.enable_local_playback or not self.audio_in_queue:
            return
        try:
            self.audio_in_queue.put_nowait(response.data)
        except asyncio.QueueFull:
            while not self.audio_in_queue.empty():
                self.audio_in_queue.get_nowait()
            try:
                self.audio_in_queue.put_nowait(response.data)
            except asyncio.QueueFull:
                pass

    async def handle_incoming_text(self, text: str):
        print(text, end="")

    async def handle_event(self, response):
        if hasattr(response, "model_turn") and response.model_turn and response.model_turn.turn_complete:
            await self.on_turn_complete()

    async def on_turn_complete(self):
        if not self.enable_local_playback or not self.audio_in_queue:
            return
        while not self.audio_in_queue.empty():
            self.audio_in_queue.get_nowait()

    async def play_audio(self):
        # Create a queue for output audio data
        output_queue = asyncio.Queue()
        
        def audio_callback(outdata, frames, time, status):
            if status:
                print(f"Audio output status: {status}")
            try:
                # Get audio data from queue (non-blocking)
                audio_data = output_queue.get_nowait()
                # Convert bytes to numpy array and reshape
                audio_array = np.frombuffer(audio_data, dtype=DTYPE).astype(np.float32) / 32767.0
                # Ensure we have the right number of frames
                if len(audio_array) >= frames:
                    outdata[:, 0] = audio_array[:frames]
                else:
                    # Pad with zeros if not enough data
                    outdata[:len(audio_array), 0] = audio_array
                    outdata[len(audio_array):, 0] = 0
            except asyncio.QueueEmpty:
                # No audio data available, output silence
                outdata[:, 0] = 0
        
        # Start the output stream
        self.output_stream = sd.OutputStream(
            samplerate=RECEIVE_SAMPLE_RATE,
            channels=CHANNELS,
            dtype='float32',
            blocksize=CHUNK_SIZE,
            callback=audio_callback
        )
        
        self.output_stream.start()
        
        try:
            while True:
                bytestream = await self.audio_in_queue.get()
                await output_queue.put(bytestream)
        finally:
            if self.output_stream:
                self.output_stream.stop()
                self.output_stream.close()
            if self.output_stream:
                self.output_stream = None

    async def shutdown(self):
        if self._shutdown_future and not self._shutdown_future.done():
            self._shutdown_future.set_result(None)
        if not self.enable_cli_text and self.text_queue:
            await self.text_queue.put(None)
        await self._close_streams()

    async def _close_streams(self):
        if self.input_stream:
            self.input_stream.stop()
            self.input_stream.close()
            self.input_stream = None
        if self.output_stream:
            self.output_stream.stop()
            self.output_stream.close()
            self.output_stream = None

    async def run(self):
        try:
            async with (
                client.aio.live.connect(model=MODEL, config=CONFIG) as session,
                asyncio.TaskGroup() as tg,
            ):
                self.session = session
                loop = asyncio.get_running_loop()
                self._loop = loop
                self.audio_in_queue = asyncio.Queue(maxsize=50)
                self.out_queue = asyncio.Queue(maxsize=10)
                if not self.enable_cli_text:
                    self.text_queue = asyncio.Queue()
                else:
                    self.text_queue = None
                self._shutdown_future = loop.create_future()

                if self.enable_cli_text or self.text_queue is not None:
                    tg.create_task(self.send_text())

                tg.create_task(self.send_realtime())

                if not self.external_media:
                    tg.create_task(self.listen_audio())
                    if self.video_mode == "camera":
                        tg.create_task(self.get_frames())
                    elif self.video_mode == "screen":
                        tg.create_task(self.get_screen())

                tg.create_task(self.receive_audio())

                if self.enable_local_playback:
                    tg.create_task(self.play_audio())

                await self._shutdown_future
                raise asyncio.CancelledError("Shutdown requested")

        except asyncio.CancelledError:
            pass
        except ExceptionGroup as EG:
            traceback.print_exception(EG)
        finally:
            await self._close_streams()


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--mode",
        type=str,
        default=DEFAULT_MODE,
        help="pixels to stream from",
        choices=["camera", "screen", "none"],
    )
    args = parser.parse_args()
    main = AudioLoop(video_mode=args.mode)
    asyncio.run(main.run())

```

```python
"""
## Documentation
Quickstart: https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI.py

## Setup

To install the dependencies for this script, run:

```
pip install google-genai opencv-python sounddevice pillow mss python-dotenv numpy

Important: **Use headphones**. This script uses the system default audio
input and output, which often won't include echo cancellation. So to prevent
the model from interrupting itself it is important that you use headphones. 

```
To run the script:
```
python main.py
```

The script takes a video-mode flag `--mode`, this can be "camera", "screen", or "none".
The default is "camera". To share your screen run:

```
python Get_started_LiveAPI.py --mode screen
```
"""

import os
import asyncio
import base64
import io
import traceback
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

import cv2
import sounddevice as sd
import numpy as np
import PIL.Image
import mss

import argparse

from google import genai
from google.genai import types

CHANNELS = 1
SEND_SAMPLE_RATE = 16000
RECEIVE_SAMPLE_RATE = 24000
CHUNK_SIZE = 1024
DTYPE = np.int16

MODEL = "models/gemini-2.5-flash-preview-native-audio-dialog"

DEFAULT_MODE = "camera"

client = genai.Client(
    http_options={"api_version": "v1beta"},
    api_key=os.environ.get("GEMINI_API_KEY"),
)


CONFIG = types.LiveConnectConfig(
    response_modalities=[
        "AUDIO",
    ],
    media_resolution="MEDIA_RESOLUTION_MEDIUM",
    speech_config=types.SpeechConfig(
        voice_config=types.VoiceConfig(
            prebuilt_voice_config=types.PrebuiltVoiceConfig(voice_name="Zephyr")
        )
    ),
    context_window_compression=types.ContextWindowCompressionConfig(
        trigger_tokens=25600,
        sliding_window=types.SlidingWindow(target_tokens=12800),
    ),
)

# Audio device will be selected in the AudioLoop class


class AudioLoop:
    def __init__(
        self,
        video_mode=DEFAULT_MODE,
        *,
        external_media: bool = False,
        enable_local_playback: bool = True,
        enable_cli_text: bool = True,
    ):
        self.video_mode = video_mode
        self.external_media = external_media
        self.enable_local_playback = enable_local_playback
        self.enable_cli_text = enable_cli_text

        self.session = None

        self.audio_in_queue = None
        self.out_queue = None
        self.text_queue = None

        self.input_stream = None
        self.output_stream = None

        self._shutdown_future = None
        self._loop = None

    async def send_text(self):
        if self.enable_cli_text:
            while True:
                text = await asyncio.to_thread(
                    input,
                    "message > ",
                )
                if text.lower() == "q":
                    await self.shutdown()
                    break
                await self.session.send(input=text or ".", end_of_turn=True)
        else:
            while True:
                message = await self.text_queue.get()
                if message is None:
                    break
                await self.session.send(input=message, end_of_turn=True)

    def _get_frame(self, cap):
        # Read the frameq
        ret, frame = cap.read()
        # Check if the frame was read successfully
        if not ret:
            return None
        # Fix: Convert BGR to RGB color space
        # OpenCV captures in BGR but PIL expects RGB format
        # This prevents the blue tint in the video feed
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        img = PIL.Image.fromarray(frame_rgb)  # Now using RGB frame
        img.thumbnail([1024, 1024])

        image_io = io.BytesIO()
        img.save(image_io, format="jpeg")
        image_io.seek(0)

        mime_type = "image/jpeg"
        image_bytes = image_io.read()
        return {"mime_type": mime_type, "data": base64.b64encode(image_bytes).decode()}

    async def get_frames(self):
        # This takes about a second, and will block the whole program
        # causing the audio pipeline to overflow if you don't to_thread it.
        cap = await asyncio.to_thread(
            cv2.VideoCapture, 0
        )  # 0 represents the default camera

        while True:
            frame = await asyncio.to_thread(self._get_frame, cap)
            if frame is None:
                break

            await asyncio.sleep(1.0)

            await self.out_queue.put(frame)

        # Release the VideoCapture object
        cap.release()

    def _get_screen(self):
        sct = mss.mss()
        monitor = sct.monitors[0]

        i = sct.grab(monitor)

        mime_type = "image/jpeg"
        image_bytes = mss.tools.to_png(i.rgb, i.size)
        img = PIL.Image.open(io.BytesIO(image_bytes))

        image_io = io.BytesIO()
        img.save(image_io, format="jpeg")
        image_io.seek(0)

        image_bytes = image_io.read()
        return {"mime_type": mime_type, "data": base64.b64encode(image_bytes).decode()}

    async def get_screen(self):

        while True:
            frame = await asyncio.to_thread(self._get_screen)
            if frame is None:
                break

            await asyncio.sleep(1.0)

            await self.out_queue.put(frame)

    async def send_realtime(self):
        while True:
            msg = await self.out_queue.get()
            await self.session.send(input=msg)

    async def queue_realtime_input(self, payload):
        if not self.out_queue:
            raise RuntimeError("Realtime queue not ready")
        if self.out_queue.full():
            try:
                self.out_queue.get_nowait()
            except asyncio.QueueEmpty:
                pass
        await self.out_queue.put(payload)

    async def queue_text(self, message: str, *, end_of_turn: bool = True):
        if self.enable_cli_text:
            raise RuntimeError("Text queue not available in CLI mode")
        if not self.text_queue:
            raise RuntimeError("Text queue not ready")
        if end_of_turn:
            await self.text_queue.put(message)
        else:
            await self.session.send(input=message, end_of_turn=False)

    async def listen_audio(self):
        # Create a queue for audio data
        audio_queue = asyncio.Queue()
        
        def audio_callback(indata, frames, time, status):
            if status:
                print(f"Audio input status: {status}")
            # Convert float32 to int16 and get raw bytes
            audio_data = (indata[:, 0] * 32767).astype(DTYPE).tobytes()
            try:
                audio_queue.put_nowait({"data": audio_data, "mime_type": "audio/pcm"})
            except asyncio.QueueFull:
                pass  # Drop frames if queue is full
        
        # Start the input stream
        self.input_stream = sd.InputStream(
            samplerate=SEND_SAMPLE_RATE,
            channels=CHANNELS,
            dtype='float32',
            blocksize=CHUNK_SIZE,
            callback=audio_callback
        )
        
        self.input_stream.start()
        
        try:
            while True:
                audio_data = await audio_queue.get()
                await self.queue_realtime_input(audio_data)
        finally:
            if self.input_stream:
                self.input_stream.stop()
                self.input_stream.close()
                self.input_stream = None

    async def receive_audio(self):
        while True:
            turn = self.session.receive()
            async for response in turn:
                if data := response.data:
                    await self.handle_incoming_audio(response)
                    continue
                if text := response.text:
                    await self.handle_incoming_text(text)
                await self.handle_event(response)
            await self.on_turn_complete()

    async def handle_incoming_audio(self, response):
        if not self.enable_local_playback or not self.audio_in_queue:
            return
        try:
            self.audio_in_queue.put_nowait(response.data)
        except asyncio.QueueFull:
            while not self.audio_in_queue.empty():
                self.audio_in_queue.get_nowait()
            try:
                self.audio_in_queue.put_nowait(response.data)
            except asyncio.QueueFull:
                pass

    async def handle_incoming_text(self, text: str):
        print(text, end="")

    async def handle_event(self, response):
        if hasattr(response, "model_turn") and response.model_turn and response.model_turn.turn_complete:
            await self.on_turn_complete()

    async def on_turn_complete(self):
        if not self.enable_local_playback or not self.audio_in_queue:
            return
        while not self.audio_in_queue.empty():
            self.audio_in_queue.get_nowait()

    async def play_audio(self):
        # Create a queue for output audio data
        output_queue = asyncio.Queue()
        
        def audio_callback(outdata, frames, time, status):
            if status:
                print(f"Audio output status: {status}")
            try:
                # Get audio data from queue (non-blocking)
                audio_data = output_queue.get_nowait()
                # Convert bytes to numpy array and reshape
                audio_array = np.frombuffer(audio_data, dtype=DTYPE).astype(np.float32) / 32767.0
                # Ensure we have the right number of frames
                if len(audio_array) >= frames:
                    outdata[:, 0] = audio_array[:frames]
                else:
                    # Pad with zeros if not enough data
                    outdata[:len(audio_array), 0] = audio_array
                    outdata[len(audio_array):, 0] = 0
            except asyncio.QueueEmpty:
                # No audio data available, output silence
                outdata[:, 0] = 0
        
        # Start the output stream
        self.output_stream = sd.OutputStream(
            samplerate=RECEIVE_SAMPLE_RATE,
            channels=CHANNELS,
            dtype='float32',
            blocksize=CHUNK_SIZE,
            callback=audio_callback
        )
        
        self.output_stream.start()
        
        try:
            while True:
                bytestream = await self.audio_in_queue.get()
                await output_queue.put(bytestream)
        finally:
            if self.output_stream:
                self.output_stream.stop()
                self.output_stream.close()
            if self.output_stream:
                self.output_stream = None

    async def shutdown(self):
        if self._shutdown_future and not self._shutdown_future.done():
            self._shutdown_future.set_result(None)
        if not self.enable_cli_text and self.text_queue:
            await self.text_queue.put(None)
        await self._close_streams()

    async def _close_streams(self):
        if self.input_stream:
            self.input_stream.stop()
            self.input_stream.close()
            self.input_stream = None
        if self.output_stream:
            self.output_stream.stop()
            self.output_stream.close()
            self.output_stream = None

    async def run(self):
        try:
            async with (
                client.aio.live.connect(model=MODEL, config=CONFIG) as session,
                asyncio.TaskGroup() as tg,
            ):
                self.session = session
                loop = asyncio.get_running_loop()
                self._loop = loop
                self.audio_in_queue = asyncio.Queue(maxsize=50)
                self.out_queue = asyncio.Queue(maxsize=10)
                if not self.enable_cli_text:
                    self.text_queue = asyncio.Queue()
                else:
                    self.text_queue = None
                self._shutdown_future = loop.create_future()

                if self.enable_cli_text or self.text_queue is not None:
                    tg.create_task(self.send_text())

                tg.create_task(self.send_realtime())

                if not self.external_media:
                    tg.create_task(self.listen_audio())
                    if self.video_mode == "camera":
                        tg.create_task(self.get_frames())
                    elif self.video_mode == "screen":
                        tg.create_task(self.get_screen())

                tg.create_task(self.receive_audio())

                if self.enable_local_playback:
                    tg.create_task(self.play_audio())

                await self._shutdown_future
                raise asyncio.CancelledError("Shutdown requested")

        except asyncio.CancelledError:
            pass
        except ExceptionGroup as EG:
            traceback.print_exception(EG)
        finally:
            await self._close_streams()


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--mode",
        type=str,
        default=DEFAULT_MODE,
        help="pixels to stream from",
        choices=["camera", "screen", "none"],
    )
    args = parser.parse_args()
    main = AudioLoop(video_mode=args.mode)
    asyncio.run(main.run())

```

```python
"""
## Documentation
Quickstart: https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI.py

## Setup

To install the dependencies for this script, run:

```
pip install google-genai opencv-python sounddevice pillow mss python-dotenv numpy

Important: **Use headphones**. This script uses the system default audio
input and output, which often won't include echo cancellation. So to prevent
the model from interrupting itself it is important that you use headphones. 

```
To run the script:
```
python main.py
```

The script takes a video-mode flag `--mode`, this can be "camera", "screen", or "none".
The default is "camera". To share your screen run:

```
python Get_started_LiveAPI.py --mode screen
```
"""

import os
import asyncio
import base64
import io
import traceback
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

import cv2
import sounddevice as sd
import numpy as np
import PIL.Image
import mss

import argparse

from google import genai
from google.genai import types

CHANNELS = 1
SEND_SAMPLE_RATE = 16000
RECEIVE_SAMPLE_RATE = 24000
CHUNK_SIZE = 1024
DTYPE = np.int16

MODEL = "models/gemini-2.5-flash-preview-native-audio-dialog"

DEFAULT_MODE = "camera"

client = genai.Client(
    http_options={"api_version": "v1beta"},
    api_key=os.environ.get("GEMINI_API_KEY"),
)


CONFIG = types.LiveConnectConfig(
    response_modalities=[
        "AUDIO",
    ],
    media_resolution="MEDIA_RESOLUTION_MEDIUM",
    speech_config=types.SpeechConfig(
        voice_config=types.VoiceConfig(
            prebuilt_voice_config=types.PrebuiltVoiceConfig(voice_name="Zephyr")
        )
    ),
    context_window_compression=types.ContextWindowCompressionConfig(
        trigger_tokens=25600,
        sliding_window=types.SlidingWindow(target_tokens=12800),
    ),
)

# Audio device will be selected in the AudioLoop class


class AudioLoop:
    def __init__(
        self,
        video_mode=DEFAULT_MODE,
        *,
        external_media: bool = False,
        enable_local_playback: bool = True,
        enable_cli_text: bool = True,
    ):
        self.video_mode = video_mode
        self.external_media = external_media
        self.enable_local_playback = enable_local_playback
        self.enable_cli_text = enable_cli_text

        self.session = None

        self.audio_in_queue = None
        self.out_queue = None
        self.text_queue = None

        self.input_stream = None
        self.output_stream = None

        self._shutdown_future = None
        self._loop = None

    async def send_text(self):
        if self.enable_cli_text:
            while True:
                text = await asyncio.to_thread(
                    input,
                    "message > ",
                )
                if text.lower() == "q":
                    await self.shutdown()
                    break
                await self.session.send(input=text or ".", end_of_turn=True)
        else:
            while True:
                message = await self.text_queue.get()
                if message is None:
                    break
                await self.session.send(input=message, end_of_turn=True)

    def _get_frame(self, cap):
        # Read the frameq
        ret, frame = cap.read()
        # Check if the frame was read successfully
        if not ret:
            return None
        # Fix: Convert BGR to RGB color space
        # OpenCV captures in BGR but PIL expects RGB format
        # This prevents the blue tint in the video feed
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        img = PIL.Image.fromarray(frame_rgb)  # Now using RGB frame
        img.thumbnail([1024, 1024])

        image_io = io.BytesIO()
        img.save(image_io, format="jpeg")
        image_io.seek(0)

        mime_type = "image/jpeg"
        image_bytes = image_io.read()
        return {"mime_type": mime_type, "data": base64.b64encode(image_bytes).decode()}

    async def get_frames(self):
        # This takes about a second, and will block the whole program
        # causing the audio pipeline to overflow if you don't to_thread it.
        cap = await asyncio.to_thread(
            cv2.VideoCapture, 0
        )  # 0 represents the default camera

        while True:
            frame = await asyncio.to_thread(self._get_frame, cap)
            if frame is None:
                break

            await asyncio.sleep(1.0)

            await self.out_queue.put(frame)

        # Release the VideoCapture object
        cap.release()

    def _get_screen(self):
        sct = mss.mss()
        monitor = sct.monitors[0]

        i = sct.grab(monitor)

        mime_type = "image/jpeg"
        image_bytes = mss.tools.to_png(i.rgb, i.size)
        img = PIL.Image.open(io.BytesIO(image_bytes))

        image_io = io.BytesIO()
        img.save(image_io, format="jpeg")
        image_io.seek(0)

        image_bytes = image_io.read()
        return {"mime_type": mime_type, "data": base64.b64encode(image_bytes).decode()}

    async def get_screen(self):

        while True:
            frame = await asyncio.to_thread(self._get_screen)
            if frame is None:
                break

            await asyncio.sleep(1.0)

            await self.out_queue.put(frame)

    async def send_realtime(self):
        while True:
            msg = await self.out_queue.get()
            await self.session.send(input=msg)

    async def queue_realtime_input(self, payload):
        if not self.out_queue:
            raise RuntimeError("Realtime queue not ready")
        if self.out_queue.full():
            try:
                self.out_queue.get_nowait()
            except asyncio.QueueEmpty:
                pass
        await self.out_queue.put(payload)

    async def queue_text(self, message: str, *, end_of_turn: bool = True):
        if self.enable_cli_text:
            raise RuntimeError("Text queue not available in CLI mode")
        if not self.text_queue:
            raise RuntimeError("Text queue not ready")
        if end_of_turn:
            await self.text_queue.put(message)
        else:
            await self.session.send(input=message, end_of_turn=False)

    async def listen_audio(self):
        # Create a queue for audio data
        audio_queue = asyncio.Queue()
        
        def audio_callback(indata, frames, time, status):
            if status:
                print(f"Audio input status: {status}")
            # Convert float32 to int16 and get raw bytes
            audio_data = (indata[:, 0] * 32767).astype(DTYPE).tobytes()
            try:
                audio_queue.put_nowait({"data": audio_data, "mime_type": "audio/pcm"})
            except asyncio.QueueFull:
                pass  # Drop frames if queue is full
        
        # Start the input stream
        self.input_stream = sd.InputStream(
            samplerate=SEND_SAMPLE_RATE,
            channels=CHANNELS,
            dtype='float32',
            blocksize=CHUNK_SIZE,
            callback=audio_callback
        )
        
        self.input_stream.start()
        
        try:
            while True:
                audio_data = await audio_queue.get()
                await self.queue_realtime_input(audio_data)
        finally:
            if self.input_stream:
                self.input_stream.stop()
                self.input_stream.close()
                self.input_stream = None

    async def receive_audio(self):
        while True:
            turn = self.session.receive()
            async for response in turn:
                if data := response.data:
                    await self.handle_incoming_audio(response)
                    continue
                if text := response.text:
                    await self.handle_incoming_text(text)
                await self.handle_event(response)
            await self.on_turn_complete()

    async def handle_incoming_audio(self, response):
        if not self.enable_local_playback or not self.audio_in_queue:
            return
        try:
            self.audio_in_queue.put_nowait(response.data)
        except asyncio.QueueFull:
            while not self.audio_in_queue.empty():
                self.audio_in_queue.get_nowait()
            try:
                self.audio_in_queue.put_nowait(response.data)
            except asyncio.QueueFull:
                pass

    async def handle_incoming_text(self, text: str):
        print(text, end="")

    async def handle_event(self, response):
        if hasattr(response, "model_turn") and response.model_turn and response.model_turn.turn_complete:
            await self.on_turn_complete()

    async def on_turn_complete(self):
        if not self.enable_local_playback or not self.audio_in_queue:
            return
        while not self.audio_in_queue.empty():
            self.audio_in_queue.get_nowait()

    async def play_audio(self):
        # Create a queue for output audio data
        output_queue = asyncio.Queue()
        
        def audio_callback(outdata, frames, time, status):
            if status:
                print(f"Audio output status: {status}")
            try:
                # Get audio data from queue (non-blocking)
                audio_data = output_queue.get_nowait()
                # Convert bytes to numpy array and reshape
                audio_array = np.frombuffer(audio_data, dtype=DTYPE).astype(np.float32) / 32767.0
                # Ensure we have the right number of frames
                if len(audio_array) >= frames:
                    outdata[:, 0] = audio_array[:frames]
                else:
                    # Pad with zeros if not enough data
                    outdata[:len(audio_array), 0] = audio_array
                    outdata[len(audio_array):, 0] = 0
            except asyncio.QueueEmpty:
                # No audio data available, output silence
                outdata[:, 0] = 0
        
        # Start the output stream
        self.output_stream = sd.OutputStream(
            samplerate=RECEIVE_SAMPLE_RATE,
            channels=CHANNELS,
            dtype='float32',
            blocksize=CHUNK_SIZE,
            callback=audio_callback
        )
        
        self.output_stream.start()
        
        try:
            while True:
                bytestream = await self.audio_in_queue.get()
                await output_queue.put(bytestream)
        finally:
            if self.output_stream:
                self.output_stream.stop()
                self.output_stream.close()
            if self.output_stream:
                self.output_stream = None

    async def shutdown(self):
        if self._shutdown_future and not self._shutdown_future.done():
            self._shutdown_future.set_result(None)
        if not self.enable_cli_text and self.text_queue:
            await self.text_queue.put(None)
        await self._close_streams()

    async def _close_streams(self):
        if self.input_stream:
            self.input_stream.stop()
            self.input_stream.close()
            self.input_stream = None
        if self.output_stream:
            self.output_stream.stop()
            self.output_stream.close()
            self.output_stream = None

    async def run(self):
        try:
            async with (
                client.aio.live.connect(model=MODEL, config=CONFIG) as session,
                asyncio.TaskGroup() as tg,
            ):
                self.session = session
                loop = asyncio.get_running_loop()
                self._loop = loop
                self.audio_in_queue = asyncio.Queue(maxsize=50)
                self.out_queue = asyncio.Queue(maxsize=10)
                if not self.enable_cli_text:
                    self.text_queue = asyncio.Queue()
                else:
                    self.text_queue = None
                self._shutdown_future = loop.create_future()

                if self.enable_cli_text or self.text_queue is not None:
                    tg.create_task(self.send_text())

                tg.create_task(self.send_realtime())

                if not self.external_media:
                    tg.create_task(self.listen_audio())
                    if self.video_mode == "camera":
                        tg.create_task(self.get_frames())
                    elif self.video_mode == "screen":
                        tg.create_task(self.get_screen())

                tg.create_task(self.receive_audio())

                if self.enable_local_playback:
                    tg.create_task(self.play_audio())

                await self._shutdown_future
                raise asyncio.CancelledError("Shutdown requested")

        except asyncio.CancelledError:
            pass
        except ExceptionGroup as EG:
            traceback.print_exception(EG)
        finally:
            await self._close_streams()


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--mode",
        type=str,
        default=DEFAULT_MODE,
        help="pixels to stream from",
        choices=["camera", "screen", "none"],
    )
    args = parser.parse_args()
    main = AudioLoop(video_mode=args.mode)
    asyncio.run(main.run())

```

```python
"""
## Documentation
Quickstart: https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI.py

## Setup

To install the dependencies for this script, run:

```
pip install google-genai opencv-python sounddevice pillow mss python-dotenv numpy

Important: **Use headphones**. This script uses the system default audio
input and output, which often won't include echo cancellation. So to prevent
the model from interrupting itself it is important that you use headphones. 

```
To run the script:
```
python main.py
```

The script takes a video-mode flag `--mode`, this can be "camera", "screen", or "none".
The default is "camera". To share your screen run:

```
python Get_started_LiveAPI.py --mode screen
```
"""

import os
import asyncio
import base64
import io
import traceback
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

import cv2
import sounddevice as sd
import numpy as np
import PIL.Image
import mss

import argparse

from google import genai
from google.genai import types

CHANNELS = 1
SEND_SAMPLE_RATE = 16000
RECEIVE_SAMPLE_RATE = 24000
CHUNK_SIZE = 1024
DTYPE = np.int16

MODEL = "models/gemini-2.5-flash-preview-native-audio-dialog"

DEFAULT_MODE = "camera"

client = genai.Client(
    http_options={"api_version": "v1beta"},
    api_key=os.environ.get("GEMINI_API_KEY"),
)


CONFIG = types.LiveConnectConfig(
    response_modalities=[
        "AUDIO",
    ],
    media_resolution="MEDIA_RESOLUTION_MEDIUM",
    speech_config=types.SpeechConfig(
        voice_config=types.VoiceConfig(
            prebuilt_voice_config=types.PrebuiltVoiceConfig(voice_name="Zephyr")
        )
    ),
    context_window_compression=types.ContextWindowCompressionConfig(
        trigger_tokens=25600,
        sliding_window=types.SlidingWindow(target_tokens=12800),
    ),
)

# Audio device will be selected in the AudioLoop class


class AudioLoop:
    def __init__(
        self,
        video_mode=DEFAULT_MODE,
        *,
        external_media: bool = False,
        enable_local_playback: bool = True,
        enable_cli_text: bool = True,
    ):
        self.video_mode = video_mode
        self.external_media = external_media
        self.enable_local_playback = enable_local_playback
        self.enable_cli_text = enable_cli_text

        self.session = None

        self.audio_in_queue = None
        self.out_queue = None
        self.text_queue = None

        self.input_stream = None
        self.output_stream = None

        self._shutdown_future = None
        self._loop = None

    async def send_text(self):
        if self.enable_cli_text:
            while True:
                text = await asyncio.to_thread(
                    input,
                    "message > ",
                )
                if text.lower() == "q":
                    await self.shutdown()
                    break
                await self.session.send(input=text or ".", end_of_turn=True)
        else:
            while True:
                message = await self.text_queue.get()
                if message is None:
                    break
                await self.session.send(input=message, end_of_turn=True)

    def _get_frame(self, cap):
        # Read the frameq
        ret, frame = cap.read()
        # Check if the frame was read successfully
        if not ret:
            return None
        # Fix: Convert BGR to RGB color space
        # OpenCV captures in BGR but PIL expects RGB format
        # This prevents the blue tint in the video feed
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        img = PIL.Image.fromarray(frame_rgb)  # Now using RGB frame
        img.thumbnail([1024, 1024])

        image_io = io.BytesIO()
        img.save(image_io, format="jpeg")
        image_io.seek(0)

        mime_type = "image/jpeg"
        image_bytes = image_io.read()
        return {"mime_type": mime_type, "data": base64.b64encode(image_bytes).decode()}

    async def get_frames(self):
        # This takes about a second, and will block the whole program
        # causing the audio pipeline to overflow if you don't to_thread it.
        cap = await asyncio.to_thread(
            cv2.VideoCapture, 0
        )  # 0 represents the default camera

        while True:
            frame = await asyncio.to_thread(self._get_frame, cap)
            if frame is None:
                break

            await asyncio.sleep(1.0)

            await self.out_queue.put(frame)

        # Release the VideoCapture object
        cap.release()

    def _get_screen(self):
        sct = mss.mss()
        monitor = sct.monitors[0]

        i = sct.grab(monitor)

        mime_type = "image/jpeg"
        image_bytes = mss.tools.to_png(i.rgb, i.size)
        img = PIL.Image.open(io.BytesIO(image_bytes))

        image_io = io.BytesIO()
        img.save(image_io, format="jpeg")
        image_io.seek(0)

        image_bytes = image_io.read()
        return {"mime_type": mime_type, "data": base64.b64encode(image_bytes).decode()}

    async def get_screen(self):

        while True:
            frame = await asyncio.to_thread(self._get_screen)
            if frame is None:
                break

            await asyncio.sleep(1.0)

            await self.out_queue.put(frame)

    async def send_realtime(self):
        while True:
            msg = await self.out_queue.get()
            await self.session.send(input=msg)

    async def queue_realtime_input(self, payload):
        if not self.out_queue:
            raise RuntimeError("Realtime queue not ready")
        if self.out_queue.full():
            try:
                self.out_queue.get_nowait()
            except asyncio.QueueEmpty:
                pass
        await self.out_queue.put(payload)

    async def queue_text(self, message: str, *, end_of_turn: bool = True):
        if self.enable_cli_text:
            raise RuntimeError("Text queue not available in CLI mode")
        if not self.text_queue:
            raise RuntimeError("Text queue not ready")
        if end_of_turn:
            await self.text_queue.put(message)
        else:
            await self.session.send(input=message, end_of_turn=False)

    async def listen_audio(self):
        # Create a queue for audio data
        audio_queue = asyncio.Queue()
        
        def audio_callback(indata, frames, time, status):
            if status:
                print(f"Audio input status: {status}")
            # Convert float32 to int16 and get raw bytes
            audio_data = (indata[:, 0] * 32767).astype(DTYPE).tobytes()
            try:
                audio_queue.put_nowait({"data": audio_data, "mime_type": "audio/pcm"})
            except asyncio.QueueFull:
                pass  # Drop frames if queue is full
        
        # Start the input stream
        self.input_stream = sd.InputStream(
            samplerate=SEND_SAMPLE_RATE,
            channels=CHANNELS,
            dtype='float32',
            blocksize=CHUNK_SIZE,
            callback=audio_callback
        )
        
        self.input_stream.start()
        
        try:
            while True:
                audio_data = await audio_queue.get()
                await self.queue_realtime_input(audio_data)
        finally:
            if self.input_stream:
                self.input_stream.stop()
                self.input_stream.close()
                self.input_stream = None

    async def receive_audio(self):
        while True:
            turn = self.session.receive()
            async for response in turn:
                if data := response.data:
                    await self.handle_incoming_audio(response)
                    continue
                if text := response.text:
                    await self.handle_incoming_text(text)
                await self.handle_event(response)
            await self.on_turn_complete()

    async def handle_incoming_audio(self, response):
        if not self.enable_local_playback or not self.audio_in_queue:
            return
        try:
            self.audio_in_queue.put_nowait(response.data)
        except asyncio.QueueFull:
            while not self.audio_in_queue.empty():
                self.audio_in_queue.get_nowait()
            try:
                self.audio_in_queue.put_nowait(response.data)
            except asyncio.QueueFull:
                pass

    async def handle_incoming_text(self, text: str):
        print(text, end="")

    async def handle_event(self, response):
        if hasattr(response, "model_turn") and response.model_turn and response.model_turn.turn_complete:
            await self.on_turn_complete()

    async def on_turn_complete(self):
        if not self.enable_local_playback or not self.audio_in_queue:
            return
        while not self.audio_in_queue.empty():
            self.audio_in_queue.get_nowait()

    async def play_audio(self):
        # Create a queue for output audio data
        output_queue = asyncio.Queue()
        
        def audio_callback(outdata, frames, time, status):
            if status:
                print(f"Audio output status: {status}")
            try:
                # Get audio data from queue (non-blocking)
                audio_data = output_queue.get_nowait()
                # Convert bytes to numpy array and reshape
                audio_array = np.frombuffer(audio_data, dtype=DTYPE).astype(np.float32) / 32767.0
                # Ensure we have the right number of frames
                if len(audio_array) >= frames:
                    outdata[:, 0] = audio_array[:frames]
                else:
                    # Pad with zeros if not enough data
                    outdata[:len(audio_array), 0] = audio_array
                    outdata[len(audio_array):, 0] = 0
            except asyncio.QueueEmpty:
                # No audio data available, output silence
                outdata[:, 0] = 0
        
        # Start the output stream
        self.output_stream = sd.OutputStream(
            samplerate=RECEIVE_SAMPLE_RATE,
            channels=CHANNELS,
            dtype='float32',
            blocksize=CHUNK_SIZE,
            callback=audio_callback
        )
        
        self.output_stream.start()
        
        try:
            while True:
                bytestream = await self.audio_in_queue.get()
                await output_queue.put(bytestream)
        finally:
            if self.output_stream:
                self.output_stream.stop()
                self.output_stream.close()
            if self.output_stream:
                self.output_stream = None

    async def shutdown(self):
        if self._shutdown_future and not self._shutdown_future.done():
            self._shutdown_future.set_result(None)
        if not self.enable_cli_text and self.text_queue:
            await self.text_queue.put(None)
        await self._close_streams()

    async def _close_streams(self):
        if self.input_stream:
            self.input_stream.stop()
            self.input_stream.close()
            self.input_stream = None
        if self.output_stream:
            self.output_stream.stop()
            self.output_stream.close()
            self.output_stream = None

    async def run(self):
        try:
            async with (
                client.aio.live.connect(model=MODEL, config=CONFIG) as session,
                asyncio.TaskGroup() as tg,
            ):
                self.session = session
                loop = asyncio.get_running_loop()
                self._loop = loop
                self.audio_in_queue = asyncio.Queue(maxsize=50)
                self.out_queue = asyncio.Queue(maxsize=10)
                if not self.enable_cli_text:
                    self.text_queue = asyncio.Queue()
                else:
                    self.text_queue = None
                self._shutdown_future = loop.create_future()

                if self.enable_cli_text or self.text_queue is not None:
                    tg.create_task(self.send_text())

                tg.create_task(self.send_realtime())

                if not self.external_media:
                    tg.create_task(self.listen_audio())
                    if self.video_mode == "camera":
                        tg.create_task(self.get_frames())
                    elif self.video_mode == "screen":
                        tg.create_task(self.get_screen())

                tg.create_task(self.receive_audio())

                if self.enable_local_playback:
                    tg.create_task(self.play_audio())

                await self._shutdown_future
                raise asyncio.CancelledError("Shutdown requested")

        except asyncio.CancelledError:
            pass
        except ExceptionGroup as EG:
            traceback.print_exception(EG)
        finally:
            await self._close_streams()


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--mode",
        type=str,
        default=DEFAULT_MODE,
        help="pixels to stream from",
        choices=["camera", "screen", "none"],
    )
    args = parser.parse_args()
    main = AudioLoop(video_mode=args.mode)
    asyncio.run(main.run())

```

```python
"""
## Documentation
Quickstart: https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI.py

## Setup

To install the dependencies for this script, run:

```
pip install google-genai opencv-python sounddevice pillow mss python-dotenv numpy

Important: **Use headphones**. This script uses the system default audio
input and output, which often won't include echo cancellation. So to prevent
the model from interrupting itself it is important that you use headphones. 

```
To run the script:
```
python main.py
```

The script takes a video-mode flag `--mode`, this can be "camera", "screen", or "none".
The default is "camera". To share your screen run:

```
python Get_started_LiveAPI.py --mode screen
```
"""

import os
import asyncio
import base64
import io
import traceback
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

import cv2
import sounddevice as sd
import numpy as np
import PIL.Image
import mss

import argparse

from google import genai
from google.genai import types

CHANNELS = 1
SEND_SAMPLE_RATE = 16000
RECEIVE_SAMPLE_RATE = 24000
CHUNK_SIZE = 1024
DTYPE = np.int16

MODEL = "models/gemini-2.5-flash-preview-native-audio-dialog"

DEFAULT_MODE = "camera"

client = genai.Client(
    http_options={"api_version": "v1beta"},
    api_key=os.environ.get("GEMINI_API_KEY"),
)


CONFIG = types.LiveConnectConfig(
    response_modalities=[
        "AUDIO",
    ],
    media_resolution="MEDIA_RESOLUTION_MEDIUM",
    speech_config=types.SpeechConfig(
        voice_config=types.VoiceConfig(
            prebuilt_voice_config=types.PrebuiltVoiceConfig(voice_name="Zephyr")
        )
    ),
    context_window_compression=types.ContextWindowCompressionConfig(
        trigger_tokens=25600,
        sliding_window=types.SlidingWindow(target_tokens=12800),
    ),
)

# Audio device will be selected in the AudioLoop class


class AudioLoop:
    def __init__(
        self,
        video_mode=DEFAULT_MODE,
        *,
        external_media: bool = False,
        enable_local_playback: bool = True,
        enable_cli_text: bool = True,
    ):
        self.video_mode = video_mode
        self.external_media = external_media
        self.enable_local_playback = enable_local_playback
        self.enable_cli_text = enable_cli_text

        self.session: genai.live.LiveConnectSession | None = None

        self.audio_in_queue: asyncio.Queue | None = None
        self.out_queue: asyncio.Queue | None = None
        self.text_queue: asyncio.Queue | None = None

        self.input_stream = None
        self.output_stream = None

        self._shutdown_future: asyncio.Future | None = None

    async def send_text(self):
        if self.enable_cli_text:
            while True:
                text = await asyncio.to_thread(
                    input,
                    "message > ",
                )
                if text.lower() == "q":
                    await self.shutdown()
                    break
                await self.session.send(input=text or ".", end_of_turn=True)
        else:
            while True:
                message = await self.text_queue.get()
                if message is None:
                    break
                await self.session.send(input=message, end_of_turn=True)

    def _get_frame(self, cap):
        # Read the frameq
        ret, frame = cap.read()
        # Check if the frame was read successfully
        if not ret:
            return None
        # Fix: Convert BGR to RGB color space
        # OpenCV captures in BGR but PIL expects RGB format
        # This prevents the blue tint in the video feed
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        img = PIL.Image.fromarray(frame_rgb)  # Now using RGB frame
        img.thumbnail([1024, 1024])

        image_io = io.BytesIO()
        img.save(image_io, format="jpeg")
        image_io.seek(0)

        mime_type = "image/jpeg"
        image_bytes = image_io.read()
        return {"mime_type": mime_type, "data": base64.b64encode(image_bytes).decode()}

    async def get_frames(self):
        # This takes about a second, and will block the whole program
        # causing the audio pipeline to overflow if you don't to_thread it.
        cap = await asyncio.to_thread(
            cv2.VideoCapture, 0
        )  # 0 represents the default camera

        while True:
            frame = await asyncio.to_thread(self._get_frame, cap)
            if frame is None:
                break

            await asyncio.sleep(1.0)

            await self.out_queue.put(frame)

        # Release the VideoCapture object
        cap.release()

    def _get_screen(self):
        sct = mss.mss()
        monitor = sct.monitors[0]

        i = sct.grab(monitor)

        mime_type = "image/jpeg"
        image_bytes = mss.tools.to_png(i.rgb, i.size)
        img = PIL.Image.open(io.BytesIO(image_bytes))

        image_io = io.BytesIO()
        img.save(image_io, format="jpeg")
        image_io.seek(0)

        image_bytes = image_io.read()
        return {"mime_type": mime_type, "data": base64.b64encode(image_bytes).decode()}

    async def get_screen(self):

        while True:
            frame = await asyncio.to_thread(self._get_screen)
            if frame is None:
                break

            await asyncio.sleep(1.0)

            await self.out_queue.put(frame)

    async def send_realtime(self):
        while True:
            msg = await self.out_queue.get()
            await self.session.send(input=msg)

    async def queue_realtime_input(self, payload):
        if not self.out_queue:
            raise RuntimeError("Realtime queue not ready")
        if self.out_queue.full():
            try:
                self.out_queue.get_nowait()
            except asyncio.QueueEmpty:
                pass
        await self.out_queue.put(payload)

    async def queue_text(self, message: str, *, end_of_turn: bool = True):
        if self.enable_cli_text:
            raise RuntimeError("Text queue not available in CLI mode")
        if not self.text_queue:
            raise RuntimeError("Text queue not ready")
        if end_of_turn:
            await self.text_queue.put(message)
        else:
            await self.session.send(input=message, end_of_turn=False)

    async def listen_audio(self):
        # Create a queue for audio data
        audio_queue = asyncio.Queue()
        
        def audio_callback(indata, frames, time, status):
            if status:
                print(f"Audio input status: {status}")
            # Convert float32 to int16 and get raw bytes
            audio_data = (indata[:, 0] * 32767).astype(DTYPE).tobytes()
            try:
                audio_queue.put_nowait({"data": audio_data, "mime_type": "audio/pcm"})
            except asyncio.QueueFull:
                pass  # Drop frames if queue is full
        
        # Start the input stream
        self.input_stream = sd.InputStream(
            samplerate=SEND_SAMPLE_RATE,
            channels=CHANNELS,
            dtype='float32',
            blocksize=CHUNK_SIZE,
            callback=audio_callback
        )
        
        self.input_stream.start()
        
        try:
            while True:
                audio_data = await audio_queue.get()
                await self.queue_realtime_input(audio_data)
        finally:
            if self.input_stream:
                self.input_stream.stop()
                self.input_stream.close()
                self.input_stream = None

    async def receive_audio(self):
        while True:
            turn = self.session.receive()
            async for response in turn:
                if data := response.data:
                    await self.handle_incoming_audio(response)
                    continue
                if text := response.text:
                    await self.handle_incoming_text(text)
                await self.handle_event(response)
            await self.on_turn_complete()

    async def handle_incoming_audio(self, response):
        if not self.enable_local_playback or not self.audio_in_queue:
            return
        try:
            self.audio_in_queue.put_nowait(response.data)
        except asyncio.QueueFull:
            while not self.audio_in_queue.empty():
                self.audio_in_queue.get_nowait()
            try:
                self.audio_in_queue.put_nowait(response.data)
            except asyncio.QueueFull:
                pass

    async def handle_incoming_text(self, text: str):
        print(text, end="")

    async def handle_event(self, response):
        if hasattr(response, "model_turn") and response.model_turn and response.model_turn.turn_complete:
            await self.on_turn_complete()

    async def on_turn_complete(self):
        if not self.enable_local_playback or not self.audio_in_queue:
            return
        while not self.audio_in_queue.empty():
            self.audio_in_queue.get_nowait()

    async def play_audio(self):
        # Create a queue for output audio data
        output_queue = asyncio.Queue()
        
        def audio_callback(outdata, frames, time, status):
            if status:
                print(f"Audio output status: {status}")
            try:
                # Get audio data from queue (non-blocking)
                audio_data = output_queue.get_nowait()
                # Convert bytes to numpy array and reshape
                audio_array = np.frombuffer(audio_data, dtype=DTYPE).astype(np.float32) / 32767.0
                # Ensure we have the right number of frames
                if len(audio_array) >= frames:
                    outdata[:, 0] = audio_array[:frames]
                else:
                    # Pad with zeros if not enough data
                    outdata[:len(audio_array), 0] = audio_array
                    outdata[len(audio_array):, 0] = 0
            except asyncio.QueueEmpty:
                # No audio data available, output silence
                outdata[:, 0] = 0
        
        # Start the output stream
        self.output_stream = sd.OutputStream(
            samplerate=RECEIVE_SAMPLE_RATE,
            channels=CHANNELS,
            dtype='float32',
            blocksize=CHUNK_SIZE,
            callback=audio_callback
        )
        
        self.output_stream.start()
        
        try:
            while True:
                bytestream = await self.audio_in_queue.get()
                await output_queue.put(bytestream)
        finally:
            if self.output_stream:
                self.output_stream.stop()
                self.output_stream.close()
            if self.output_stream:
                self.output_stream = None

    async def shutdown(self):
        if self._shutdown_future and not self._shutdown_future.done():
            self._shutdown_future.set_result(None)
        if not self.enable_cli_text and self.text_queue:
            await self.text_queue.put(None)
        await self._close_streams()

    async def _close_streams(self):
        if self.input_stream:
            self.input_stream.stop()
            self.input_stream.close()
            self.input_stream = None
        if self.output_stream:
            self.output_stream.stop()
            self.output_stream.close()
            self.output_stream = None

    async def run(self):
        try:
            async with (
                client.aio.live.connect(model=MODEL, config=CONFIG) as session,
                asyncio.TaskGroup() as tg,
            ):
                self.session = session
                loop = asyncio.get_running_loop()
                self._loop = loop
                self.audio_in_queue = asyncio.Queue(maxsize=50)
                self.out_queue = asyncio.Queue(maxsize=10)
                if not self.enable_cli_text:
                    self.text_queue = asyncio.Queue()
                else:
                    self.text_queue = None
                self._shutdown_future = loop.create_future()

                if self.enable_cli_text or self.text_queue is not None:
                    tg.create_task(self.send_text())

                tg.create_task(self.send_realtime())

                if not self.external_media:
                    tg.create_task(self.listen_audio())
                    if self.video_mode == "camera":
                        tg.create_task(self.get_frames())
                    elif self.video_mode == "screen":
                        tg.create_task(self.get_screen())

                tg.create_task(self.receive_audio())

                if self.enable_local_playback:
                    tg.create_task(self.play_audio())

                await self._shutdown_future
                raise asyncio.CancelledError("Shutdown requested")

        except asyncio.CancelledError:
            pass
        except ExceptionGroup as EG:
            traceback.print_exception(EG)
        finally:
            await self._close_streams()


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--mode",
        type=str,
        default=DEFAULT_MODE,
        help="pixels to stream from",
        choices=["camera", "screen", "none"],
    )
    args = parser.parse_args()
    main = AudioLoop(video_mode=args.mode)
    asyncio.run(main.run())

```

```python
"""
Web frontend for the Gemini AI Audio/Video streaming application
"""

import os
import asyncio
import base64
import io
import json
import traceback
from dotenv import load_dotenv
from flask import Flask, render_template, request, jsonify
from flask_socketio import SocketIO, emit
import threading
import time

# Load environment variables from .env file
load_dotenv()

from google import genai
from google.genai import types

# Import the AudioLoop class from main.py
from main import AudioLoop, CONFIG, MODEL, client

app = Flask(__name__)
app.config['SECRET_KEY'] = 'your-secret-key-here'
socketio = SocketIO(app, cors_allowed_origins="*")

# Global variables
active_loops = {}

@app.route('/')
def index():
    return render_template('index.html')

@socketio.on('connect')
def handle_connect():
    print('Client connected')
    emit('status', {'message': 'Connected to server'})

@socketio.on('disconnect')
def handle_disconnect():
    sid = request.sid
    print(f'Client disconnected: {sid}')
    loop_entry = active_loops.pop(sid, None)
    if loop_entry:
        loop_entry.stop()

@socketio.on('start_streaming')
def handle_start_streaming(data):
    sid = request.sid

    if sid in active_loops:
        emit('error', {'message': 'Already streaming'})
        return
    
    mode = data.get('mode', 'camera')
    
    try:
        controller = LoopController(mode=mode, sid=sid)
        controller.start()
        active_loops[sid] = controller
        emit('status', {'message': f'Started streaming with {mode} mode'})

    except Exception as e:
        emit('error', {'message': f'Failed to start streaming: {str(e)}'})

@socketio.on('stop_streaming')
def handle_stop_streaming():
    global is_streaming, audio_loop
    
    sid = request.sid
    controller = active_loops.pop(sid, None)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return
    
    try:
        controller.stop()
        emit('status', {'message': 'Stopped streaming'})
        
    except Exception as e:
        emit('error', {'message': f'Failed to stop streaming: {str(e)}'})

@socketio.on('send_message')
def handle_send_message(data):
    sid = request.sid
    controller = active_loops.get(sid)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return
    
    message = data.get('message', '')
    if message:
        # Add message to the audio loop's queue
        try:
            controller.enqueue_text(message)
            emit('status', {'message': f'Sent message: {message}'})
        except Exception as e:
            emit('error', {'message': f'Failed to send message: {str(e)}'})


@socketio.on('video_frame')
def handle_video_frame(data):
    sid = request.sid
    controller = active_loops.get(sid)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return

    frame_data = data.get('data')
    mime_type = data.get('mime_type', 'image/jpeg')
    if not frame_data:
        emit('error', {'message': 'Missing frame data'})
        return

    try:
        controller.enqueue_frame(mime_type, frame_data)
    except Exception as exc:
        emit('error', {'message': f'Failed to queue frame: {exc}'})


@socketio.on('audio_chunk')
def handle_audio_chunk(data):
    sid = request.sid
    controller = active_loops.get(sid)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return

    chunk = data.get('data')
    sample_rate = data.get('sampleRate', SEND_SAMPLE_RATE)
    if not chunk:
        emit('error', {'message': 'Missing audio data'})
        return

    try:
        controller.enqueue_audio(chunk, sample_rate)
    except Exception as exc:
        emit('error', {'message': f'Failed to queue audio: {exc}'})


class LoopController:
    """Thread-safe controller bridging Flask events to AudioLoop."""

    def __init__(self, mode: str, sid: str):
        self.mode = mode
        self.sid = sid
        self.thread = None
        self.loop = None
        self.audio_loop: WebAudioLoop | None = None

    def start(self):
        def runner():
            asyncio.run(self._run())

        self.thread = threading.Thread(target=runner, daemon=True)
        self.thread.start()

    def stop(self):
        if self.audio_loop:
            self.audio_loop.stop()
        if self.loop:
            asyncio.run_coroutine_threadsafe(asyncio.sleep(0), self.loop)
        if self.thread:
            self.thread.join(timeout=1)

    def enqueue_text(self, message: str):
        if not self.audio_loop or not self.audio_loop.text_queue:
            raise RuntimeError("Text queue unavailable")
        asyncio.run_coroutine_threadsafe(
            self.audio_loop.queue_text(message),
            self.loop,
        )

    def enqueue_frame(self, mime_type: str, base64_data: str):
        payload = {"mime_type": mime_type, "data": base64_data}
        asyncio.run_coroutine_threadsafe(
            self.audio_loop.queue_realtime_input(payload),
            self.loop,
        )

    def enqueue_audio(self, base64_data: str, sample_rate: int):
        audio_bytes = base64.b64decode(base64_data)
        payload = {"data": audio_bytes, "mime_type": "audio/pcm", "sample_rate": sample_rate}
        asyncio.run_coroutine_threadsafe(
            self.audio_loop.queue_realtime_input(payload),
            self.loop,
        )

    async def _run(self):
        self.loop = asyncio.get_event_loop()
        self.audio_loop = WebAudioLoop(mode=self.mode, socket_sid=self.sid)
        await self.audio_loop.run()
    def __init__(self, mode="camera", *, socket_sid: str):
        super().__init__(
            video_mode=mode,
            external_media=True,
            enable_local_playback=False,
            enable_cli_text=False,
        )
        self.socket_sid = socket_sid
        self.loop = None
        self.text_queue = asyncio.Queue()
    
    async def send_text_message(self, message):
        """Send a text message to the AI"""
        if self.session:
            await self.session.send(input=message, end_of_turn=True)
    
    async def send_text(self):
        """Override to handle web-based text input"""
        while True:
            try:
                # Wait for messages from the web interface
                message = await self.text_queue.get()
                if message.lower() == "q":
                    break
                await self.session.send(input=message, end_of_turn=True)
            except Exception as e:
                print(f"Error in send_text: {e}")
                break
    
    async def receive_audio(self):
        """Override to emit audio data to web clients"""
        while True:
            try:
                turn = self.session.receive()
                async for response in turn:
                    if data := response.data:
                        self.audio_in_queue.put_nowait(data)
                        # Emit audio data to web clients
                        socketio.emit('audio_data', {
                            'data': base64.b64encode(data).decode(),
                            'sampleRate': 24000
                        })
                        continue
                    if text := response.text:
                        print(text, end="")
                        # Emit text to web clients
                        socketio.emit('ai_response', {'text': text})

                # If you interrupt the model, it sends a turn_complete.
                while not self.audio_in_queue.empty():
                    self.audio_in_queue.get_nowait()
                    
            except Exception as e:
                print(f"Error in receive_audio: {e}")
                break
    
    async def run(self):
        """Override run method to store the event loop"""
        self.loop = asyncio.get_event_loop()
        
        try:
            async with (
                client.aio.live.connect(model=MODEL, config=CONFIG) as session,
                asyncio.TaskGroup() as tg,
            ):
                self.session = session

                self.audio_in_queue = asyncio.Queue()
                self.out_queue = asyncio.Queue(maxsize=5)

                tg.create_task(self.send_text())
                tg.create_task(self.send_realtime())
                tg.create_task(self.listen_audio())
                
                if self.video_mode == "camera":
                    tg.create_task(self.get_frames())
                elif self.video_mode == "screen":
                    tg.create_task(self.get_screen())

                tg.create_task(self.receive_audio())
                tg.create_task(self.play_audio())

                # Wait until streaming is stopped
                while is_streaming:
                    await asyncio.sleep(0.1)
                
                raise asyncio.CancelledError("Streaming stopped")

        except asyncio.CancelledError:
            pass
        except Exception as e:
            print(f"Error in WebAudioLoop.run: {e}")
            traceback.print_exc()
        finally:
            # Cleanup
            if hasattr(self, 'input_stream') and self.input_stream:
                self.input_stream.stop()
                self.input_stream.close()
            if hasattr(self, 'output_stream') and self.output_stream:
                self.output_stream.stop()
                self.output_stream.close()


if __name__ == '__main__':
    print("Starting Gemini AI Web Interface...")
    print("Open your browser and go to http://localhost:8080")
    socketio.run(app, debug=True, host='0.0.0.0', port=8080, allow_unsafe_werkzeug=True)

```

```python
"""
Web frontend for the Gemini AI Audio/Video streaming application
"""

import os
import asyncio
import base64
import io
import json
import traceback
from dotenv import load_dotenv
from flask import Flask, render_template, request
from flask_socketio import SocketIO, emit
import threading
import time

# Load environment variables from .env file
load_dotenv()

from google import genai
from google.genai import types

# Import the AudioLoop class from main.py
from main import AudioLoop, CONFIG, MODEL, client

app = Flask(__name__)
app.config['SECRET_KEY'] = 'your-secret-key-here'
socketio = SocketIO(app, cors_allowed_origins="*")

# Global variables
active_loops = {}

@app.route('/')
def index():
    return render_template('index.html')

@socketio.on('connect')
def handle_connect():
    print('Client connected')
    emit('status', {'message': 'Connected to server'})

@socketio.on('disconnect')
def handle_disconnect():
    sid = request.sid
    print(f'Client disconnected: {sid}')
    loop_entry = active_loops.pop(sid, None)
    if loop_entry:
        loop_entry.stop()

@socketio.on('start_streaming')
def handle_start_streaming(data):
    sid = request.sid

    if sid in active_loops:
        emit('error', {'message': 'Already streaming'})
        return
    
    mode = data.get('mode', 'camera')
    
    try:
        controller = LoopController(mode=mode, sid=sid)
        controller.start()
        active_loops[sid] = controller
        emit('status', {'message': f'Started streaming with {mode} mode'})

    except Exception as e:
        emit('error', {'message': f'Failed to start streaming: {str(e)}'})

@socketio.on('stop_streaming')
def handle_stop_streaming():
    global is_streaming, audio_loop
    
    sid = request.sid
    controller = active_loops.pop(sid, None)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return
    
    try:
        controller.stop()
        emit('status', {'message': 'Stopped streaming'})
        
    except Exception as e:
        emit('error', {'message': f'Failed to stop streaming: {str(e)}'})

@socketio.on('send_message')
def handle_send_message(data):
    sid = request.sid
    controller = active_loops.get(sid)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return
    
    message = data.get('message', '')
    if message:
        # Add message to the audio loop's queue
        try:
            controller.enqueue_text(message)
            emit('status', {'message': f'Sent message: {message}'})
        except Exception as e:
            emit('error', {'message': f'Failed to send message: {str(e)}'})


@socketio.on('video_frame')
def handle_video_frame(data):
    sid = request.sid
    controller = active_loops.get(sid)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return

    frame_data = data.get('data')
    mime_type = data.get('mime_type', 'image/jpeg')
    if not frame_data:
        emit('error', {'message': 'Missing frame data'})
        return

    try:
        controller.enqueue_frame(mime_type, frame_data)
    except Exception as exc:
        emit('error', {'message': f'Failed to queue frame: {exc}'})


@socketio.on('audio_chunk')
def handle_audio_chunk(data):
    sid = request.sid
    controller = active_loops.get(sid)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return

    chunk = data.get('data')
    sample_rate = data.get('sampleRate', SEND_SAMPLE_RATE)
    if not chunk:
        emit('error', {'message': 'Missing audio data'})
        return

    try:
        controller.enqueue_audio(chunk, sample_rate)
    except Exception as exc:
        emit('error', {'message': f'Failed to queue audio: {exc}'})


class LoopController:
    """Thread-safe controller bridging Flask events to AudioLoop."""

    def __init__(self, mode: str, sid: str):
        self.mode = mode
        self.sid = sid
        self.thread = None
        self.loop = None
        self.audio_loop: WebAudioLoop | None = None

    def start(self):
        def runner():
            asyncio.run(self._run())

        self.thread = threading.Thread(target=runner, daemon=True)
        self.thread.start()

    def stop(self):
        if self.audio_loop:
            self.audio_loop.stop()
        if self.loop:
            asyncio.run_coroutine_threadsafe(asyncio.sleep(0), self.loop)
        if self.thread:
            self.thread.join(timeout=1)

    def enqueue_text(self, message: str):
        if not self.audio_loop or not self.audio_loop.text_queue:
            raise RuntimeError("Text queue unavailable")
        asyncio.run_coroutine_threadsafe(
            self.audio_loop.queue_text(message),
            self.loop,
        )

    def enqueue_frame(self, mime_type: str, base64_data: str):
        payload = {"mime_type": mime_type, "data": base64_data}
        asyncio.run_coroutine_threadsafe(
            self.audio_loop.queue_realtime_input(payload),
            self.loop,
        )

    def enqueue_audio(self, base64_data: str, sample_rate: int):
        audio_bytes = base64.b64decode(base64_data)
        payload = {"data": audio_bytes, "mime_type": "audio/pcm", "sample_rate": sample_rate}
        asyncio.run_coroutine_threadsafe(
            self.audio_loop.queue_realtime_input(payload),
            self.loop,
        )

    async def _run(self):
        self.loop = asyncio.get_event_loop()
        self.audio_loop = WebAudioLoop(mode=self.mode, socket_sid=self.sid)
        await self.audio_loop.run()
    def __init__(self, mode="camera", *, socket_sid: str):
        super().__init__(
            video_mode=mode,
            external_media=True,
            enable_local_playback=False,
            enable_cli_text=False,
        )
        self.socket_sid = socket_sid
        self.loop = None
        self.text_queue = asyncio.Queue()
    
    async def send_text_message(self, message):
        """Send a text message to the AI"""
        if self.session:
            await self.session.send(input=message, end_of_turn=True)
    
    async def send_text(self):
        """Override to handle web-based text input"""
        while True:
            try:
                # Wait for messages from the web interface
                message = await self.text_queue.get()
                if message.lower() == "q":
                    break
                await self.session.send(input=message, end_of_turn=True)
            except Exception as e:
                print(f"Error in send_text: {e}")
                break
    
    async def receive_audio(self):
        """Override to emit audio data to web clients"""
        while True:
            try:
                turn = self.session.receive()
                async for response in turn:
                    if data := response.data:
                        self.audio_in_queue.put_nowait(data)
                        # Emit audio data to web clients
                        socketio.emit('audio_data', {
                            'data': base64.b64encode(data).decode(),
                            'sampleRate': 24000
                        })
                        continue
                    if text := response.text:
                        print(text, end="")
                        # Emit text to web clients
                        socketio.emit('ai_response', {'text': text})

                # If you interrupt the model, it sends a turn_complete.
                while not self.audio_in_queue.empty():
                    self.audio_in_queue.get_nowait()
                    
            except Exception as e:
                print(f"Error in receive_audio: {e}")
                break
    
    async def run(self):
        """Override run method to store the event loop"""
        self.loop = asyncio.get_event_loop()
        
        try:
            async with (
                client.aio.live.connect(model=MODEL, config=CONFIG) as session,
                asyncio.TaskGroup() as tg,
            ):
                self.session = session

                self.audio_in_queue = asyncio.Queue()
                self.out_queue = asyncio.Queue(maxsize=5)

                tg.create_task(self.send_text())
                tg.create_task(self.send_realtime())
                tg.create_task(self.listen_audio())
                
                if self.video_mode == "camera":
                    tg.create_task(self.get_frames())
                elif self.video_mode == "screen":
                    tg.create_task(self.get_screen())

                tg.create_task(self.receive_audio())
                tg.create_task(self.play_audio())

                # Wait until streaming is stopped
                while is_streaming:
                    await asyncio.sleep(0.1)
                
                raise asyncio.CancelledError("Streaming stopped")

        except asyncio.CancelledError:
            pass
        except Exception as e:
            print(f"Error in WebAudioLoop.run: {e}")
            traceback.print_exc()
        finally:
            # Cleanup
            if hasattr(self, 'input_stream') and self.input_stream:
                self.input_stream.stop()
                self.input_stream.close()
            if hasattr(self, 'output_stream') and self.output_stream:
                self.output_stream.stop()
                self.output_stream.close()


if __name__ == '__main__':
    print("Starting Gemini AI Web Interface...")
    print("Open your browser and go to http://localhost:8080")
    socketio.run(app, debug=True, host='0.0.0.0', port=8080, allow_unsafe_werkzeug=True)

```

```python
"""
Web frontend for the Gemini AI Audio/Video streaming application
"""

import os
import asyncio
import base64
import io
import json
import traceback
from dotenv import load_dotenv
from flask import Flask, render_template, request
from flask_socketio import SocketIO, emit
import threading
import time

# Load environment variables from .env file
load_dotenv()

from google import genai
from google.genai import types

# Import the AudioLoop class from main.py
from main import AudioLoop, CONFIG, MODEL, client

app = Flask(__name__)
app.config['SECRET_KEY'] = 'your-secret-key-here'
socketio = SocketIO(app, cors_allowed_origins="*")

# Global variables
active_loops: dict[str, "LoopController"] = {}

@app.route('/')
def index():
    return render_template('index.html')

@socketio.on('connect')
def handle_connect():
    print('Client connected')
    emit('status', {'message': 'Connected to server'})

@socketio.on('disconnect')
def handle_disconnect():
    sid = request.sid
    print(f'Client disconnected: {sid}')
    loop_entry = active_loops.pop(sid, None)
    if loop_entry:
        loop_entry.stop()

@socketio.on('start_streaming')
def handle_start_streaming(data):
    sid = request.sid

    if sid in active_loops:
        emit('error', {'message': 'Already streaming'})
        return
    
    mode = data.get('mode', 'camera')
    
    controller = LoopController(mode=mode, sid=sid)
    controller.start()
    active_loops[sid] = controller
    emit('status', {'message': f'Started streaming with {mode} mode'})

@socketio.on('stop_streaming')
def handle_stop_streaming():
    global is_streaming, audio_loop
    
    sid = request.sid
    controller = active_loops.pop(sid, None)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return
    
    try:
        controller.stop()
        emit('status', {'message': 'Stopped streaming'})
        
    except Exception as e:
        emit('error', {'message': f'Failed to stop streaming: {str(e)}'})

@socketio.on('send_message')
def handle_send_message(data):
    sid = request.sid
    controller = active_loops.get(sid)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return
    
    message = data.get('message', '')
    if message:
        # Add message to the audio loop's queue
        try:
            controller.enqueue_text(message)
            emit('status', {'message': f'Sent message: {message}'})
        except Exception as e:
            emit('error', {'message': f'Failed to send message: {str(e)}'})


@socketio.on('video_frame')
def handle_video_frame(data):
    sid = request.sid
    controller = active_loops.get(sid)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return

    frame_data = data.get('data')
    mime_type = data.get('mime_type', 'image/jpeg')
    if not frame_data:
        emit('error', {'message': 'Missing frame data'})
        return

    try:
        controller.enqueue_frame(mime_type, frame_data)
    except Exception as exc:
        emit('error', {'message': f'Failed to queue frame: {exc}'})


@socketio.on('audio_chunk')
def handle_audio_chunk(data):
    sid = request.sid
    controller = active_loops.get(sid)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return

    chunk = data.get('data')
    sample_rate = data.get('sampleRate', SEND_SAMPLE_RATE)
    if not chunk:
        emit('error', {'message': 'Missing audio data'})
        return

    try:
        controller.enqueue_audio(chunk, sample_rate)
    except Exception as exc:
        emit('error', {'message': f'Failed to queue audio: {exc}'})


class LoopController:
    """Thread-safe controller bridging Flask events to AudioLoop."""

    def __init__(self, mode: str, sid: str):
        self.mode = mode
        self.sid = sid
        self.thread = None
        self.loop = None
        self.audio_loop: WebAudioLoop | None = None

    def start(self):
        def runner():
            asyncio.run(self._run())

        self.thread = threading.Thread(target=runner, daemon=True)
        self.thread.start()

    def stop(self):
        if self.audio_loop:
            self.audio_loop.stop()
        if self.loop:
            asyncio.run_coroutine_threadsafe(asyncio.sleep(0), self.loop)
        if self.thread:
            self.thread.join(timeout=1)

    def enqueue_text(self, message: str):
        if not self.audio_loop or not self.audio_loop.text_queue:
            raise RuntimeError("Text queue unavailable")
        asyncio.run_coroutine_threadsafe(
            self.audio_loop.queue_text(message),
            self.loop,
        )

    def enqueue_frame(self, mime_type: str, base64_data: str):
        payload = {"mime_type": mime_type, "data": base64_data}
        asyncio.run_coroutine_threadsafe(
            self.audio_loop.queue_realtime_input(payload),
            self.loop,
        )

    def enqueue_audio(self, base64_data: str, sample_rate: int):
        audio_bytes = base64.b64decode(base64_data)
        payload = {"data": audio_bytes, "mime_type": "audio/pcm", "sample_rate": sample_rate}
        asyncio.run_coroutine_threadsafe(
            self.audio_loop.queue_realtime_input(payload),
            self.loop,
        )

    async def _run(self):
        self.loop = asyncio.get_event_loop()
        self.audio_loop = WebAudioLoop(mode=self.mode, socket_sid=self.sid)
        await self.audio_loop.run()
    def __init__(self, mode="camera", *, socket_sid: str):
        super().__init__(
            video_mode=mode,
            external_media=True,
            enable_local_playback=False,
            enable_cli_text=False,
        )
        self.socket_sid = socket_sid
        self.loop = None
        self.text_queue = asyncio.Queue()
    
    async def send_text_message(self, message):
        """Send a text message to the AI"""
        if self.session:
            await self.session.send(input=message, end_of_turn=True)
    
    async def send_text(self):
        """Override to handle web-based text input"""
        while True:
            try:
                # Wait for messages from the web interface
                message = await self.text_queue.get()
                if message.lower() == "q":
                    break
                await self.session.send(input=message, end_of_turn=True)
            except Exception as e:
                print(f"Error in send_text: {e}")
                break
    
    async def receive_audio(self):
        """Override to emit audio data to web clients"""
        while True:
            try:
                turn = self.session.receive()
                async for response in turn:
                    if data := response.data:
                        self.audio_in_queue.put_nowait(data)
                        # Emit audio data to web clients
                        socketio.emit('audio_data', {
                            'data': base64.b64encode(data).decode(),
                            'sampleRate': 24000
                        })
                        continue
                    if text := response.text:
                        print(text, end="")
                        # Emit text to web clients
                        socketio.emit('ai_response', {'text': text})

                # If you interrupt the model, it sends a turn_complete.
                while not self.audio_in_queue.empty():
                    self.audio_in_queue.get_nowait()
                    
            except Exception as e:
                print(f"Error in receive_audio: {e}")
                break
    
    async def run(self):
        """Override run method to store the event loop"""
        self.loop = asyncio.get_event_loop()
        
        try:
            async with (
                client.aio.live.connect(model=MODEL, config=CONFIG) as session,
                asyncio.TaskGroup() as tg,
            ):
                self.session = session

                self.audio_in_queue = asyncio.Queue()
                self.out_queue = asyncio.Queue(maxsize=5)

                tg.create_task(self.send_text())
                tg.create_task(self.send_realtime())
                tg.create_task(self.listen_audio())
                
                if self.video_mode == "camera":
                    tg.create_task(self.get_frames())
                elif self.video_mode == "screen":
                    tg.create_task(self.get_screen())

                tg.create_task(self.receive_audio())
                tg.create_task(self.play_audio())

                # Wait until streaming is stopped
                while is_streaming:
                    await asyncio.sleep(0.1)
                
                raise asyncio.CancelledError("Streaming stopped")

        except asyncio.CancelledError:
            pass
        except Exception as e:
            print(f"Error in WebAudioLoop.run: {e}")
            traceback.print_exc()
        finally:
            # Cleanup
            if hasattr(self, 'input_stream') and self.input_stream:
                self.input_stream.stop()
                self.input_stream.close()
            if hasattr(self, 'output_stream') and self.output_stream:
                self.output_stream.stop()
                self.output_stream.close()


if __name__ == '__main__':
    print("Starting Gemini AI Web Interface...")
    print("Open your browser and go to http://localhost:8080")
    socketio.run(app, debug=True, host='0.0.0.0', port=8080, allow_unsafe_werkzeug=True)

```

```python
"""
Web frontend for the Gemini AI Audio/Video streaming application
"""

import os
import asyncio
import base64
import io
import json
import traceback
from dotenv import load_dotenv
from flask import Flask, render_template, request
from flask_socketio import SocketIO, emit
import threading
import time

# Load environment variables from .env file
load_dotenv()

from google import genai
from google.genai import types

# Import the AudioLoop class from main.py
from main import AudioLoop, CONFIG, MODEL, client

app = Flask(__name__)
app.config['SECRET_KEY'] = 'your-secret-key-here'
socketio = SocketIO(app, cors_allowed_origins="*")

# Global variables
active_loops: dict[str, "LoopController"] = {}

@app.route('/')
def index():
    return render_template('index.html')

@socketio.on('connect')
def handle_connect():
    print('Client connected')
    emit('status', {'message': 'Connected to server'})

@socketio.on('disconnect')
def handle_disconnect():
    sid = request.sid
    print(f'Client disconnected: {sid}')
    loop_entry = active_loops.pop(sid, None)
    if loop_entry:
        loop_entry.stop()

@socketio.on('start_streaming')
def handle_start_streaming(data):
    sid = request.sid

    if sid in active_loops:
        emit('error', {'message': 'Already streaming'})
        return
    
    mode = data.get('mode', 'camera')
    
    controller = LoopController(mode=mode, sid=sid)
    controller.start()
    active_loops[sid] = controller
    emit('status', {'message': f'Started streaming with {mode} mode'})

@socketio.on('stop_streaming')
def handle_stop_streaming():
    sid = request.sid
    controller = active_loops.pop(sid, None)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return
    
    try:
        controller.stop()
        emit('status', {'message': 'Stopped streaming'})
        
    except Exception as e:
        emit('error', {'message': f'Failed to stop streaming: {str(e)}'})

@socketio.on('send_message')
def handle_send_message(data):
    sid = request.sid
    controller = active_loops.get(sid)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return
    
    message = data.get('message', '')
    if message:
        # Add message to the audio loop's queue
        try:
            controller.enqueue_text(message)
            emit('status', {'message': f'Sent message: {message}'})
        except Exception as e:
            emit('error', {'message': f'Failed to send message: {str(e)}'})


@socketio.on('video_frame')
def handle_video_frame(data):
    sid = request.sid
    controller = active_loops.get(sid)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return

    frame_data = data.get('data')
    mime_type = data.get('mime_type', 'image/jpeg')
    if not frame_data:
        emit('error', {'message': 'Missing frame data'})
        return

    try:
        controller.enqueue_frame(mime_type, frame_data)
    except Exception as exc:
        emit('error', {'message': f'Failed to queue frame: {exc}'})


@socketio.on('audio_chunk')
def handle_audio_chunk(data):
    sid = request.sid
    controller = active_loops.get(sid)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return

    chunk = data.get('data')
    sample_rate = data.get('sampleRate', SEND_SAMPLE_RATE)
    if not chunk:
        emit('error', {'message': 'Missing audio data'})
        return

    try:
        controller.enqueue_audio(chunk, sample_rate)
    except Exception as exc:
        emit('error', {'message': f'Failed to queue audio: {exc}'})


class LoopController:
    """Thread-safe controller bridging Flask events to AudioLoop."""

    def __init__(self, mode: str, sid: str):
        self.mode = mode
        self.sid = sid
        self.thread = None
        self.loop = None
        self.audio_loop: WebAudioLoop | None = None

    def start(self):
        def runner():
            asyncio.run(self._run())

        self.thread = threading.Thread(target=runner, daemon=True)
        self.thread.start()

    def stop(self):
        if self.audio_loop:
            self.audio_loop.stop()
        if self.loop:
            asyncio.run_coroutine_threadsafe(asyncio.sleep(0), self.loop)
        if self.thread:
            self.thread.join(timeout=1)

    def enqueue_text(self, message: str):
        if not self.audio_loop or not self.audio_loop.text_queue:
            raise RuntimeError("Text queue unavailable")
        asyncio.run_coroutine_threadsafe(
            self.audio_loop.queue_text(message),
            self.loop,
        )

    def enqueue_frame(self, mime_type: str, base64_data: str):
        payload = {"mime_type": mime_type, "data": base64_data}
        asyncio.run_coroutine_threadsafe(
            self.audio_loop.queue_realtime_input(payload),
            self.loop,
        )

    def enqueue_audio(self, base64_data: str, sample_rate: int):
        audio_bytes = base64.b64decode(base64_data)
        payload = {"data": audio_bytes, "mime_type": "audio/pcm", "sample_rate": sample_rate}
        asyncio.run_coroutine_threadsafe(
            self.audio_loop.queue_realtime_input(payload),
            self.loop,
        )

    async def _run(self):
        self.loop = asyncio.get_event_loop()
        self.audio_loop = WebAudioLoop(mode=self.mode, socket_sid=self.sid)
        await self.audio_loop.run()
    def __init__(self, mode="camera", *, socket_sid: str):
        super().__init__(
            video_mode=mode,
            external_media=True,
            enable_local_playback=False,
            enable_cli_text=False,
        )
        self.socket_sid = socket_sid
        self.loop = None
        self.text_queue = asyncio.Queue()
    
    async def send_text_message(self, message):
        """Send a text message to the AI"""
        if self.session:
            await self.session.send(input=message, end_of_turn=True)
    
    async def send_text(self):
        """Override to handle web-based text input"""
        while True:
            try:
                # Wait for messages from the web interface
                message = await self.text_queue.get()
                if message.lower() == "q":
                    break
                await self.session.send(input=message, end_of_turn=True)
            except Exception as e:
                print(f"Error in send_text: {e}")
                break
    
    async def receive_audio(self):
        """Override to emit audio data to web clients"""
        while True:
            try:
                turn = self.session.receive()
                async for response in turn:
                    if data := response.data:
                        self.audio_in_queue.put_nowait(data)
                        # Emit audio data to web clients
                        socketio.emit('audio_data', {
                            'data': base64.b64encode(data).decode(),
                            'sampleRate': 24000
                        })
                        continue
                    if text := response.text:
                        print(text, end="")
                        # Emit text to web clients
                        socketio.emit('ai_response', {'text': text})

                # If you interrupt the model, it sends a turn_complete.
                while not self.audio_in_queue.empty():
                    self.audio_in_queue.get_nowait()
                    
            except Exception as e:
                print(f"Error in receive_audio: {e}")
                break
    
    async def run(self):
        """Override run method to store the event loop"""
        self.loop = asyncio.get_event_loop()
        
        try:
            async with (
                client.aio.live.connect(model=MODEL, config=CONFIG) as session,
                asyncio.TaskGroup() as tg,
            ):
                self.session = session

                self.audio_in_queue = asyncio.Queue()
                self.out_queue = asyncio.Queue(maxsize=5)

                tg.create_task(self.send_text())
                tg.create_task(self.send_realtime())
                tg.create_task(self.listen_audio())
                
                if self.video_mode == "camera":
                    tg.create_task(self.get_frames())
                elif self.video_mode == "screen":
                    tg.create_task(self.get_screen())

                tg.create_task(self.receive_audio())
                tg.create_task(self.play_audio())

                # Wait until streaming is stopped
                while is_streaming:
                    await asyncio.sleep(0.1)
                
                raise asyncio.CancelledError("Streaming stopped")

        except asyncio.CancelledError:
            pass
        except Exception as e:
            print(f"Error in WebAudioLoop.run: {e}")
            traceback.print_exc()
        finally:
            # Cleanup
            if hasattr(self, 'input_stream') and self.input_stream:
                self.input_stream.stop()
                self.input_stream.close()
            if hasattr(self, 'output_stream') and self.output_stream:
                self.output_stream.stop()
                self.output_stream.close()


if __name__ == '__main__':
    print("Starting Gemini AI Web Interface...")
    print("Open your browser and go to http://localhost:8080")
    socketio.run(app, debug=True, host='0.0.0.0', port=8080, allow_unsafe_werkzeug=True)

```

```python
*** Begin Patch
*** Update File: /Users/eirvav/PythonProjects/A-EYE/WebInterface/app.py
@@
-import os
-import asyncio
-import base64
-import io
-import json
-import traceback
+import asyncio
+import base64
 import threading
-import time
@@
-from dotenv import load_dotenv
-from flask import Flask, render_template, request
-from flask_socketio import SocketIO, emit
-from main import AudioLoop, CONFIG, MODEL, client
+from flask import Flask, render_template, request
+from flask_socketio import SocketIO, emit
+from dotenv import load_dotenv
+
+from main import AudioLoop
*** End Patch
```

```python
"""
Web frontend for the Gemini AI Audio/Video streaming application
"""

import asyncio
import base64
import threading

# Load environment variables from .env file
load_dotenv()

from google import genai
from google.genai import types

# Import the AudioLoop class from main.py
from main import AudioLoop, CONFIG, MODEL, client

app = Flask(__name__)
app.config['SECRET_KEY'] = 'your-secret-key-here'
socketio = SocketIO(app, cors_allowed_origins="*")

# Global variables
active_loops: dict[str, "LoopController"] = {}

@app.route('/')
def index():
    return render_template('index.html')

@socketio.on('connect')
def handle_connect():
    print('Client connected')
    emit('status', {'message': 'Connected to server'})

@socketio.on('disconnect')
def handle_disconnect():
    sid = request.sid
    print(f'Client disconnected: {sid}')
    loop_entry = active_loops.pop(sid, None)
    if loop_entry:
        loop_entry.stop()

@socketio.on('start_streaming')
def handle_start_streaming(data):
    sid = request.sid

    if sid in active_loops:
        emit('error', {'message': 'Already streaming'})
        return
    
    mode = data.get('mode', 'camera')
    
    controller = LoopController(mode=mode, sid=sid)
    controller.start()
    active_loops[sid] = controller
    emit('status', {'message': f'Started streaming with {mode} mode'})

@socketio.on('stop_streaming')
def handle_stop_streaming():
    sid = request.sid
    controller = active_loops.pop(sid, None)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return
    
    try:
        controller.stop()
        emit('status', {'message': 'Stopped streaming'})
        
    except Exception as e:
        emit('error', {'message': f'Failed to stop streaming: {str(e)}'})

@socketio.on('send_message')
def handle_send_message(data):
    sid = request.sid
    controller = active_loops.get(sid)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return
    
    message = data.get('message', '')
    if message:
        # Add message to the audio loop's queue
        try:
            controller.enqueue_text(message)
            emit('status', {'message': f'Sent message: {message}'})
        except Exception as e:
            emit('error', {'message': f'Failed to send message: {str(e)}'})


@socketio.on('video_frame')
def handle_video_frame(data):
    sid = request.sid
    controller = active_loops.get(sid)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return

    frame_data = data.get('data')
    mime_type = data.get('mime_type', 'image/jpeg')
    if not frame_data:
        emit('error', {'message': 'Missing frame data'})
        return

    try:
        controller.enqueue_frame(mime_type, frame_data)
    except Exception as exc:
        emit('error', {'message': f'Failed to queue frame: {exc}'})


@socketio.on('audio_chunk')
def handle_audio_chunk(data):
    sid = request.sid
    controller = active_loops.get(sid)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return

    chunk = data.get('data')
    sample_rate = data.get('sampleRate', SEND_SAMPLE_RATE)
    if not chunk:
        emit('error', {'message': 'Missing audio data'})
        return

    try:
        controller.enqueue_audio(chunk, sample_rate)
    except Exception as exc:
        emit('error', {'message': f'Failed to queue audio: {exc}'})


class LoopController:
    """Thread-safe controller bridging Flask events to AudioLoop."""

    def __init__(self, mode: str, sid: str):
        self.mode = mode
        self.sid = sid
        self.thread = None
        self.loop = None
        self.audio_loop: WebAudioLoop | None = None

    def start(self):
        def runner():
            asyncio.run(self._run())

        self.thread = threading.Thread(target=runner, daemon=True)
        self.thread.start()

    def stop(self):
        if self.audio_loop:
            self.audio_loop.stop()
        if self.loop:
            asyncio.run_coroutine_threadsafe(asyncio.sleep(0), self.loop)
        if self.thread:
            self.thread.join(timeout=1)

    def enqueue_text(self, message: str):
        if not self.audio_loop or not self.audio_loop.text_queue:
            raise RuntimeError("Text queue unavailable")
        asyncio.run_coroutine_threadsafe(
            self.audio_loop.queue_text(message),
            self.loop,
        )

    def enqueue_frame(self, mime_type: str, base64_data: str):
        payload = {"mime_type": mime_type, "data": base64_data}
        asyncio.run_coroutine_threadsafe(
            self.audio_loop.queue_realtime_input(payload),
            self.loop,
        )

    def enqueue_audio(self, base64_data: str, sample_rate: int):
        audio_bytes = base64.b64decode(base64_data)
        payload = {"data": audio_bytes, "mime_type": "audio/pcm", "sample_rate": sample_rate}
        asyncio.run_coroutine_threadsafe(
            self.audio_loop.queue_realtime_input(payload),
            self.loop,
        )

    async def _run(self):
        self.loop = asyncio.get_event_loop()
        self.audio_loop = WebAudioLoop(mode=self.mode, socket_sid=self.sid)
        await self.audio_loop.run()
    def __init__(self, mode="camera", *, socket_sid: str):
        super().__init__(
            video_mode=mode,
            external_media=True,
            enable_local_playback=False,
            enable_cli_text=False,
        )
        self.socket_sid = socket_sid
        self.loop = None
        self.text_queue = asyncio.Queue()
    
    async def send_text_message(self, message):
        """Send a text message to the AI"""
        if self.session:
            await self.session.send(input=message, end_of_turn=True)
    
    async def send_text(self):
        """Override to handle web-based text input"""
        while True:
            try:
                # Wait for messages from the web interface
                message = await self.text_queue.get()
                if message.lower() == "q":
                    break
                await self.session.send(input=message, end_of_turn=True)
            except Exception as e:
                print(f"Error in send_text: {e}")
                break
    
    async def receive_audio(self):
        """Override to emit audio data to web clients"""
        while True:
            try:
                turn = self.session.receive()
                async for response in turn:
                    if data := response.data:
                        self.audio_in_queue.put_nowait(data)
                        # Emit audio data to web clients
                        socketio.emit('audio_data', {
                            'data': base64.b64encode(data).decode(),
                            'sampleRate': 24000
                        })
                        continue
                    if text := response.text:
                        print(text, end="")
                        # Emit text to web clients
                        socketio.emit('ai_response', {'text': text})

                # If you interrupt the model, it sends a turn_complete.
                while not self.audio_in_queue.empty():
                    self.audio_in_queue.get_nowait()
                    
            except Exception as e:
                print(f"Error in receive_audio: {e}")
                break
    
    async def run(self):
        """Override run method to store the event loop"""
        self.loop = asyncio.get_event_loop()
        
        try:
            async with (
                client.aio.live.connect(model=MODEL, config=CONFIG) as session,
                asyncio.TaskGroup() as tg,
            ):
                self.session = session

                self.audio_in_queue = asyncio.Queue()
                self.out_queue = asyncio.Queue(maxsize=5)

                tg.create_task(self.send_text())
                tg.create_task(self.send_realtime())
                tg.create_task(self.listen_audio())
                
                if self.video_mode == "camera":
                    tg.create_task(self.get_frames())
                elif self.video_mode == "screen":
                    tg.create_task(self.get_screen())

                tg.create_task(self.receive_audio())
                tg.create_task(self.play_audio())

                # Wait until streaming is stopped
                while is_streaming:
                    await asyncio.sleep(0.1)
                
                raise asyncio.CancelledError("Streaming stopped")

        except asyncio.CancelledError:
            pass
        except Exception as e:
            print(f"Error in WebAudioLoop.run: {e}")
            traceback.print_exc()
        finally:
            # Cleanup
            if hasattr(self, 'input_stream') and self.input_stream:
                self.input_stream.stop()
                self.input_stream.close()
            if hasattr(self, 'output_stream') and self.output_stream:
                self.output_stream.stop()
                self.output_stream.close()


if __name__ == '__main__':
    print("Starting Gemini AI Web Interface...")
    print("Open your browser and go to http://localhost:8080")
    socketio.run(app, debug=True, host='0.0.0.0', port=8080, allow_unsafe_werkzeug=True)

```

```python
*** Begin Patch
*** Update File: /Users/eirvav/PythonProjects/A-EYE/WebInterface/app.py
@@
-import asyncio
-import base64
-import threading
+import asyncio
+import base64
+import threading
 
 from flask import Flask, render_template, request
 from flask_socketio import SocketIO, emit
 from dotenv import load_dotenv
 
-from google import genai
-from google.genai import types
-
-# Import the AudioLoop class from main.py
-from main import AudioLoop, CONFIG, MODEL, client
+from main import AudioLoop
*** End Patch
```

```python
*** Begin Patch
*** Update File: /Users/eirvav/PythonProjects/A-EYE/WebInterface/app.py
@@
-import asyncio
-import base64
-import threading
-
-# Load environment variables from .env file
-load_dotenv()
-
-from google import genai
-from google.genai import types
-
-# Import the AudioLoop class from main.py
-from main import AudioLoop, CONFIG, MODEL, client
+import asyncio
+import base64
+import threading
 
 from flask import Flask, render_template, request
 from flask_socketio import SocketIO, emit
 from dotenv import load_dotenv
 
+# Load environment variables from .env file
+load_dotenv()
+
+# Import the AudioLoop class from main.py
+from main import AudioLoop
*** End Patch
```

```python
*** Begin Patch
*** Update File: /Users/eirvav/PythonProjects/A-EYE/WebInterface/app.py
@@
-import asyncio
-import base64
-import threading
-
-from flask import Flask, render_template, request
-from flask_socketio import SocketIO, emit
-from dotenv import load_dotenv
-
-# Load environment variables from .env file
-load_dotenv()
-
-# Import the AudioLoop class from main.py
-from main import AudioLoop, CONFIG, MODEL, client
+import asyncio
+import base64
+import threading
+
+from flask import Flask, render_template, request
+from flask_socketio import SocketIO, emit
+from dotenv import load_dotenv
+
+# Load environment variables from .env file
+load_dotenv()
+
+from main import AudioLoop
*** End Patch
```

```python
"""
Web frontend for the Gemini AI Audio/Video streaming application
"""

import asyncio
import base64
import threading

from flask import Flask, render_template, request
from flask_socketio import SocketIO, emit
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Import the AudioLoop class from main.py
from main import AudioLoop

app = Flask(__name__)
app.config['SECRET_KEY'] = 'your-secret-key-here'
socketio = SocketIO(app, cors_allowed_origins="*")

# Global variables
active_loops: dict[str, "LoopController"] = {}

@app.route('/')
def index():
    return render_template('index.html')

@socketio.on('connect')
def handle_connect():
    print('Client connected')
    emit('status', {'message': 'Connected to server'})

@socketio.on('disconnect')
def handle_disconnect():
    sid = request.sid
    print(f'Client disconnected: {sid}')
    loop_entry = active_loops.pop(sid, None)
    if loop_entry:
        loop_entry.stop()

@socketio.on('start_streaming')
def handle_start_streaming(data):
    sid = request.sid

    if sid in active_loops:
        emit('error', {'message': 'Already streaming'})
        return
    
    mode = data.get('mode', 'camera')
    
    controller = LoopController(mode=mode, sid=sid)
    controller.start()
    active_loops[sid] = controller
    emit('status', {'message': f'Started streaming with {mode} mode'})

@socketio.on('stop_streaming')
def handle_stop_streaming():
    sid = request.sid
    controller = active_loops.pop(sid, None)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return
    
    try:
        controller.stop()
        emit('status', {'message': 'Stopped streaming'})
        
    except Exception as e:
        emit('error', {'message': f'Failed to stop streaming: {str(e)}'})

@socketio.on('send_message')
def handle_send_message(data):
    sid = request.sid
    controller = active_loops.get(sid)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return
    
    message = data.get('message', '')
    if message:
        # Add message to the audio loop's queue
        try:
            controller.enqueue_text(message)
            emit('status', {'message': f'Sent message: {message}'})
        except Exception as e:
            emit('error', {'message': f'Failed to send message: {str(e)}'})


@socketio.on('video_frame')
def handle_video_frame(data):
    sid = request.sid
    controller = active_loops.get(sid)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return

    frame_data = data.get('data')
    mime_type = data.get('mime_type', 'image/jpeg')
    if not frame_data:
        emit('error', {'message': 'Missing frame data'})
        return

    try:
        controller.enqueue_frame(mime_type, frame_data)
    except Exception as exc:
        emit('error', {'message': f'Failed to queue frame: {exc}'})


@socketio.on('audio_chunk')
def handle_audio_chunk(data):
    sid = request.sid
    controller = active_loops.get(sid)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return

    chunk = data.get('data')
    sample_rate = data.get('sampleRate', SEND_SAMPLE_RATE)
    if not chunk:
        emit('error', {'message': 'Missing audio data'})
        return

    try:
        controller.enqueue_audio(chunk, sample_rate)
    except Exception as exc:
        emit('error', {'message': f'Failed to queue audio: {exc}'})


class LoopController:
    """Thread-safe controller bridging Flask events to AudioLoop."""

    def __init__(self, mode: str, sid: str):
        self.mode = mode
        self.sid = sid
        self.thread = None
        self.loop = None
        self.audio_loop: WebAudioLoop | None = None

    def start(self):
        def runner():
            asyncio.run(self._run())

        self.thread = threading.Thread(target=runner, daemon=True)
        self.thread.start()

    def stop(self):
        if self.audio_loop:
            self.audio_loop.stop()
        if self.loop:
            asyncio.run_coroutine_threadsafe(asyncio.sleep(0), self.loop)
        if self.thread:
            self.thread.join(timeout=1)

    def enqueue_text(self, message: str):
        if not self.audio_loop or not self.audio_loop.text_queue:
            raise RuntimeError("Text queue unavailable")
        asyncio.run_coroutine_threadsafe(
            self.audio_loop.queue_text(message),
            self.loop,
        )

    def enqueue_frame(self, mime_type: str, base64_data: str):
        payload = {"mime_type": mime_type, "data": base64_data}
        asyncio.run_coroutine_threadsafe(
            self.audio_loop.queue_realtime_input(payload),
            self.loop,
        )

    def enqueue_audio(self, base64_data: str, sample_rate: int):
        audio_bytes = base64.b64decode(base64_data)
        payload = {"data": audio_bytes, "mime_type": "audio/pcm", "sample_rate": sample_rate}
        asyncio.run_coroutine_threadsafe(
            self.audio_loop.queue_realtime_input(payload),
            self.loop,
        )

    async def _run(self):
        self.loop = asyncio.get_event_loop()
        self.audio_loop = WebAudioLoop(mode=self.mode, socket_sid=self.sid)
        await self.audio_loop.run()
    def __init__(self, mode="camera", *, socket_sid: str):
        super().__init__(
            video_mode=mode,
            external_media=True,
            enable_local_playback=False,
            enable_cli_text=False,
        )
        self.socket_sid = socket_sid
        self.loop = None
        self.text_queue = asyncio.Queue()
    
    async def send_text_message(self, message):
        """Send a text message to the AI"""
        if self.session:
            await self.session.send(input=message, end_of_turn=True)
    
    async def send_text(self):
        """Override to handle web-based text input"""
        while True:
            try:
                # Wait for messages from the web interface
                message = await self.text_queue.get()
                if message.lower() == "q":
                    break
                await self.session.send(input=message, end_of_turn=True)
            except Exception as e:
                print(f"Error in send_text: {e}")
                break
    
    async def receive_audio(self):
        """Override to emit audio data to web clients"""
        while True:
            try:
                turn = self.session.receive()
                async for response in turn:
                    if data := response.data:
                        self.audio_in_queue.put_nowait(data)
                        # Emit audio data to web clients
                        socketio.emit('audio_data', {
                            'data': base64.b64encode(data).decode(),
                            'sampleRate': 24000
                        })
                        continue
                    if text := response.text:
                        print(text, end="")
                        # Emit text to web clients
                        socketio.emit('ai_response', {'text': text})

                # If you interrupt the model, it sends a turn_complete.
                while not self.audio_in_queue.empty():
                    self.audio_in_queue.get_nowait()
                    
            except Exception as e:
                print(f"Error in receive_audio: {e}")
                break
    
    async def run(self):
        """Override run method to store the event loop"""
        self.loop = asyncio.get_event_loop()
        
        try:
            async with (
                client.aio.live.connect(model=MODEL, config=CONFIG) as session,
                asyncio.TaskGroup() as tg,
            ):
                self.session = session

                self.audio_in_queue = asyncio.Queue()
                self.out_queue = asyncio.Queue(maxsize=5)

                tg.create_task(self.send_text())
                tg.create_task(self.send_realtime())
                tg.create_task(self.listen_audio())
                
                if self.video_mode == "camera":
                    tg.create_task(self.get_frames())
                elif self.video_mode == "screen":
                    tg.create_task(self.get_screen())

                tg.create_task(self.receive_audio())
                tg.create_task(self.play_audio())

                # Wait until streaming is stopped
                while is_streaming:
                    await asyncio.sleep(0.1)
                
                raise asyncio.CancelledError("Streaming stopped")

        except asyncio.CancelledError:
            pass
        except Exception as e:
            print(f"Error in WebAudioLoop.run: {e}")
            traceback.print_exc()
        finally:
            # Cleanup
            if hasattr(self, 'input_stream') and self.input_stream:
                self.input_stream.stop()
                self.input_stream.close()
            if hasattr(self, 'output_stream') and self.output_stream:
                self.output_stream.stop()
                self.output_stream.close()


if __name__ == '__main__':
    print("Starting Gemini AI Web Interface...")
    print("Open your browser and go to http://localhost:8080")
    socketio.run(app, debug=True, host='0.0.0.0', port=8080, allow_unsafe_werkzeug=True)

```

```python
"""
Web frontend for the Gemini AI Audio/Video streaming application
"""

import asyncio
import base64
import threading

from flask import Flask, render_template, request
from flask_socketio import SocketIO, emit
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Import the AudioLoop class from main.py
from main import AudioLoop

app = Flask(__name__)
app.config['SECRET_KEY'] = 'your-secret-key-here'
socketio = SocketIO(app, cors_allowed_origins="*")

# Global variables
active_loops: dict[str, "LoopController"] = {}

@app.route('/')
def index():
    return render_template('index.html')

@socketio.on('connect')
def handle_connect():
    print('Client connected')
    emit('status', {'message': 'Connected to server'})

@socketio.on('disconnect')
def handle_disconnect():
    sid = request.sid
    print(f'Client disconnected: {sid}')
    loop_entry = active_loops.pop(sid, None)
    if loop_entry:
        loop_entry.stop()

@socketio.on('start_streaming')
def handle_start_streaming(data):
    sid = request.sid

    if sid in active_loops:
        emit('error', {'message': 'Already streaming'})
        return
    
    mode = data.get('mode', 'camera')
    
    controller = LoopController(mode=mode, sid=sid)
    controller.start()
    active_loops[sid] = controller
    emit('status', {'message': f'Started streaming with {mode} mode'})

@socketio.on('stop_streaming')
def handle_stop_streaming():
    sid = request.sid
    controller = active_loops.pop(sid, None)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return
    
    try:
        controller.stop()
        emit('status', {'message': 'Stopped streaming'})
        
    except Exception as e:
        emit('error', {'message': f'Failed to stop streaming: {str(e)}'})

@socketio.on('send_message')
def handle_send_message(data):
    sid = request.sid
    controller = active_loops.get(sid)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return
    
    message = data.get('message', '')
    if message:
        # Add message to the audio loop's queue
        try:
            controller.enqueue_text(message)
            emit('status', {'message': f'Sent message: {message}'})
        except Exception as e:
            emit('error', {'message': f'Failed to send message: {str(e)}'})


@socketio.on('video_frame')
def handle_video_frame(data):
    sid = request.sid
    controller = active_loops.get(sid)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return

    frame_data = data.get('data')
    mime_type = data.get('mime_type', 'image/jpeg')
    if not frame_data:
        emit('error', {'message': 'Missing frame data'})
        return

    try:
        controller.enqueue_frame(mime_type, frame_data)
    except Exception as exc:
        emit('error', {'message': f'Failed to queue frame: {exc}'})


@socketio.on('audio_chunk')
def handle_audio_chunk(data):
    sid = request.sid
    controller = active_loops.get(sid)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return

    chunk = data.get('data')
    sample_rate = data.get('sampleRate', SEND_SAMPLE_RATE)
    if not chunk:
        emit('error', {'message': 'Missing audio data'})
        return

    try:
        controller.enqueue_audio(chunk, sample_rate)
    except Exception as exc:
        emit('error', {'message': f'Failed to queue audio: {exc}'})


class LoopController:
    """Thread-safe controller bridging Flask events to AudioLoop."""

    def __init__(self, mode: str, sid: str):
        self.mode = mode
        self.sid = sid
        self.thread = None
        self.loop = None
        self.audio_loop: WebAudioLoop | None = None

    def start(self):
        def runner():
            asyncio.run(self._run())

        self.thread = threading.Thread(target=runner, daemon=True)
        self.thread.start()

    def stop(self):
        if self.audio_loop:
            self.audio_loop.stop()
        if self.loop:
            asyncio.run_coroutine_threadsafe(asyncio.sleep(0), self.loop)
        if self.thread:
            self.thread.join(timeout=1)

    def enqueue_text(self, message: str):
        if not self.audio_loop or not self.audio_loop.text_queue:
            raise RuntimeError("Text queue unavailable")
        asyncio.run_coroutine_threadsafe(
            self.audio_loop.queue_text(message),
            self.loop,
        )

    def enqueue_frame(self, mime_type: str, base64_data: str):
        payload = {"mime_type": mime_type, "data": base64_data}
        asyncio.run_coroutine_threadsafe(
            self.audio_loop.queue_realtime_input(payload),
            self.loop,
        )

    def enqueue_audio(self, base64_data: str, sample_rate: int):
        audio_bytes = base64.b64decode(base64_data)
        payload = {"data": audio_bytes, "mime_type": "audio/pcm", "sample_rate": sample_rate}
        asyncio.run_coroutine_threadsafe(
            self.audio_loop.queue_realtime_input(payload),
            self.loop,
        )

    async def _run(self):
        self.loop = asyncio.get_event_loop()
        self.audio_loop = WebAudioLoop(mode=self.mode, socket_sid=self.sid)
        await self.audio_loop.run()


class WebAudioLoop(AudioLoop):
    """Extended AudioLoop class for Socket.IO bridge"""

    def __init__(self, mode="camera", *, socket_sid: str):
        super().__init__(
            video_mode=mode,
            external_media=True,
            enable_local_playback=False,
            enable_cli_text=False,
        )
        self.socket_sid = socket_sid
        self.loop = None

    async def send_text(self):
        while True:
            message = await self.text_queue.get()
            if message is None:
                break
            await self.session.send(input=message, end_of_turn=True)

    async def handle_incoming_text(self, text: str):
        super().handle_incoming_text(text)
        socketio.emit('ai_response', {'text': text}, room=self.socket_sid)

    async def handle_incoming_audio(self, response):
        await super().handle_incoming_audio(response)
        if response.data:
            socketio.emit(
                'audio_data',
                {
                    'data': base64.b64encode(response.data).decode(),
                    'sampleRate': 24000,
                },
                room=self.socket_sid,
            )

    async def run(self):
        self.loop = asyncio.get_event_loop()
        await super().run()


if __name__ == '__main__':
    print("Starting Gemini AI Web Interface...")
    print("Open your browser and go to http://localhost:8080")
    socketio.run(app, debug=True, host='0.0.0.0', port=8080, allow_unsafe_werkzeug=True)

```

```python
"""
Web frontend for the Gemini AI Audio/Video streaming application
"""

import asyncio
import base64
import threading

from flask import Flask, render_template, request
from flask_socketio import SocketIO, emit
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Import the AudioLoop class from main.py
from main import AudioLoop

app = Flask(__name__)
app.config['SECRET_KEY'] = 'your-secret-key-here'
socketio = SocketIO(app, cors_allowed_origins="*")

# Global variables
active_loops: dict[str, "LoopController"] = {}

@app.route('/')
def index():
    return render_template('index.html')

@socketio.on('connect')
def handle_connect():
    print('Client connected')
    emit('status', {'message': 'Connected to server'})

@socketio.on('disconnect')
def handle_disconnect():
    sid = request.sid
    print(f'Client disconnected: {sid}')
    loop_entry = active_loops.pop(sid, None)
    if loop_entry:
        loop_entry.stop()

@socketio.on('start_streaming')
def handle_start_streaming(data):
    sid = request.sid

    if sid in active_loops:
        emit('error', {'message': 'Already streaming'})
        return
    
    mode = data.get('mode', 'camera')
    
    controller = LoopController(mode=mode, sid=sid)
    controller.start()
    active_loops[sid] = controller
    emit('status', {'message': f'Started streaming with {mode} mode'})

@socketio.on('stop_streaming')
def handle_stop_streaming():
    sid = request.sid
    controller = active_loops.pop(sid, None)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return
    
    try:
        controller.stop()
        emit('status', {'message': 'Stopped streaming'})
        
    except Exception as e:
        emit('error', {'message': f'Failed to stop streaming: {str(e)}'})

@socketio.on('send_message')
def handle_send_message(data):
    sid = request.sid
    controller = active_loops.get(sid)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return
    
    message = data.get('message', '')
    if message:
        # Add message to the audio loop's queue
        try:
            controller.enqueue_text(message)
            emit('status', {'message': f'Sent message: {message}'})
        except Exception as e:
            emit('error', {'message': f'Failed to send message: {str(e)}'})


@socketio.on('video_frame')
def handle_video_frame(data):
    sid = request.sid
    controller = active_loops.get(sid)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return

    frame_data = data.get('data')
    mime_type = data.get('mime_type', 'image/jpeg')
    if not frame_data:
        emit('error', {'message': 'Missing frame data'})
        return

    try:
        controller.enqueue_frame(mime_type, frame_data)
    except Exception as exc:
        emit('error', {'message': f'Failed to queue frame: {exc}'})


@socketio.on('audio_chunk')
def handle_audio_chunk(data):
    sid = request.sid
    controller = active_loops.get(sid)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return

    chunk = data.get('data')
    sample_rate = data.get('sampleRate', SEND_SAMPLE_RATE)
    if not chunk:
        emit('error', {'message': 'Missing audio data'})
        return

    try:
        controller.enqueue_audio(chunk, sample_rate)
    except Exception as exc:
        emit('error', {'message': f'Failed to queue audio: {exc}'})


class LoopController:
    """Thread-safe controller bridging Flask events to AudioLoop."""

    def __init__(self, mode: str, sid: str):
        self.mode = mode
        self.sid = sid
        self.thread = None
        self.loop = None
        self.audio_loop: WebAudioLoop | None = None

    def start(self):
        def runner():
            asyncio.run(self._run())

        self.thread = threading.Thread(target=runner, daemon=True)
        self.thread.start()

    def stop(self):
        if self.audio_loop:
            self.audio_loop.stop()
        if self.loop:
            asyncio.run_coroutine_threadsafe(asyncio.sleep(0), self.loop)
        if self.thread:
            self.thread.join(timeout=1)

    def enqueue_text(self, message: str):
        if not self.audio_loop or not self.audio_loop.text_queue:
            raise RuntimeError("Text queue unavailable")
        asyncio.run_coroutine_threadsafe(
            self.audio_loop.queue_text(message),
            self.loop,
        )

    def enqueue_frame(self, mime_type: str, base64_data: str):
        payload = {"mime_type": mime_type, "data": base64_data}
        asyncio.run_coroutine_threadsafe(
            self.audio_loop.queue_realtime_input(payload),
            self.loop,
        )

    def enqueue_audio(self, base64_data: str, sample_rate: int):
        audio_bytes = base64.b64decode(base64_data)
        payload = {"data": audio_bytes, "mime_type": "audio/pcm", "sample_rate": sample_rate}
        asyncio.run_coroutine_threadsafe(
            self.audio_loop.queue_realtime_input(payload),
            self.loop,
        )

    async def _run(self):
        self.loop = asyncio.get_event_loop()
        self.audio_loop = WebAudioLoop(mode=self.mode, socket_sid=self.sid)
        await self.audio_loop.run()


if __name__ == '__main__':
    print("Starting Gemini AI Web Interface...")
    print("Open your browser and go to http://localhost:8080")
    socketio.run(app, debug=True, host='0.0.0.0', port=8080, allow_unsafe_werkzeug=True)

```

```python
"""
## Documentation
Quickstart: https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI.py

## Setup

To install the dependencies for this script, run:

```
pip install google-genai opencv-python sounddevice pillow mss python-dotenv numpy

Important: **Use headphones**. This script uses the system default audio
input and output, which often won't include echo cancellation. So to prevent
the model from interrupting itself it is important that you use headphones. 

```
To run the script:
```
python main.py
```

The script takes a video-mode flag `--mode`, this can be "camera", "screen", or "none".
The default is "camera". To share your screen run:

```
python Get_started_LiveAPI.py --mode screen
```
"""

import os
import asyncio
import base64
import io
import traceback
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

import cv2
import sounddevice as sd
import numpy as np
import PIL.Image
import mss

import argparse

from google import genai
from google.genai import types

CHANNELS = 1
SEND_SAMPLE_RATE = 16000
RECEIVE_SAMPLE_RATE = 24000
CHUNK_SIZE = 1024
DTYPE = np.int16

MODEL = "models/gemini-2.5-flash-preview-native-audio-dialog"

DEFAULT_MODE = "camera"

client = genai.Client(
    http_options={"api_version": "v1beta"},
    api_key=os.environ.get("GEMINI_API_KEY"),
)


CONFIG = types.LiveConnectConfig(
    response_modalities=[
        "AUDIO",
    ],
    media_resolution="MEDIA_RESOLUTION_MEDIUM",
    speech_config=types.SpeechConfig(
        voice_config=types.VoiceConfig(
            prebuilt_voice_config=types.PrebuiltVoiceConfig(voice_name="Zephyr")
        )
    ),
    context_window_compression=types.ContextWindowCompressionConfig(
        trigger_tokens=25600,
        sliding_window=types.SlidingWindow(target_tokens=12800),
    ),
)

# Audio device will be selected in the AudioLoop class


class AudioLoop:
    def __init__(
        self,
        video_mode=DEFAULT_MODE,
        *,
        external_media: bool = False,
        enable_local_playback: bool = True,
        enable_cli_text: bool = True,
    ):
        self.video_mode = video_mode
        self.external_media = external_media
        self.enable_local_playback = enable_local_playback
        self.enable_cli_text = enable_cli_text

        self.session: genai.live.LiveConnectSession | None = None

        self.audio_in_queue: asyncio.Queue | None = None
        self.out_queue: asyncio.Queue | None = None
        self.text_queue: asyncio.Queue | None = None

        self.input_stream = None
        self.output_stream = None

        self._shutdown_future: asyncio.Future | None = None

    async def send_text(self):
        if self.enable_cli_text:
            while True:
                text = await asyncio.to_thread(
                    input,
                    "message > ",
                )
                if text.lower() == "q":
                    await self.shutdown()
                    break
                await self.session.send(input=text or ".", end_of_turn=True)
        else:
            while True:
                message = await self.text_queue.get()
                if message is None:
                    break
                await self.session.send(input=message, end_of_turn=True)

    def _get_frame(self, cap):
        # Read the frameq
        ret, frame = cap.read()
        # Check if the frame was read successfully
        if not ret:
            return None
        # Fix: Convert BGR to RGB color space
        # OpenCV captures in BGR but PIL expects RGB format
        # This prevents the blue tint in the video feed
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        img = PIL.Image.fromarray(frame_rgb)  # Now using RGB frame
        img.thumbnail([1024, 1024])

        image_io = io.BytesIO()
        img.save(image_io, format="jpeg")
        image_io.seek(0)

        mime_type = "image/jpeg"
        image_bytes = image_io.read()
        return {"mime_type": mime_type, "data": base64.b64encode(image_bytes).decode()}

    async def get_frames(self):
        # This takes about a second, and will block the whole program
        # causing the audio pipeline to overflow if you don't to_thread it.
        cap = await asyncio.to_thread(
            cv2.VideoCapture, 0
        )  # 0 represents the default camera

        while True:
            frame = await asyncio.to_thread(self._get_frame, cap)
            if frame is None:
                break

            await asyncio.sleep(1.0)

            await self.out_queue.put(frame)

        # Release the VideoCapture object
        cap.release()

    def _get_screen(self):
        sct = mss.mss()
        monitor = sct.monitors[0]

        i = sct.grab(monitor)

        mime_type = "image/jpeg"
        image_bytes = mss.tools.to_png(i.rgb, i.size)
        img = PIL.Image.open(io.BytesIO(image_bytes))

        image_io = io.BytesIO()
        img.save(image_io, format="jpeg")
        image_io.seek(0)

        image_bytes = image_io.read()
        return {"mime_type": mime_type, "data": base64.b64encode(image_bytes).decode()}

    async def get_screen(self):

        while True:
            frame = await asyncio.to_thread(self._get_screen)
            if frame is None:
                break

            await asyncio.sleep(1.0)

            await self.out_queue.put(frame)

    async def send_realtime(self):
        while True:
            msg = await self.out_queue.get()
            await self.session.send(input=msg)

    async def queue_realtime_input(self, payload):
        if not self.out_queue:
            raise RuntimeError("Realtime queue not ready")
        if self.out_queue.full():
            try:
                self.out_queue.get_nowait()
            except asyncio.QueueEmpty:
                pass
        await self.out_queue.put(payload)

    async def queue_text(self, message: str, *, end_of_turn: bool = True):
        if self.enable_cli_text:
            raise RuntimeError("Text queue not available in CLI mode")
        if not self.text_queue:
            raise RuntimeError("Text queue not ready")
        if end_of_turn:
            await self.text_queue.put(message)
        else:
            await self.session.send(input=message, end_of_turn=False)

    async def listen_audio(self):
        # Create a queue for audio data
        audio_queue = asyncio.Queue()
        
        def audio_callback(indata, frames, time, status):
            if status:
                print(f"Audio input status: {status}")
            # Convert float32 to int16 and get raw bytes
            audio_data = (indata[:, 0] * 32767).astype(DTYPE).tobytes()
            try:
                audio_queue.put_nowait({"data": audio_data, "mime_type": "audio/pcm"})
            except asyncio.QueueFull:
                pass  # Drop frames if queue is full
        
        # Start the input stream
        self.input_stream = sd.InputStream(
            samplerate=SEND_SAMPLE_RATE,
            channels=CHANNELS,
            dtype='float32',
            blocksize=CHUNK_SIZE,
            callback=audio_callback
        )
        
        self.input_stream.start()
        
        try:
            while True:
                audio_data = await audio_queue.get()
                await self.queue_realtime_input(audio_data)
        finally:
            if self.input_stream:
                self.input_stream.stop()
                self.input_stream.close()
                self.input_stream = None

    async def receive_audio(self):
        while True:
            turn = self.session.receive()
            async for response in turn:
                if data := response.data:
                    await self.handle_incoming_audio(response)
                    continue
                if text := response.text:
                    await self.handle_incoming_text(text)
                await self.handle_event(response)
            await self.on_turn_complete()

    async def handle_incoming_audio(self, response):
        if not self.enable_local_playback or not self.audio_in_queue:
            return
        try:
            self.audio_in_queue.put_nowait(response.data)
        except asyncio.QueueFull:
            while not self.audio_in_queue.empty():
                self.audio_in_queue.get_nowait()
            try:
                self.audio_in_queue.put_nowait(response.data)
            except asyncio.QueueFull:
                pass

    async def handle_incoming_text(self, text: str):
        print(text, end="")

    async def handle_event(self, response):
        if hasattr(response, "model_turn") and response.model_turn and response.model_turn.turn_complete:
            await self.on_turn_complete()

    async def on_turn_complete(self):
        if not self.enable_local_playback or not self.audio_in_queue:
            return
        while not self.audio_in_queue.empty():
            self.audio_in_queue.get_nowait()

    async def play_audio(self):
        # Create a queue for output audio data
        output_queue = asyncio.Queue()
        
        def audio_callback(outdata, frames, time, status):
            if status:
                print(f"Audio output status: {status}")
            try:
                # Get audio data from queue (non-blocking)
                audio_data = output_queue.get_nowait()
                # Convert bytes to numpy array and reshape
                audio_array = np.frombuffer(audio_data, dtype=DTYPE).astype(np.float32) / 32767.0
                # Ensure we have the right number of frames
                if len(audio_array) >= frames:
                    outdata[:, 0] = audio_array[:frames]
                else:
                    # Pad with zeros if not enough data
                    outdata[:len(audio_array), 0] = audio_array
                    outdata[len(audio_array):, 0] = 0
            except asyncio.QueueEmpty:
                # No audio data available, output silence
                outdata[:, 0] = 0
        
        # Start the output stream
        self.output_stream = sd.OutputStream(
            samplerate=RECEIVE_SAMPLE_RATE,
            channels=CHANNELS,
            dtype='float32',
            blocksize=CHUNK_SIZE,
            callback=audio_callback
        )
        
        self.output_stream.start()
        
        try:
            while True:
                bytestream = await self.audio_in_queue.get()
                await output_queue.put(bytestream)
        finally:
            if self.output_stream:
                self.output_stream.stop()
                self.output_stream.close()
            if self.output_stream:
                self.output_stream = None

    async def shutdown(self):
        if self._shutdown_future and not self._shutdown_future.done():
            self._shutdown_future.set_result(None)
        if not self.enable_cli_text and self.text_queue:
            await self.text_queue.put(None)
        await self._close_streams()

    async def _close_streams(self):
        if self.input_stream:
            self.input_stream.stop()
            self.input_stream.close()
            self.input_stream = None
        if self.output_stream:
            self.output_stream.stop()
            self.output_stream.close()
            self.output_stream = None

    async def run(self):
        try:
            async with (
                client.aio.live.connect(model=MODEL, config=CONFIG) as session,
                asyncio.TaskGroup() as tg,
            ):
                self.session = session
                loop = asyncio.get_running_loop()
                self._loop = loop
                self.audio_in_queue = asyncio.Queue(maxsize=50)
                self.out_queue = asyncio.Queue(maxsize=10)
                if not self.enable_cli_text:
                    self.text_queue = asyncio.Queue()
                else:
                    self.text_queue = None
                self._shutdown_future = loop.create_future()

                if self.enable_cli_text or self.text_queue is not None:
                    tg.create_task(self.send_text())

                tg.create_task(self.send_realtime())

                if not self.external_media:
                    tg.create_task(self.listen_audio())
                    if self.video_mode == "camera":
                        tg.create_task(self.get_frames())
                    elif self.video_mode == "screen":
                        tg.create_task(self.get_screen())

                tg.create_task(self.receive_audio())

                if self.enable_local_playback:
                    tg.create_task(self.play_audio())

                await self._shutdown_future
                raise asyncio.CancelledError("Shutdown requested")

        except asyncio.CancelledError:
            pass
        except ExceptionGroup as EG:
            traceback.print_exception(EG)
        finally:
            await self._close_streams()


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--mode",
        type=str,
        default=DEFAULT_MODE,
        help="pixels to stream from",
        choices=["camera", "screen", "none"],
    )
    args = parser.parse_args()
    main = AudioLoop(video_mode=args.mode)
    asyncio.run(main.run())

```

```python
"""
Web frontend for the Gemini AI Audio/Video streaming application
"""

import asyncio
import base64
import threading

from flask import Flask, render_template, request
from flask_socketio import SocketIO, emit
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Import the AudioLoop class from main.py
from main import AudioLoop

app = Flask(__name__)
app.config['SECRET_KEY'] = 'your-secret-key-here'
socketio = SocketIO(app, cors_allowed_origins="*")

# Global variables
active_loops: dict[str, "LoopController"] = {}

@app.route('/')
def index():
    return render_template('index.html')

@socketio.on('connect')
def handle_connect():
    print('Client connected')
    emit('status', {'message': 'Connected to server'})

@socketio.on('disconnect')
def handle_disconnect():
    sid = request.sid
    print(f'Client disconnected: {sid}')
    loop_entry = active_loops.pop(sid, None)
    if loop_entry:
        loop_entry.stop()

@socketio.on('start_streaming')
def handle_start_streaming(data):
    sid = request.sid

    if sid in active_loops:
        emit('error', {'message': 'Already streaming'})
        return
    
    mode = data.get('mode', 'camera')
    
    controller = LoopController(mode=mode, sid=sid)
    controller.start()
    active_loops[sid] = controller
    emit('status', {'message': f'Started streaming with {mode} mode'})

@socketio.on('stop_streaming')
def handle_stop_streaming():
    sid = request.sid
    controller = active_loops.pop(sid, None)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return
    
    try:
        controller.stop()
        emit('status', {'message': 'Stopped streaming'})
        
    except Exception as e:
        emit('error', {'message': f'Failed to stop streaming: {str(e)}'})

@socketio.on('send_message')
def handle_send_message(data):
    sid = request.sid
    controller = active_loops.get(sid)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return
    
    message = data.get('message', '')
    if message:
        # Add message to the audio loop's queue
        try:
            controller.enqueue_text(message)
            emit('status', {'message': f'Sent message: {message}'})
        except Exception as e:
            emit('error', {'message': f'Failed to send message: {str(e)}'})


@socketio.on('video_frame')
def handle_video_frame(data):
    sid = request.sid
    controller = active_loops.get(sid)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return

    frame_data = data.get('data')
    mime_type = data.get('mime_type', 'image/jpeg')
    if not frame_data:
        emit('error', {'message': 'Missing frame data'})
        return

    try:
        controller.enqueue_frame(mime_type, frame_data)
    except Exception as exc:
        emit('error', {'message': f'Failed to queue frame: {exc}'})


@socketio.on('audio_chunk')
def handle_audio_chunk(data):
    sid = request.sid
    controller = active_loops.get(sid)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return

    chunk = data.get('data')
    sample_rate = data.get('sampleRate', SEND_SAMPLE_RATE)
    if not chunk:
        emit('error', {'message': 'Missing audio data'})
        return

    try:
        controller.enqueue_audio(chunk, sample_rate)
    except Exception as exc:
        emit('error', {'message': f'Failed to queue audio: {exc}'})


class LoopController:
    """Thread-safe controller bridging Flask events to AudioLoop."""

    def __init__(self, mode: str, sid: str):
        self.mode = mode
        self.sid = sid
        self.thread = None
        self.loop = None
        self.audio_loop: WebAudioLoop | None = None
        self._ready = threading.Event()
        self._error: Exception | None = None

    def start(self):
        def runner():
            try:
                asyncio.run(self._run())
            except Exception as exc:  # pragma: no cover - surfaced via _error
                self._error = exc
                self._ready.set()

        self.thread = threading.Thread(target=runner, daemon=True)
        self.thread.start()
        if not self._ready.wait(timeout=5):
            raise RuntimeError("Timed out waiting for streaming loop to start")
        if self._error:
            raise self._error

    def stop(self):
        if self.audio_loop and self.loop:
            asyncio.run_coroutine_threadsafe(self.audio_loop.shutdown(), self.loop)
        if self.thread:
            self.thread.join(timeout=2)

    def enqueue_text(self, message: str):
        self._ensure_ready()
        if not self.audio_loop or not self.audio_loop.text_queue:
            raise RuntimeError("Text queue unavailable")
        asyncio.run_coroutine_threadsafe(
            self.audio_loop.queue_text(message),
            self.loop,
        )

    def enqueue_frame(self, mime_type: str, base64_data: str):
        self._ensure_ready()
        payload = {"mime_type": mime_type, "data": base64_data}
        asyncio.run_coroutine_threadsafe(
            self.audio_loop.queue_realtime_input(payload),
            self.loop,
        )

    def enqueue_audio(self, base64_data: str, sample_rate: int):  # noqa: ARG002 sample_rate kept for logging if needed
        self._ensure_ready()
        audio_bytes = base64.b64decode(base64_data)
        payload = {"data": audio_bytes, "mime_type": "audio/pcm"}
        asyncio.run_coroutine_threadsafe(
            self.audio_loop.queue_realtime_input(payload),
            self.loop,
        )

    def _ensure_ready(self):
        if not self._ready.is_set():
            raise RuntimeError("Loop is not ready yet")

    async def _run(self):
        loop = asyncio.get_running_loop()
        self.loop = loop
        self.audio_loop = WebAudioLoop(mode=self.mode, socket_sid=self.sid)
        self._ready.set()
        await self.audio_loop.run()


if __name__ == '__main__':
    print("Starting Gemini AI Web Interface...")
    print("Open your browser and go to http://localhost:8080")
    socketio.run(app, debug=True, host='0.0.0.0', port=8080, allow_unsafe_werkzeug=True)

```

```python
"""
Web frontend for the Gemini AI Audio/Video streaming application
"""

import asyncio
import base64
import threading

from flask import Flask, render_template, request
from flask_socketio import SocketIO, emit
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Import the AudioLoop class from main.py
from main import AudioLoop

app = Flask(__name__)
app.config['SECRET_KEY'] = 'your-secret-key-here'
socketio = SocketIO(app, cors_allowed_origins="*")

# Global variables
active_loops: dict[str, "LoopController"] = {}

@app.route('/')
def index():
    return render_template('index.html')

@socketio.on('connect')
def handle_connect():
    print('Client connected')
    emit('status', {'message': 'Connected to server'})

@socketio.on('disconnect')
def handle_disconnect():
    sid = request.sid
    print(f'Client disconnected: {sid}')
    loop_entry = active_loops.pop(sid, None)
    if loop_entry:
        loop_entry.stop()

@socketio.on('start_streaming')
def handle_start_streaming(data):
    sid = request.sid

    if sid in active_loops:
        emit('error', {'message': 'Already streaming'})
        return
    
    mode = data.get('mode', 'camera')
    
    controller = LoopController(mode=mode, sid=sid)
    controller.start()
    active_loops[sid] = controller
    emit('status', {'message': f'Started streaming with {mode} mode'})

@socketio.on('stop_streaming')
def handle_stop_streaming():
    sid = request.sid
    controller = active_loops.pop(sid, None)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return
    
    try:
        controller.stop()
        emit('status', {'message': 'Stopped streaming'})
        
    except Exception as e:
        emit('error', {'message': f'Failed to stop streaming: {str(e)}'})

@socketio.on('send_message')
def handle_send_message(data):
    sid = request.sid
    controller = active_loops.get(sid)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return
    
    message = data.get('message', '')
    if message:
        # Add message to the audio loop's queue
        try:
            controller.enqueue_text(message)
            emit('status', {'message': f'Sent message: {message}'})
        except Exception as e:
            emit('error', {'message': f'Failed to send message: {str(e)}'})


@socketio.on('video_frame')
def handle_video_frame(data):
    sid = request.sid
    controller = active_loops.get(sid)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return

    frame_data = data.get('data')
    mime_type = data.get('mime_type', 'image/jpeg')
    if not frame_data:
        emit('error', {'message': 'Missing frame data'})
        return

    try:
        controller.enqueue_frame(mime_type, frame_data)
    except Exception as exc:
        emit('error', {'message': f'Failed to queue frame: {exc}'})


@socketio.on('audio_chunk')
def handle_audio_chunk(data):
    sid = request.sid
    controller = active_loops.get(sid)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return

    chunk = data.get('data')
    sample_rate = data.get('sampleRate', SEND_SAMPLE_RATE)
    if not chunk:
        emit('error', {'message': 'Missing audio data'})
        return

    try:
        controller.enqueue_audio(chunk, sample_rate)
    except Exception as exc:
        emit('error', {'message': f'Failed to queue audio: {exc}'})


class LoopController:
    """Thread-safe controller bridging Flask events to AudioLoop."""

    def __init__(self, mode: str, sid: str):
        self.mode = mode
        self.sid = sid
        self.thread = None
        self.loop = None
        self.audio_loop: WebAudioLoop | None = None
        self._ready = threading.Event()
        self._error: Exception | None = None

    def start(self):
        def runner():
            try:
                asyncio.run(self._run())
            except Exception as exc:  # pragma: no cover - surfaced via _error
                self._error = exc
                self._ready.set()

        self.thread = threading.Thread(target=runner, daemon=True)
        self.thread.start()
        if not self._ready.wait(timeout=5):
            raise RuntimeError("Timed out waiting for streaming loop to start")
        if self._error:
            raise self._error

    def stop(self):
        if self.audio_loop and self.loop:
            asyncio.run_coroutine_threadsafe(self.audio_loop.shutdown(), self.loop)
        if self.thread:
            self.thread.join(timeout=2)

    def enqueue_text(self, message: str):
        self._ensure_ready()
        if not self.audio_loop or not self.audio_loop.text_queue:
            raise RuntimeError("Text queue unavailable")
        asyncio.run_coroutine_threadsafe(
            self.audio_loop.queue_text(message),
            self.loop,
        )

    def enqueue_frame(self, mime_type: str, base64_data: str):
        self._ensure_ready()
        payload = {"mime_type": mime_type, "data": base64_data}
        asyncio.run_coroutine_threadsafe(
            self.audio_loop.queue_realtime_input(payload),
            self.loop,
        )

    def enqueue_audio(self, base64_data: str, sample_rate: int):  # noqa: ARG002 sample_rate kept for logging if needed
        self._ensure_ready()
        audio_bytes = base64.b64decode(base64_data)
        payload = {"data": audio_bytes, "mime_type": "audio/pcm"}
        asyncio.run_coroutine_threadsafe(
            self.audio_loop.queue_realtime_input(payload),
            self.loop,
        )

    def _ensure_ready(self):
        if not self._ready.is_set():
            raise RuntimeError("Loop is not ready yet")

    async def _run(self):
        loop = asyncio.get_running_loop()
        self.loop = loop
        self.audio_loop = WebAudioLoop(mode=self.mode, socket_sid=self.sid)
        self._ready.set()
        await self.audio_loop.run()


class WebAudioLoop(AudioLoop):
    """AudioLoop variant that bridges Gemini Live to Socket.IO clients."""

    def __init__(self, mode="camera", *, socket_sid: str):
        super().__init__(
            video_mode=mode,
            external_media=True,
            enable_local_playback=False,
            enable_cli_text=False,
        )
        self.socket_sid = socket_sid

    async def send_text(self):
        while True:
            message = await self.text_queue.get()
            if message is None:
                break
            await self.session.send(input=message, end_of_turn=True)

    async def handle_incoming_text(self, text: str):
        await super().handle_incoming_text(text)
        socketio.emit('ai_response', {'text': text}, room=self.socket_sid)

    async def handle_incoming_audio(self, response):
        if response.data:
            socketio.emit(
                'audio_data',
                {
                    'data': base64.b64encode(response.data).decode(),
                    'sampleRate': RECEIVE_SAMPLE_RATE,
                },
                room=self.socket_sid,
            )

    async def handle_event(self, response):
        await super().handle_event(response)

    async def on_turn_complete(self):
        await super().on_turn_complete()


if __name__ == '__main__':
    print("Starting Gemini AI Web Interface...")
    print("Open your browser and go to http://localhost:8080")
    socketio.run(app, debug=True, host='0.0.0.0', port=8080, allow_unsafe_werkzeug=True)

```

```python
"""
Web frontend for the Gemini AI Audio/Video streaming application
"""

import asyncio
import base64
import threading

from flask import Flask, render_template, request
from flask_socketio import SocketIO, emit
from dotenv import load_dotenv

from main import AudioLoop, RECEIVE_SAMPLE_RATE, SEND_SAMPLE_RATE

# Load environment variables from .env file
load_dotenv()

# Import the AudioLoop class from main.py
from main import AudioLoop

app = Flask(__name__)
app.config['SECRET_KEY'] = 'your-secret-key-here'
socketio = SocketIO(app, cors_allowed_origins="*")

# Global variables
active_loops: dict[str, "LoopController"] = {}

@app.route('/')
def index():
    return render_template('index.html')

@socketio.on('connect')
def handle_connect():
    print('Client connected')
    emit('status', {'message': 'Connected to server'})

@socketio.on('disconnect')
def handle_disconnect():
    sid = request.sid
    print(f'Client disconnected: {sid}')
    loop_entry = active_loops.pop(sid, None)
    if loop_entry:
        loop_entry.stop()

@socketio.on('start_streaming')
def handle_start_streaming(data):
    sid = request.sid

    if sid in active_loops:
        emit('error', {'message': 'Already streaming'})
        return
    
    mode = data.get('mode', 'camera')
    
    controller = LoopController(mode=mode, sid=sid)
    controller.start()
    active_loops[sid] = controller
    emit('status', {'message': f'Started streaming with {mode} mode'})

@socketio.on('stop_streaming')
def handle_stop_streaming():
    sid = request.sid
    controller = active_loops.pop(sid, None)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return
    
    try:
        controller.stop()
        emit('status', {'message': 'Stopped streaming'})
        
    except Exception as e:
        emit('error', {'message': f'Failed to stop streaming: {str(e)}'})

@socketio.on('send_message')
def handle_send_message(data):
    sid = request.sid
    controller = active_loops.get(sid)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return
    
    message = data.get('message', '')
    if message:
        # Add message to the audio loop's queue
        try:
            controller.enqueue_text(message)
            emit('status', {'message': f'Sent message: {message}'})
        except Exception as e:
            emit('error', {'message': f'Failed to send message: {str(e)}'})


@socketio.on('video_frame')
def handle_video_frame(data):
    sid = request.sid
    controller = active_loops.get(sid)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return

    frame_data = data.get('data')
    mime_type = data.get('mime_type', 'image/jpeg')
    if not frame_data:
        emit('error', {'message': 'Missing frame data'})
        return

    try:
        controller.enqueue_frame(mime_type, frame_data)
    except Exception as exc:
        emit('error', {'message': f'Failed to queue frame: {exc}'})


@socketio.on('audio_chunk')
def handle_audio_chunk(data):
    sid = request.sid
    controller = active_loops.get(sid)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return

    chunk = data.get('data')
    sample_rate = data.get('sampleRate', SEND_SAMPLE_RATE)
    if not chunk:
        emit('error', {'message': 'Missing audio data'})
        return

    try:
        controller.enqueue_audio(chunk, sample_rate)
    except Exception as exc:
        emit('error', {'message': f'Failed to queue audio: {exc}'})


class LoopController:
    """Thread-safe controller bridging Flask events to AudioLoop."""

    def __init__(self, mode: str, sid: str):
        self.mode = mode
        self.sid = sid
        self.thread = None
        self.loop = None
        self.audio_loop: WebAudioLoop | None = None
        self._ready = threading.Event()
        self._error: Exception | None = None

    def start(self):
        def runner():
            try:
                asyncio.run(self._run())
            except Exception as exc:  # pragma: no cover - surfaced via _error
                self._error = exc
                self._ready.set()

        self.thread = threading.Thread(target=runner, daemon=True)
        self.thread.start()
        if not self._ready.wait(timeout=5):
            raise RuntimeError("Timed out waiting for streaming loop to start")
        if self._error:
            raise self._error

    def stop(self):
        if self.audio_loop and self.loop:
            asyncio.run_coroutine_threadsafe(self.audio_loop.shutdown(), self.loop)
        if self.thread:
            self.thread.join(timeout=2)

    def enqueue_text(self, message: str):
        self._ensure_ready()
        if not self.audio_loop or not self.audio_loop.text_queue:
            raise RuntimeError("Text queue unavailable")
        asyncio.run_coroutine_threadsafe(
            self.audio_loop.queue_text(message),
            self.loop,
        )

    def enqueue_frame(self, mime_type: str, base64_data: str):
        self._ensure_ready()
        payload = {"mime_type": mime_type, "data": base64_data}
        asyncio.run_coroutine_threadsafe(
            self.audio_loop.queue_realtime_input(payload),
            self.loop,
        )

    def enqueue_audio(self, base64_data: str, sample_rate: int):  # noqa: ARG002 sample_rate kept for logging if needed
        self._ensure_ready()
        audio_bytes = base64.b64decode(base64_data)
        payload = {"data": audio_bytes, "mime_type": "audio/pcm"}
        asyncio.run_coroutine_threadsafe(
            self.audio_loop.queue_realtime_input(payload),
            self.loop,
        )

    def _ensure_ready(self):
        if not self._ready.is_set():
            raise RuntimeError("Loop is not ready yet")

    async def _run(self):
        loop = asyncio.get_running_loop()
        self.loop = loop
        self.audio_loop = WebAudioLoop(mode=self.mode, socket_sid=self.sid)
        self._ready.set()
        await self.audio_loop.run()


class WebAudioLoop(AudioLoop):
    """AudioLoop variant that bridges Gemini Live to Socket.IO clients."""

    def __init__(self, mode="camera", *, socket_sid: str):
        super().__init__(
            video_mode=mode,
            external_media=True,
            enable_local_playback=False,
            enable_cli_text=False,
        )
        self.socket_sid = socket_sid

    async def send_text(self):
        while True:
            message = await self.text_queue.get()
            if message is None:
                break
            await self.session.send(input=message, end_of_turn=True)

    async def handle_incoming_text(self, text: str):
        await super().handle_incoming_text(text)
        socketio.emit('ai_response', {'text': text}, room=self.socket_sid)

    async def handle_incoming_audio(self, response):
        if response.data:
            socketio.emit(
                'audio_data',
                {
                    'data': base64.b64encode(response.data).decode(),
                    'sampleRate': RECEIVE_SAMPLE_RATE,
                },
                room=self.socket_sid,
            )

    async def handle_event(self, response):
        await super().handle_event(response)

    async def on_turn_complete(self):
        await super().on_turn_complete()


if __name__ == '__main__':
    print("Starting Gemini AI Web Interface...")
    print("Open your browser and go to http://localhost:8080")
    socketio.run(app, debug=True, host='0.0.0.0', port=8080, allow_unsafe_werkzeug=True)

```

```python
"""
## Documentation
Quickstart: https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI.py

## Setup

To install the dependencies for this script, run:

```
pip install google-genai opencv-python sounddevice pillow mss python-dotenv numpy

Important: **Use headphones**. This script uses the system default audio
input and output, which often won't include echo cancellation. So to prevent
the model from interrupting itself it is important that you use headphones. 

```
To run the script:
```
python main.py
```

The script takes a video-mode flag `--mode`, this can be "camera", "screen", or "none".
The default is "camera". To share your screen run:

```
python Get_started_LiveAPI.py --mode screen
```
"""

import os
import asyncio
import base64
import io
import traceback
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

import cv2
import sounddevice as sd
import numpy as np
import PIL.Image
import mss

import argparse

from google import genai
from google.genai import types

CHANNELS = 1
SEND_SAMPLE_RATE = 16000
RECEIVE_SAMPLE_RATE = 24000
CHUNK_SIZE = 1024
DTYPE = np.int16

MODEL = "models/gemini-2.5-flash-preview-native-audio-dialog"

DEFAULT_MODE = "camera"

client = genai.Client(
    http_options={"api_version": "v1beta"},
    api_key=os.environ.get("GEMINI_API_KEY"),
)


CONFIG = types.LiveConnectConfig(
    response_modalities=[
        "AUDIO",
    ],
    media_resolution="MEDIA_RESOLUTION_MEDIUM",
    speech_config=types.SpeechConfig(
        voice_config=types.VoiceConfig(
            prebuilt_voice_config=types.PrebuiltVoiceConfig(voice_name="Zephyr")
        )
    ),
    context_window_compression=types.ContextWindowCompressionConfig(
        trigger_tokens=25600,
        sliding_window=types.SlidingWindow(target_tokens=12800),
    ),
)

# Audio device will be selected in the AudioLoop class


class AudioLoop:
    def __init__(
        self,
        video_mode=DEFAULT_MODE,
        *,
        external_media: bool = False,
        enable_local_playback: bool = True,
        enable_cli_text: bool = True,
    ):
        self.video_mode = video_mode
        self.external_media = external_media
        self.enable_local_playback = enable_local_playback
        self.enable_cli_text = enable_cli_text

        self.session: genai.live.LiveConnectSession | None = None

        self.audio_in_queue: asyncio.Queue | None = None
        self.out_queue: asyncio.Queue | None = None
        self.text_queue: asyncio.Queue | None = None

        self.input_stream = None
        self.output_stream = None

        self._shutdown_future: asyncio.Future | None = None

    async def send_text(self):
        if self.enable_cli_text:
            while True:
                text = await asyncio.to_thread(
                    input,
                    "message > ",
                )
                if text.lower() == "q":
                    await self.shutdown()
                    break
                await self.session.send(input=text or ".", end_of_turn=True)
        else:
            while True:
                message = await self.text_queue.get()
                if message is None:
                    break
                await self.session.send(input=message, end_of_turn=True)

    def _get_frame(self, cap):
        # Read the frameq
        ret, frame = cap.read()
        # Check if the frame was read successfully
        if not ret:
            return None
        # Fix: Convert BGR to RGB color space
        # OpenCV captures in BGR but PIL expects RGB format
        # This prevents the blue tint in the video feed
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        img = PIL.Image.fromarray(frame_rgb)  # Now using RGB frame
        img.thumbnail([1024, 1024])

        image_io = io.BytesIO()
        img.save(image_io, format="jpeg")
        image_io.seek(0)

        mime_type = "image/jpeg"
        image_bytes = image_io.read()
        return {"mime_type": mime_type, "data": base64.b64encode(image_bytes).decode()}

    async def get_frames(self):
        # This takes about a second, and will block the whole program
        # causing the audio pipeline to overflow if you don't to_thread it.
        cap = await asyncio.to_thread(
            cv2.VideoCapture, 0
        )  # 0 represents the default camera

        while True:
            frame = await asyncio.to_thread(self._get_frame, cap)
            if frame is None:
                break

            await asyncio.sleep(1.0)

            await self.out_queue.put(frame)

        # Release the VideoCapture object
        cap.release()

    def _get_screen(self):
        sct = mss.mss()
        monitor = sct.monitors[0]

        i = sct.grab(monitor)

        mime_type = "image/jpeg"
        image_bytes = mss.tools.to_png(i.rgb, i.size)
        img = PIL.Image.open(io.BytesIO(image_bytes))

        image_io = io.BytesIO()
        img.save(image_io, format="jpeg")
        image_io.seek(0)

        image_bytes = image_io.read()
        return {"mime_type": mime_type, "data": base64.b64encode(image_bytes).decode()}

    async def get_screen(self):

        while True:
            frame = await asyncio.to_thread(self._get_screen)
            if frame is None:
                break

            await asyncio.sleep(1.0)

            await self.out_queue.put(frame)

    async def send_realtime(self):
        while True:
            msg = await self.out_queue.get()
            await self.session.send(input=msg)

    async def queue_realtime_input(self, payload):
        if not self.out_queue:
            raise RuntimeError("Realtime queue not ready")
        if self.out_queue.full():
            try:
                self.out_queue.get_nowait()
            except asyncio.QueueEmpty:
                pass
        await self.out_queue.put(payload)

    async def queue_text(self, message: str, *, end_of_turn: bool = True):
        if self.enable_cli_text:
            raise RuntimeError("Text queue not available in CLI mode")
        if not self.text_queue:
            raise RuntimeError("Text queue not ready")
        if end_of_turn:
            await self.text_queue.put(message)
        else:
            await self.session.send(input=message, end_of_turn=False)

    async def listen_audio(self):
        # Create a queue for audio data
        audio_queue = asyncio.Queue()
        
        def audio_callback(indata, frames, time, status):
            if status:
                print(f"Audio input status: {status}")
            # Convert float32 to int16 and get raw bytes
            audio_data = (indata[:, 0] * 32767).astype(DTYPE).tobytes()
            try:
                audio_queue.put_nowait({"data": audio_data, "mime_type": "audio/pcm"})
            except asyncio.QueueFull:
                pass  # Drop frames if queue is full
        
        # Start the input stream
        self.input_stream = sd.InputStream(
            samplerate=SEND_SAMPLE_RATE,
            channels=CHANNELS,
            dtype='float32',
            blocksize=CHUNK_SIZE,
            callback=audio_callback
        )
        
        self.input_stream.start()
        
        try:
            while True:
                audio_data = await audio_queue.get()
                await self.queue_realtime_input(audio_data)
        finally:
            if self.input_stream:
                self.input_stream.stop()
                self.input_stream.close()
                self.input_stream = None

    async def receive_audio(self):
        while True:
            turn = self.session.receive()
            async for response in turn:
                if data := response.data:
                    await self.handle_incoming_audio(response)
                    continue
                if text := response.text:
                    await self.handle_incoming_text(text)
                await self.handle_event(response)
            await self.on_turn_complete()

    async def handle_incoming_audio(self, response):
        if not self.enable_local_playback or not self.audio_in_queue:
            return
        try:
            self.audio_in_queue.put_nowait(response.data)
        except asyncio.QueueFull:
            while not self.audio_in_queue.empty():
                self.audio_in_queue.get_nowait()
            try:
                self.audio_in_queue.put_nowait(response.data)
            except asyncio.QueueFull:
                pass

    async def handle_incoming_text(self, text: str):
        print(text, end="")

    async def handle_event(self, response):
        model_turn = getattr(response, "model_turn", None)
        if model_turn and getattr(model_turn, "turn_complete", False):
            await self.on_turn_complete()

    async def on_turn_complete(self):
        if not self.enable_local_playback or not self.audio_in_queue:
            return
        while not self.audio_in_queue.empty():
            self.audio_in_queue.get_nowait()

    async def play_audio(self):
        # Create a queue for output audio data
        output_queue = asyncio.Queue()
        
        def audio_callback(outdata, frames, time, status):
            if status:
                print(f"Audio output status: {status}")
            try:
                # Get audio data from queue (non-blocking)
                audio_data = output_queue.get_nowait()
                # Convert bytes to numpy array and reshape
                audio_array = np.frombuffer(audio_data, dtype=DTYPE).astype(np.float32) / 32767.0
                # Ensure we have the right number of frames
                if len(audio_array) >= frames:
                    outdata[:, 0] = audio_array[:frames]
                else:
                    # Pad with zeros if not enough data
                    outdata[:len(audio_array), 0] = audio_array
                    outdata[len(audio_array):, 0] = 0
            except asyncio.QueueEmpty:
                # No audio data available, output silence
                outdata[:, 0] = 0
        
        # Start the output stream
        self.output_stream = sd.OutputStream(
            samplerate=RECEIVE_SAMPLE_RATE,
            channels=CHANNELS,
            dtype='float32',
            blocksize=CHUNK_SIZE,
            callback=audio_callback
        )
        
        self.output_stream.start()
        
        try:
            while True:
                bytestream = await self.audio_in_queue.get()
                await output_queue.put(bytestream)
        finally:
            if self.output_stream:
                self.output_stream.stop()
                self.output_stream.close()
            if self.output_stream:
                self.output_stream = None

    async def shutdown(self):
        if self._shutdown_future and not self._shutdown_future.done():
            self._shutdown_future.set_result(None)
        if not self.enable_cli_text and self.text_queue:
            await self.text_queue.put(None)
        await self._close_streams()

    async def _close_streams(self):
        if self.input_stream:
            self.input_stream.stop()
            self.input_stream.close()
            self.input_stream = None
        if self.output_stream:
            self.output_stream.stop()
            self.output_stream.close()
            self.output_stream = None

    async def run(self):
        try:
            async with (
                client.aio.live.connect(model=MODEL, config=CONFIG) as session,
                asyncio.TaskGroup() as tg,
            ):
                self.session = session
                loop = asyncio.get_running_loop()
                self._loop = loop
                self.audio_in_queue = asyncio.Queue(maxsize=50)
                self.out_queue = asyncio.Queue(maxsize=10)
                if not self.enable_cli_text:
                    self.text_queue = asyncio.Queue()
                else:
                    self.text_queue = None
                self._shutdown_future = loop.create_future()

                if self.enable_cli_text or self.text_queue is not None:
                    tg.create_task(self.send_text())

                tg.create_task(self.send_realtime())

                if not self.external_media:
                    tg.create_task(self.listen_audio())
                    if self.video_mode == "camera":
                        tg.create_task(self.get_frames())
                    elif self.video_mode == "screen":
                        tg.create_task(self.get_screen())

                tg.create_task(self.receive_audio())

                if self.enable_local_playback:
                    tg.create_task(self.play_audio())

                await self._shutdown_future
                raise asyncio.CancelledError("Shutdown requested")

        except asyncio.CancelledError:
            pass
        except ExceptionGroup as EG:
            traceback.print_exception(EG)
        finally:
            await self._close_streams()


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--mode",
        type=str,
        default=DEFAULT_MODE,
        help="pixels to stream from",
        choices=["camera", "screen", "none"],
    )
    args = parser.parse_args()
    main = AudioLoop(video_mode=args.mode)
    asyncio.run(main.run())

```

```python
"""
Web frontend for the Gemini AI Audio/Video streaming application
"""

import asyncio
import base64
import threading

from flask import Flask, render_template, request
from flask_socketio import SocketIO, emit
from dotenv import load_dotenv

from main import AudioLoop, RECEIVE_SAMPLE_RATE, SEND_SAMPLE_RATE

# Load environment variables from .env file
load_dotenv()

# Import the AudioLoop class from main.py
from main import AudioLoop

app = Flask(__name__)
app.config['SECRET_KEY'] = 'your-secret-key-here'
socketio = SocketIO(app, cors_allowed_origins="*")

# Global variables
active_loops: dict[str, "LoopController"] = {}

@app.route('/')
def index():
    return render_template('index.html')

@socketio.on('connect')
def handle_connect():
    print('Client connected')
    emit('status', {'message': 'Connected to server'})

@socketio.on('disconnect')
def handle_disconnect():
    sid = request.sid
    print(f'Client disconnected: {sid}')
    loop_entry = active_loops.pop(sid, None)
    if loop_entry:
        loop_entry.stop()

@socketio.on('start_streaming')
def handle_start_streaming(data):
    sid = request.sid

    if sid in active_loops:
        emit('error', {'message': 'Already streaming'})
        return
    
    mode = data.get('mode', 'camera')
    
    controller = LoopController(mode=mode, sid=sid)
    controller.start()
    active_loops[sid] = controller
    emit('status', {'message': f'Started streaming with {mode} mode'})

@socketio.on('stop_streaming')
def handle_stop_streaming():
    sid = request.sid
    controller = active_loops.pop(sid, None)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return
    
    try:
        controller.stop()
        emit('status', {'message': 'Stopped streaming'})
        
    except Exception as e:
        emit('error', {'message': f'Failed to stop streaming: {str(e)}'})

@socketio.on('send_message')
def handle_send_message(data):
    sid = request.sid
    controller = active_loops.get(sid)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return
    
    message = data.get('message', '')
    if message:
        # Add message to the audio loop's queue
        try:
            controller.enqueue_text(message)
            emit('status', {'message': f'Sent message: {message}'})
        except Exception as e:
            emit('error', {'message': f'Failed to send message: {str(e)}'})


@socketio.on('video_frame')
def handle_video_frame(data):
    sid = request.sid
    controller = active_loops.get(sid)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return

    frame_data = data.get('data')
    mime_type = data.get('mime_type', 'image/jpeg')
    if not frame_data:
        emit('error', {'message': 'Missing frame data'})
        return

    try:
        controller.enqueue_frame(mime_type, frame_data)
    except Exception as exc:
        emit('error', {'message': f'Failed to queue frame: {exc}'})


@socketio.on('audio_chunk')
def handle_audio_chunk(data):
    sid = request.sid
    controller = active_loops.get(sid)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return

    chunk = data.get('data')
    sample_rate = data.get('sampleRate', SEND_SAMPLE_RATE)
    if not chunk:
        emit('error', {'message': 'Missing audio data'})
        return

    try:
        controller.enqueue_audio(chunk, sample_rate)
    except Exception as exc:
        emit('error', {'message': f'Failed to queue audio: {exc}'})


class LoopController:
    """Thread-safe controller bridging Flask events to AudioLoop."""

    def __init__(self, mode: str, sid: str):
        self.mode = mode
        self.sid = sid
        self.thread = None
        self.loop = None
        self.audio_loop: WebAudioLoop | None = None
        self._ready = threading.Event()
        self._error: Exception | None = None

    def start(self):
        def runner():
            try:
                asyncio.run(self._run())
            except Exception as exc:  # pragma: no cover - surfaced via _error
                self._error = exc
                self._ready.set()

        self.thread = threading.Thread(target=runner, daemon=True)
        self.thread.start()
        if not self._ready.wait(timeout=5):
            raise RuntimeError("Timed out waiting for streaming loop to start")
        if self._error:
            raise self._error

    def stop(self):
        if self.audio_loop and self.loop:
            asyncio.run_coroutine_threadsafe(self.audio_loop.shutdown(), self.loop)
        if self.thread:
            self.thread.join(timeout=2)

    def enqueue_text(self, message: str):
        self._ensure_ready()
        if not self.audio_loop or not self.audio_loop.text_queue:
            raise RuntimeError("Text queue unavailable")
        asyncio.run_coroutine_threadsafe(
            self.audio_loop.queue_text(message),
            self.loop,
        )

    def enqueue_frame(self, mime_type: str, base64_data: str):
        self._ensure_ready()
        payload = {"mime_type": mime_type, "data": base64_data}
        asyncio.run_coroutine_threadsafe(
            self.audio_loop.queue_realtime_input(payload),
            self.loop,
        )

    def enqueue_audio(self, base64_data: str, sample_rate: int):  # noqa: ARG002 sample_rate kept for logging if needed
        self._ensure_ready()
        audio_bytes = base64.b64decode(base64_data)
        payload = {"data": audio_bytes, "mime_type": "audio/pcm"}
        asyncio.run_coroutine_threadsafe(
            self.audio_loop.queue_realtime_input(payload),
            self.loop,
        )

    def _ensure_ready(self):
        if not self._ready.is_set():
            raise RuntimeError("Loop is not ready yet")

    async def _run(self):
        loop = asyncio.get_running_loop()
        self.loop = loop
        self.audio_loop = WebAudioLoop(mode=self.mode, socket_sid=self.sid)
        self._ready.set()
        await self.audio_loop.run()


class WebAudioLoop(AudioLoop):
    """AudioLoop variant that bridges Gemini Live to Socket.IO clients."""

    def __init__(self, mode="camera", *, socket_sid: str):
        super().__init__(
            video_mode=mode,
            external_media=True,
            enable_local_playback=False,
            enable_cli_text=False,
        )
        self.socket_sid = socket_sid

    async def send_text(self):
        while True:
            message = await self.text_queue.get()
            if message is None:
                break
            await self.session.send(input=message, end_of_turn=True)

    async def handle_incoming_text(self, text: str):
        await super().handle_incoming_text(text)
        socketio.emit('ai_response', {'text': text}, room=self.socket_sid)

    async def handle_incoming_audio(self, response):
        if response.data:
            socketio.emit(
                'audio_data',
                {
                    'data': base64.b64encode(response.data).decode(),
                    'sampleRate': RECEIVE_SAMPLE_RATE,
                },
                room=self.socket_sid,
            )

    async def handle_event(self, response):
        await super().handle_event(response)

    async def on_turn_complete(self):
        await super().on_turn_complete()


if __name__ == '__main__':
    print("Starting Gemini AI Web Interface...")
    print("Open your browser and go to http://localhost:8080")
    socketio.run(app, debug=True, host='0.0.0.0', port=8080, allow_unsafe_werkzeug=True)

```

```python
"""
## Documentation
Quickstart: https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI.py

## Setup

To install the dependencies for this script, run:

```
pip install google-genai opencv-python sounddevice pillow mss python-dotenv numpy

Important: **Use headphones**. This script uses the system default audio
input and output, which often won't include echo cancellation. So to prevent
the model from interrupting itself it is important that you use headphones. 

```
To run the script:
```
python main.py
```

The script takes a video-mode flag `--mode`, this can be "camera", "screen", or "none".
The default is "camera". To share your screen run:

```
python Get_started_LiveAPI.py --mode screen
```
"""

import os
import asyncio
import base64
import io
import traceback
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

import cv2
import sounddevice as sd
import numpy as np
import PIL.Image
import mss

import argparse

from google import genai
from google.genai import types

CHANNELS = 1
SEND_SAMPLE_RATE = 16000
RECEIVE_SAMPLE_RATE = 24000
CHUNK_SIZE = 1024
DTYPE = np.int16

MODEL = "models/gemini-2.5-flash-preview-native-audio-dialog"

DEFAULT_MODE = "camera"

client = genai.Client(
    http_options={"api_version": "v1beta"},
    api_key=os.environ.get("GEMINI_API_KEY"),
)


CONFIG = types.LiveConnectConfig(
    response_modalities=[
        "AUDIO",
    ],
    media_resolution="MEDIA_RESOLUTION_MEDIUM",
    speech_config=types.SpeechConfig(
        voice_config=types.VoiceConfig(
            prebuilt_voice_config=types.PrebuiltVoiceConfig(voice_name="Zephyr")
        )
    ),
    context_window_compression=types.ContextWindowCompressionConfig(
        trigger_tokens=25600,
        sliding_window=types.SlidingWindow(target_tokens=12800),
    ),
)

# Audio device will be selected in the AudioLoop class


class AudioLoop:
    def __init__(
        self,
        video_mode=DEFAULT_MODE,
        *,
        external_media: bool = False,
        enable_local_playback: bool = True,
        enable_cli_text: bool = True,
    ):
        self.video_mode = video_mode
        self.external_media = external_media
        self.enable_local_playback = enable_local_playback
        self.enable_cli_text = enable_cli_text

        self.session: genai.live.LiveConnectSession | None = None

        self.audio_in_queue: asyncio.Queue | None = None
        self.out_queue: asyncio.Queue | None = None
        self.text_queue: asyncio.Queue | None = None

        self.input_stream = None
        self.output_stream = None

        self._shutdown_future: asyncio.Future | None = None

    async def send_text(self):
        if self.enable_cli_text:
            while True:
                text = await asyncio.to_thread(
                    input,
                    "message > ",
                )
                if text.lower() == "q":
                    await self.shutdown()
                    break
                await self.session.send(input=text or ".", end_of_turn=True)
        else:
            while True:
                message = await self.text_queue.get()
                if message is None:
                    break
                await self.session.send(input=message, end_of_turn=True)

    def _get_frame(self, cap):
        # Read the frameq
        ret, frame = cap.read()
        # Check if the frame was read successfully
        if not ret:
            return None
        # Fix: Convert BGR to RGB color space
        # OpenCV captures in BGR but PIL expects RGB format
        # This prevents the blue tint in the video feed
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        img = PIL.Image.fromarray(frame_rgb)  # Now using RGB frame
        img.thumbnail([1024, 1024])

        image_io = io.BytesIO()
        img.save(image_io, format="jpeg")
        image_io.seek(0)

        mime_type = "image/jpeg"
        image_bytes = image_io.read()
        return {"mime_type": mime_type, "data": base64.b64encode(image_bytes).decode()}

    async def get_frames(self):
        # This takes about a second, and will block the whole program
        # causing the audio pipeline to overflow if you don't to_thread it.
        cap = await asyncio.to_thread(
            cv2.VideoCapture, 0
        )  # 0 represents the default camera

        while True:
            frame = await asyncio.to_thread(self._get_frame, cap)
            if frame is None:
                break

            await asyncio.sleep(1.0)

            await self.out_queue.put(frame)

        # Release the VideoCapture object
        cap.release()

    def _get_screen(self):
        sct = mss.mss()
        monitor = sct.monitors[0]

        i = sct.grab(monitor)

        mime_type = "image/jpeg"
        image_bytes = mss.tools.to_png(i.rgb, i.size)
        img = PIL.Image.open(io.BytesIO(image_bytes))

        image_io = io.BytesIO()
        img.save(image_io, format="jpeg")
        image_io.seek(0)

        image_bytes = image_io.read()
        return {"mime_type": mime_type, "data": base64.b64encode(image_bytes).decode()}

    async def get_screen(self):

        while True:
            frame = await asyncio.to_thread(self._get_screen)
            if frame is None:
                break

            await asyncio.sleep(1.0)

            await self.out_queue.put(frame)

    async def send_realtime(self):
        while True:
            msg = await self.out_queue.get()
            await self.session.send(input=msg)

    async def queue_realtime_input(self, payload):
        if not self.out_queue:
            raise RuntimeError("Realtime queue not ready")
        if self.out_queue.full():
            try:
                self.out_queue.get_nowait()
            except asyncio.QueueEmpty:
                pass
        await self.out_queue.put(payload)

    async def queue_text(self, message: str, *, end_of_turn: bool = True):
        if self.enable_cli_text:
            raise RuntimeError("Text queue not available in CLI mode")
        if not self.text_queue:
            raise RuntimeError("Text queue not ready")
        if end_of_turn:
            await self.text_queue.put(message)
        else:
            await self.session.send(input=message, end_of_turn=False)

    async def listen_audio(self):
        # Create a queue for audio data
        audio_queue = asyncio.Queue()
        
        def audio_callback(indata, frames, time, status):
            if status:
                print(f"Audio input status: {status}")
            # Convert float32 to int16 and get raw bytes
            audio_data = (indata[:, 0] * 32767).astype(DTYPE).tobytes()
            try:
                audio_queue.put_nowait({"data": audio_data, "mime_type": "audio/pcm"})
            except asyncio.QueueFull:
                pass  # Drop frames if queue is full
        
        # Start the input stream
        self.input_stream = sd.InputStream(
            samplerate=SEND_SAMPLE_RATE,
            channels=CHANNELS,
            dtype='float32',
            blocksize=CHUNK_SIZE,
            callback=audio_callback
        )
        
        self.input_stream.start()
        
        try:
            while True:
                audio_data = await audio_queue.get()
                await self.queue_realtime_input(audio_data)
        finally:
            if self.input_stream:
                self.input_stream.stop()
                self.input_stream.close()
                self.input_stream = None

    async def receive_audio(self):
        while True:
            turn = self.session.receive()
            async for response in turn:
                if data := response.data:
                    await self.handle_incoming_audio(response)
                    continue
                if text := response.text:
                    await self.handle_incoming_text(text)
                await self.handle_event(response)
            await self.on_turn_complete()

    async def handle_incoming_audio(self, response):
        if not self.enable_local_playback or not self.audio_in_queue:
            return
        try:
            self.audio_in_queue.put_nowait(response.data)
        except asyncio.QueueFull:
            while not self.audio_in_queue.empty():
                self.audio_in_queue.get_nowait()
            try:
                self.audio_in_queue.put_nowait(response.data)
            except asyncio.QueueFull:
                pass

    async def handle_incoming_text(self, text: str):
        print(text, end="")

    async def handle_event(self, response):
        model_turn = getattr(response, "model_turn", None)
        if model_turn and getattr(model_turn, "turn_complete", False):
            await self.on_turn_complete()

    async def on_turn_complete(self):
        if not self.enable_local_playback or not self.audio_in_queue:
            return
        while not self.audio_in_queue.empty():
            self.audio_in_queue.get_nowait()

    async def play_audio(self):
        # Create a queue for output audio data
        output_queue = asyncio.Queue()
        
        def audio_callback(outdata, frames, time, status):
            if status:
                print(f"Audio output status: {status}")
            try:
                # Get audio data from queue (non-blocking)
                audio_data = output_queue.get_nowait()
                # Convert bytes to numpy array and reshape
                audio_array = np.frombuffer(audio_data, dtype=DTYPE).astype(np.float32) / 32767.0
                # Ensure we have the right number of frames
                if len(audio_array) >= frames:
                    outdata[:, 0] = audio_array[:frames]
                else:
                    # Pad with zeros if not enough data
                    outdata[:len(audio_array), 0] = audio_array
                    outdata[len(audio_array):, 0] = 0
            except asyncio.QueueEmpty:
                # No audio data available, output silence
                outdata[:, 0] = 0
        
        # Start the output stream
        self.output_stream = sd.OutputStream(
            samplerate=RECEIVE_SAMPLE_RATE,
            channels=CHANNELS,
            dtype='float32',
            blocksize=CHUNK_SIZE,
            callback=audio_callback
        )
        
        self.output_stream.start()
        
        try:
            while True:
                bytestream = await self.audio_in_queue.get()
                await output_queue.put(bytestream)
        finally:
            if self.output_stream:
                self.output_stream.stop()
                self.output_stream.close()
            if self.output_stream:
                self.output_stream = None

    async def shutdown(self):
        if self._shutdown_future and not self._shutdown_future.done():
            self._shutdown_future.set_result(None)
        if not self.enable_cli_text and self.text_queue:
            await self.text_queue.put(None)
        await self._close_streams()

    async def _close_streams(self):
        if self.input_stream:
            self.input_stream.stop()
            self.input_stream.close()
            self.input_stream = None
        if self.output_stream:
            self.output_stream.stop()
            self.output_stream.close()
            self.output_stream = None

    async def run(self):
        try:
            async with (
                client.aio.live.connect(model=MODEL, config=CONFIG) as session,
                asyncio.TaskGroup() as tg,
            ):
                self.session = session
                loop = asyncio.get_running_loop()
                self._loop = loop
                self.audio_in_queue = asyncio.Queue(maxsize=50)
                self.out_queue = asyncio.Queue(maxsize=10)
                if not self.enable_cli_text:
                    self.text_queue = asyncio.Queue()
                else:
                    self.text_queue = None
                self._shutdown_future = loop.create_future()

                if self.enable_cli_text or self.text_queue is not None:
                    tg.create_task(self.send_text())

                tg.create_task(self.send_realtime())

                if not self.external_media:
                    tg.create_task(self.listen_audio())
                    if self.video_mode == "camera":
                        tg.create_task(self.get_frames())
                    elif self.video_mode == "screen":
                        tg.create_task(self.get_screen())

                tg.create_task(self.receive_audio())

                if self.enable_local_playback:
                    tg.create_task(self.play_audio())

                await self._shutdown_future
                raise asyncio.CancelledError("Shutdown requested")

        except asyncio.CancelledError:
            pass
        except ExceptionGroup as EG:
            traceback.print_exception(EG)
        finally:
            await self._close_streams()


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--mode",
        type=str,
        default=DEFAULT_MODE,
        help="pixels to stream from",
        choices=["camera", "screen", "none"],
    )
    args = parser.parse_args()
    main = AudioLoop(video_mode=args.mode)
    asyncio.run(main.run())

```

```python
"""
Web frontend for the Gemini AI Audio/Video streaming application
"""

import asyncio
import base64
import threading

from flask import Flask, render_template, request
from flask_socketio import SocketIO, emit
from dotenv import load_dotenv

from main import AudioLoop, RECEIVE_SAMPLE_RATE, SEND_SAMPLE_RATE

# Load environment variables from .env file
load_dotenv()

# Import the AudioLoop class from main.py
from main import AudioLoop

app = Flask(__name__)
app.config['SECRET_KEY'] = 'your-secret-key-here'
socketio = SocketIO(app, cors_allowed_origins="*")

# Global variables
active_loops: dict[str, "LoopController"] = {}

@app.route('/')
def index():
    return render_template('index.html')

@socketio.on('connect')
def handle_connect():
    print('Client connected')
    emit('status', {'message': 'Connected to server'})

@socketio.on('disconnect')
def handle_disconnect():
    sid = request.sid
    print(f'Client disconnected: {sid}')
    loop_entry = active_loops.pop(sid, None)
    if loop_entry:
        loop_entry.stop()

@socketio.on('start_streaming')
def handle_start_streaming(data):
    sid = request.sid

    if sid in active_loops:
        emit('error', {'message': 'Already streaming'})
        return
    
    mode = data.get('mode', 'camera')
    
    controller = LoopController(mode=mode, sid=sid)
    controller.start()
    active_loops[sid] = controller
    emit('status', {'message': f'Started streaming with {mode} mode'})

@socketio.on('stop_streaming')
def handle_stop_streaming():
    sid = request.sid
    controller = active_loops.pop(sid, None)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return
    
    try:
        controller.stop()
        emit('status', {'message': 'Stopped streaming'})
        
    except Exception as e:
        emit('error', {'message': f'Failed to stop streaming: {str(e)}'})

@socketio.on('send_message')
def handle_send_message(data):
    sid = request.sid
    controller = active_loops.get(sid)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return
    
    message = data.get('message', '')
    if message:
        # Add message to the audio loop's queue
        try:
            controller.enqueue_text(message)
            emit('status', {'message': f'Sent message: {message}'})
        except Exception as e:
            emit('error', {'message': f'Failed to send message: {str(e)}'})


@socketio.on('video_frame')
def handle_video_frame(data):
    sid = request.sid
    controller = active_loops.get(sid)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return

    frame_data = data.get('data')
    mime_type = data.get('mime_type', 'image/jpeg')
    if not frame_data:
        emit('error', {'message': 'Missing frame data'})
        return

    try:
        controller.enqueue_frame(mime_type, frame_data)
    except Exception as exc:
        emit('error', {'message': f'Failed to queue frame: {exc}'})


@socketio.on('audio_chunk')
def handle_audio_chunk(data):
    sid = request.sid
    controller = active_loops.get(sid)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return

    chunk = data.get('data')
    sample_rate = data.get('sampleRate', SEND_SAMPLE_RATE)
    if not chunk:
        emit('error', {'message': 'Missing audio data'})
        return

    try:
        controller.enqueue_audio(chunk, sample_rate)
    except Exception as exc:
        emit('error', {'message': f'Failed to queue audio: {exc}'})


class LoopController:
    """Thread-safe controller bridging Flask events to AudioLoop."""

    def __init__(self, mode: str, sid: str):
        self.mode = mode
        self.sid = sid
        self.thread = None
        self.loop = None
        self.audio_loop: WebAudioLoop | None = None
        self._ready = threading.Event()
        self._error: Exception | None = None

    def start(self):
        def runner():
            try:
                asyncio.run(self._run())
            except Exception as exc:  # pragma: no cover - surfaced via _error
                self._error = exc
                self._ready.set()

        self.thread = threading.Thread(target=runner, daemon=True)
        self.thread.start()
        if not self._ready.wait(timeout=5):
            raise RuntimeError("Timed out waiting for streaming loop to start")
        if self._error:
            raise self._error

    def stop(self):
        if self.audio_loop and self.loop:
            asyncio.run_coroutine_threadsafe(self.audio_loop.shutdown(), self.loop)
        if self.thread:
            self.thread.join(timeout=2)

    def enqueue_text(self, message: str):
        self._ensure_ready()
        if not self.audio_loop or not self.audio_loop.text_queue:
            raise RuntimeError("Text queue unavailable")
        asyncio.run_coroutine_threadsafe(
            self.audio_loop.queue_text(message),
            self.loop,
        )

    def enqueue_frame(self, mime_type: str, base64_data: str):
        self._ensure_ready()
        payload = {"mime_type": mime_type, "data": base64_data}
        asyncio.run_coroutine_threadsafe(
            self.audio_loop.queue_realtime_input(payload),
            self.loop,
        )

    def enqueue_audio(self, base64_data: str, sample_rate: int):  # noqa: ARG002 sample_rate kept for logging if needed
        self._ensure_ready()
        audio_bytes = base64.b64decode(base64_data)
        payload = {"data": audio_bytes, "mime_type": "audio/pcm"}
        asyncio.run_coroutine_threadsafe(
            self.audio_loop.queue_realtime_input(payload),
            self.loop,
        )

    def _ensure_ready(self):
        if not self._ready.is_set():
            raise RuntimeError("Loop is not ready yet")

    async def _run(self):
        loop = asyncio.get_running_loop()
        self.loop = loop
        self.audio_loop = WebAudioLoop(mode=self.mode, socket_sid=self.sid)
        self._ready.set()
        await self.audio_loop.run()


class WebAudioLoop(AudioLoop):
    """AudioLoop variant that bridges Gemini Live to Socket.IO clients."""

    def __init__(self, mode="camera", *, socket_sid: str):
        super().__init__(
            video_mode=mode,
            external_media=True,
            enable_local_playback=False,
            enable_cli_text=False,
        )
        self.socket_sid = socket_sid

    async def send_text(self):
        while True:
            message = await self.text_queue.get()
            if message is None:
                break
            await self.session.send(input=message, end_of_turn=True)

    async def handle_incoming_text(self, text: str):
        await super().handle_incoming_text(text)
        socketio.emit('ai_response', {'text': text}, room=self.socket_sid)

    async def handle_incoming_audio(self, response):
        if response.data:
            socketio.emit(
                'audio_data',
                {
                    'data': base64.b64encode(response.data).decode(),
                    'sampleRate': RECEIVE_SAMPLE_RATE,
                },
                room=self.socket_sid,
            )

    async def handle_event(self, response):
        await super().handle_event(response)

    async def on_turn_complete(self):
        await super().on_turn_complete()


if __name__ == '__main__':
    print("Starting Gemini AI Web Interface...")
    print("Open your browser and go to http://localhost:8080")
    socketio.run(app, debug=True, host='0.0.0.0', port=8080, allow_unsafe_werkzeug=True)

```

```python
"""
## Documentation
Quickstart: https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI.py

## Setup

To install the dependencies for this script, run:

```
pip install google-genai opencv-python sounddevice pillow mss python-dotenv numpy

Important: **Use headphones**. This script uses the system default audio
input and output, which often won't include echo cancellation. So to prevent
the model from interrupting itself it is important that you use headphones. 

```
To run the script:
```
python main.py
```

The script takes a video-mode flag `--mode`, this can be "camera", "screen", or "none".
The default is "camera". To share your screen run:

```
python Get_started_LiveAPI.py --mode screen
```
"""

import os
import asyncio
import base64
import io
import traceback
from typing import Optional
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

import cv2
import sounddevice as sd
import numpy as np
import PIL.Image
import mss

import argparse

from google import genai
from google.genai import types

CHANNELS = 1
SEND_SAMPLE_RATE = 16000
RECEIVE_SAMPLE_RATE = 24000
CHUNK_SIZE = 1024
DTYPE = np.int16

MODEL = "models/gemini-2.5-flash-preview-native-audio-dialog"

DEFAULT_MODE = "camera"

client = genai.Client(
    http_options={"api_version": "v1beta"},
    api_key=os.environ.get("GEMINI_API_KEY"),
)


CONFIG = types.LiveConnectConfig(
    response_modalities=[
        "AUDIO",
    ],
    media_resolution="MEDIA_RESOLUTION_MEDIUM",
    speech_config=types.SpeechConfig(
        voice_config=types.VoiceConfig(
            prebuilt_voice_config=types.PrebuiltVoiceConfig(voice_name="Zephyr")
        )
    ),
    context_window_compression=types.ContextWindowCompressionConfig(
        trigger_tokens=25600,
        sliding_window=types.SlidingWindow(target_tokens=12800),
    ),
)

# Audio device will be selected in the AudioLoop class


class AudioLoop:
    def __init__(
        self,
        video_mode=DEFAULT_MODE,
        *,
        external_media: bool = False,
        enable_local_playback: bool = True,
        enable_cli_text: bool = True,
    ):
        self.video_mode = video_mode
        self.external_media = external_media
        self.enable_local_playback = enable_local_playback
        self.enable_cli_text = enable_cli_text

        self.session: Optional[genai.live.LiveConnectSession] = None

        self.audio_in_queue: Optional[asyncio.Queue] = None
        self.out_queue: Optional[asyncio.Queue] = None
        self.text_queue: Optional[asyncio.Queue] = None

        self.input_stream = None
        self.output_stream = None

        self._shutdown_future: Optional[asyncio.Future] = None

    async def send_text(self):
        if self.enable_cli_text:
            while True:
                text = await asyncio.to_thread(
                    input,
                    "message > ",
                )
                if text.lower() == "q":
                    await self.shutdown()
                    break
                await self.session.send(input=text or ".", end_of_turn=True)
        else:
            while True:
                message = await self.text_queue.get()
                if message is None:
                    break
                await self.session.send(input=message, end_of_turn=True)

    def _get_frame(self, cap):
        # Read the frameq
        ret, frame = cap.read()
        # Check if the frame was read successfully
        if not ret:
            return None
        # Fix: Convert BGR to RGB color space
        # OpenCV captures in BGR but PIL expects RGB format
        # This prevents the blue tint in the video feed
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        img = PIL.Image.fromarray(frame_rgb)  # Now using RGB frame
        img.thumbnail([1024, 1024])

        image_io = io.BytesIO()
        img.save(image_io, format="jpeg")
        image_io.seek(0)

        mime_type = "image/jpeg"
        image_bytes = image_io.read()
        return {"mime_type": mime_type, "data": base64.b64encode(image_bytes).decode()}

    async def get_frames(self):
        # This takes about a second, and will block the whole program
        # causing the audio pipeline to overflow if you don't to_thread it.
        cap = await asyncio.to_thread(
            cv2.VideoCapture, 0
        )  # 0 represents the default camera

        while True:
            frame = await asyncio.to_thread(self._get_frame, cap)
            if frame is None:
                break

            await asyncio.sleep(1.0)

            await self.out_queue.put(frame)

        # Release the VideoCapture object
        cap.release()

    def _get_screen(self):
        sct = mss.mss()
        monitor = sct.monitors[0]

        i = sct.grab(monitor)

        mime_type = "image/jpeg"
        image_bytes = mss.tools.to_png(i.rgb, i.size)
        img = PIL.Image.open(io.BytesIO(image_bytes))

        image_io = io.BytesIO()
        img.save(image_io, format="jpeg")
        image_io.seek(0)

        image_bytes = image_io.read()
        return {"mime_type": mime_type, "data": base64.b64encode(image_bytes).decode()}

    async def get_screen(self):

        while True:
            frame = await asyncio.to_thread(self._get_screen)
            if frame is None:
                break

            await asyncio.sleep(1.0)

            await self.out_queue.put(frame)

    async def send_realtime(self):
        while True:
            msg = await self.out_queue.get()
            await self.session.send(input=msg)

    async def queue_realtime_input(self, payload):
        if not self.out_queue:
            raise RuntimeError("Realtime queue not ready")
        if self.out_queue.full():
            try:
                self.out_queue.get_nowait()
            except asyncio.QueueEmpty:
                pass
        await self.out_queue.put(payload)

    async def queue_text(self, message: str, *, end_of_turn: bool = True):
        if self.enable_cli_text:
            raise RuntimeError("Text queue not available in CLI mode")
        if not self.text_queue:
            raise RuntimeError("Text queue not ready")
        if end_of_turn:
            await self.text_queue.put(message)
        else:
            await self.session.send(input=message, end_of_turn=False)

    async def listen_audio(self):
        # Create a queue for audio data
        audio_queue = asyncio.Queue()
        
        def audio_callback(indata, frames, time, status):
            if status:
                print(f"Audio input status: {status}")
            # Convert float32 to int16 and get raw bytes
            audio_data = (indata[:, 0] * 32767).astype(DTYPE).tobytes()
            try:
                audio_queue.put_nowait({"data": audio_data, "mime_type": "audio/pcm"})
            except asyncio.QueueFull:
                pass  # Drop frames if queue is full
        
        # Start the input stream
        self.input_stream = sd.InputStream(
            samplerate=SEND_SAMPLE_RATE,
            channels=CHANNELS,
            dtype='float32',
            blocksize=CHUNK_SIZE,
            callback=audio_callback
        )
        
        self.input_stream.start()
        
        try:
            while True:
                audio_data = await audio_queue.get()
                await self.queue_realtime_input(audio_data)
        finally:
            if self.input_stream:
                self.input_stream.stop()
                self.input_stream.close()
                self.input_stream = None

    async def receive_audio(self):
        while True:
            turn = self.session.receive()
            async for response in turn:
                if data := response.data:
                    await self.handle_incoming_audio(response)
                    continue
                if text := response.text:
                    await self.handle_incoming_text(text)
                await self.handle_event(response)
            await self.on_turn_complete()

    async def handle_incoming_audio(self, response):
        if not self.enable_local_playback or not self.audio_in_queue:
            return
        try:
            self.audio_in_queue.put_nowait(response.data)
        except asyncio.QueueFull:
            while not self.audio_in_queue.empty():
                self.audio_in_queue.get_nowait()
            try:
                self.audio_in_queue.put_nowait(response.data)
            except asyncio.QueueFull:
                pass

    async def handle_incoming_text(self, text: str):
        print(text, end="")

    async def handle_event(self, response):
        model_turn = getattr(response, "model_turn", None)
        if model_turn and getattr(model_turn, "turn_complete", False):
            await self.on_turn_complete()

    async def on_turn_complete(self):
        if not self.enable_local_playback or not self.audio_in_queue:
            return
        while not self.audio_in_queue.empty():
            self.audio_in_queue.get_nowait()

    async def play_audio(self):
        # Create a queue for output audio data
        output_queue = asyncio.Queue()
        
        def audio_callback(outdata, frames, time, status):
            if status:
                print(f"Audio output status: {status}")
            try:
                # Get audio data from queue (non-blocking)
                audio_data = output_queue.get_nowait()
                # Convert bytes to numpy array and reshape
                audio_array = np.frombuffer(audio_data, dtype=DTYPE).astype(np.float32) / 32767.0
                # Ensure we have the right number of frames
                if len(audio_array) >= frames:
                    outdata[:, 0] = audio_array[:frames]
                else:
                    # Pad with zeros if not enough data
                    outdata[:len(audio_array), 0] = audio_array
                    outdata[len(audio_array):, 0] = 0
            except asyncio.QueueEmpty:
                # No audio data available, output silence
                outdata[:, 0] = 0
        
        # Start the output stream
        self.output_stream = sd.OutputStream(
            samplerate=RECEIVE_SAMPLE_RATE,
            channels=CHANNELS,
            dtype='float32',
            blocksize=CHUNK_SIZE,
            callback=audio_callback
        )
        
        self.output_stream.start()
        
        try:
            while True:
                bytestream = await self.audio_in_queue.get()
                await output_queue.put(bytestream)
        finally:
            if self.output_stream:
                self.output_stream.stop()
                self.output_stream.close()
            if self.output_stream:
                self.output_stream = None

    async def shutdown(self):
        if self._shutdown_future and not self._shutdown_future.done():
            self._shutdown_future.set_result(None)
        if not self.enable_cli_text and self.text_queue:
            await self.text_queue.put(None)
        await self._close_streams()

    async def _close_streams(self):
        if self.input_stream:
            self.input_stream.stop()
            self.input_stream.close()
            self.input_stream = None
        if self.output_stream:
            self.output_stream.stop()
            self.output_stream.close()
            self.output_stream = None

    async def run(self):
        try:
            async with (
                client.aio.live.connect(model=MODEL, config=CONFIG) as session,
                asyncio.TaskGroup() as tg,
            ):
                self.session = session
                loop = asyncio.get_running_loop()
                self._loop = loop
                self.audio_in_queue = asyncio.Queue(maxsize=50)
                self.out_queue = asyncio.Queue(maxsize=10)
                if not self.enable_cli_text:
                    self.text_queue = asyncio.Queue()
                else:
                    self.text_queue = None
                self._shutdown_future = loop.create_future()

                if self.enable_cli_text or self.text_queue is not None:
                    tg.create_task(self.send_text())

                tg.create_task(self.send_realtime())

                if not self.external_media:
                    tg.create_task(self.listen_audio())
                    if self.video_mode == "camera":
                        tg.create_task(self.get_frames())
                    elif self.video_mode == "screen":
                        tg.create_task(self.get_screen())

                tg.create_task(self.receive_audio())

                if self.enable_local_playback:
                    tg.create_task(self.play_audio())

                await self._shutdown_future
                raise asyncio.CancelledError("Shutdown requested")

        except asyncio.CancelledError:
            pass
        except ExceptionGroup as EG:
            traceback.print_exception(EG)
        finally:
            await self._close_streams()


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--mode",
        type=str,
        default=DEFAULT_MODE,
        help="pixels to stream from",
        choices=["camera", "screen", "none"],
    )
    args = parser.parse_args()
    main = AudioLoop(video_mode=args.mode)
    asyncio.run(main.run())

```

```python
"""
Web frontend for the Gemini AI Audio/Video streaming application
"""

import asyncio
import base64
import threading

from flask import Flask, render_template, request
from flask_socketio import SocketIO, emit
from dotenv import load_dotenv

from main import AudioLoop, RECEIVE_SAMPLE_RATE, SEND_SAMPLE_RATE

# Load environment variables from .env file
load_dotenv()

app = Flask(__name__)
app.config['SECRET_KEY'] = 'your-secret-key-here'
socketio = SocketIO(app, cors_allowed_origins="*")

# Global variables
active_loops: dict[str, "LoopController"] = {}

@app.route('/')
def index():
    return render_template('index.html')

@socketio.on('connect')
def handle_connect():
    print('Client connected')
    emit('status', {'message': 'Connected to server'})

@socketio.on('disconnect')
def handle_disconnect():
    sid = request.sid
    print(f'Client disconnected: {sid}')
    loop_entry = active_loops.pop(sid, None)
    if loop_entry:
        loop_entry.stop()

@socketio.on('start_streaming')
def handle_start_streaming(data):
    sid = request.sid

    if sid in active_loops:
        emit('error', {'message': 'Already streaming'})
        return
    
    mode = data.get('mode', 'camera')
    
    controller = LoopController(mode=mode, sid=sid)
    controller.start()
    active_loops[sid] = controller
    emit('status', {'message': f'Started streaming with {mode} mode'})

@socketio.on('stop_streaming')
def handle_stop_streaming():
    sid = request.sid
    controller = active_loops.pop(sid, None)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return
    
    try:
        controller.stop()
        emit('status', {'message': 'Stopped streaming'})
        
    except Exception as e:
        emit('error', {'message': f'Failed to stop streaming: {str(e)}'})

@socketio.on('send_message')
def handle_send_message(data):
    sid = request.sid
    controller = active_loops.get(sid)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return
    
    message = data.get('message', '')
    if message:
        # Add message to the audio loop's queue
        try:
            controller.enqueue_text(message)
            emit('status', {'message': f'Sent message: {message}'})
        except Exception as e:
            emit('error', {'message': f'Failed to send message: {str(e)}'})


@socketio.on('video_frame')
def handle_video_frame(data):
    sid = request.sid
    controller = active_loops.get(sid)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return

    frame_data = data.get('data')
    mime_type = data.get('mime_type', 'image/jpeg')
    if not frame_data:
        emit('error', {'message': 'Missing frame data'})
        return

    try:
        controller.enqueue_frame(mime_type, frame_data)
    except Exception as exc:
        emit('error', {'message': f'Failed to queue frame: {exc}'})


@socketio.on('audio_chunk')
def handle_audio_chunk(data):
    sid = request.sid
    controller = active_loops.get(sid)
    if not controller:
        emit('error', {'message': 'Not currently streaming'})
        return

    chunk = data.get('data')
    sample_rate = data.get('sampleRate', SEND_SAMPLE_RATE)
    if not chunk:
        emit('error', {'message': 'Missing audio data'})
        return

    try:
        controller.enqueue_audio(chunk, sample_rate)
    except Exception as exc:
        emit('error', {'message': f'Failed to queue audio: {exc}'})


class LoopController:
    """Thread-safe controller bridging Flask events to AudioLoop."""

    def __init__(self, mode: str, sid: str):
        self.mode = mode
        self.sid = sid
        self.thread = None
        self.loop = None
        self.audio_loop: WebAudioLoop | None = None
        self._ready = threading.Event()
        self._error: Exception | None = None

    def start(self):
        def runner():
            try:
                asyncio.run(self._run())
            except Exception as exc:  # pragma: no cover - surfaced via _error
                self._error = exc
                self._ready.set()

        self.thread = threading.Thread(target=runner, daemon=True)
        self.thread.start()
        if not self._ready.wait(timeout=5):
            raise RuntimeError("Timed out waiting for streaming loop to start")
        if self._error:
            raise self._error

    def stop(self):
        if self.audio_loop and self.loop:
            asyncio.run_coroutine_threadsafe(self.audio_loop.shutdown(), self.loop)
        if self.thread:
            self.thread.join(timeout=2)

    def enqueue_text(self, message: str):
        self._ensure_ready()
        if not self.audio_loop or not self.audio_loop.text_queue:
            raise RuntimeError("Text queue unavailable")
        asyncio.run_coroutine_threadsafe(
            self.audio_loop.queue_text(message),
            self.loop,
        )

    def enqueue_frame(self, mime_type: str, base64_data: str):
        self._ensure_ready()
        payload = {"mime_type": mime_type, "data": base64_data}
        asyncio.run_coroutine_threadsafe(
            self.audio_loop.queue_realtime_input(payload),
            self.loop,
        )

    def enqueue_audio(self, base64_data: str, sample_rate: int):  # noqa: ARG002 sample_rate kept for logging if needed
        self._ensure_ready()
        audio_bytes = base64.b64decode(base64_data)
        payload = {"data": audio_bytes, "mime_type": "audio/pcm"}
        asyncio.run_coroutine_threadsafe(
            self.audio_loop.queue_realtime_input(payload),
            self.loop,
        )

    def _ensure_ready(self):
        if not self._ready.is_set():
            raise RuntimeError("Loop is not ready yet")

    async def _run(self):
        loop = asyncio.get_running_loop()
        self.loop = loop
        self.audio_loop = WebAudioLoop(mode=self.mode, socket_sid=self.sid)
        self._ready.set()
        await self.audio_loop.run()


class WebAudioLoop(AudioLoop):
    """AudioLoop variant that bridges Gemini Live to Socket.IO clients."""

    def __init__(self, mode="camera", *, socket_sid: str):
        super().__init__(
            video_mode=mode,
            external_media=True,
            enable_local_playback=False,
            enable_cli_text=False,
        )
        self.socket_sid = socket_sid

    async def send_text(self):
        while True:
            message = await self.text_queue.get()
            if message is None:
                break
            await self.session.send(input=message, end_of_turn=True)

    async def handle_incoming_text(self, text: str):
        await super().handle_incoming_text(text)
        socketio.emit('ai_response', {'text': text}, room=self.socket_sid)

    async def handle_incoming_audio(self, response):
        if response.data:
            socketio.emit(
                'audio_data',
                {
                    'data': base64.b64encode(response.data).decode(),
                    'sampleRate': RECEIVE_SAMPLE_RATE,
                },
                room=self.socket_sid,
            )

    async def handle_event(self, response):
        await super().handle_event(response)

    async def on_turn_complete(self):
        await super().on_turn_complete()


if __name__ == '__main__':
    print("Starting Gemini AI Web Interface...")
    print("Open your browser and go to http://localhost:8080")
    socketio.run(app, debug=True, host='0.0.0.0', port=8080, allow_unsafe_werkzeug=True)

```

```swift
import Foundation
import AVFoundation
import UIKit
import SocketIO

protocol VideoCaptureServiceDelegate: AnyObject {
    func videoCaptureService(_ service: VideoCaptureService, didProduceJPEG data: Data)
}

protocol AudioCaptureServiceDelegate: AnyObject {
    func audioCaptureService(_ service: AudioCaptureService, didProducePCMChunk data: Data)
}

final class VideoCaptureService: NSObject {
    weak var delegate: VideoCaptureServiceDelegate?

    private let session = AVCaptureSession()
    private let sessionQueue = DispatchQueue(label: "com.aeye.videoSessionQueue")
    private let videoOutput = AVCaptureVideoDataOutput()
    private let processingQueue = DispatchQueue(label: "com.aeye.videoProcessingQueue")
    private let ciContext = CIContext()
    private var lastFrameDate = Date.distantPast
    var frameInterval: TimeInterval = 1.0

    func start() throws {
        sessionQueue.sync {
            guard self.session.isRunning == false else { return }
            self.configureSessionIfNeeded()
            self.session.startRunning()
        }
    }

    func stop() {
        sessionQueue.sync {
            guard self.session.isRunning else { return }
            self.session.stopRunning()
        }
    }

    private func configureSessionIfNeeded() {
        session.beginConfiguration()
        session.sessionPreset = .vga640x480
        session.inputs.forEach { session.removeInput($0) }

        guard
            let device = AVCaptureDevice.default(.builtInWideAngleCamera, for: .video, position: .back),
            let input = try? AVCaptureDeviceInput(device: device),
            session.canAddInput(input)
        else {
            session.commitConfiguration()
            return
        }
        session.addInput(input)

        videoOutput.videoSettings = [kCVPixelBufferPixelFormatTypeKey as String: kCVPixelFormatType_32BGRA]
        videoOutput.alwaysDiscardsLateVideoFrames = true
        videoOutput.setSampleBufferDelegate(self, queue: processingQueue)

        if session.canAddOutput(videoOutput) {
            session.addOutput(videoOutput)
        }

        session.commitConfiguration()
    }
}

extension VideoCaptureService: AVCaptureVideoDataOutputSampleBufferDelegate {
    func captureOutput(
        _ output: AVCaptureOutput,
        didOutput sampleBuffer: CMSampleBuffer,
        from connection: AVCaptureConnection
    ) {
        let now = Date()
        guard now.timeIntervalSince(lastFrameDate) >= frameInterval else { return }
        lastFrameDate = now

        guard
            let pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer)
        else { return }

        var orientationTransform = CGAffineTransform.identity
        if UIDevice.current.orientation == .landscapeRight {
            orientationTransform = CGAffineTransform(rotationAngle: .pi)
        }

        let ciImage = CIImage(cvPixelBuffer: pixelBuffer).transformed(by: orientationTransform)
        guard let cgImage = ciContext.createCGImage(ciImage, from: ciImage.extent) else { return }
        let uiImage = UIImage(cgImage: cgImage)
        guard let resized = uiImage.scaled(toMaxDimension: 1024),
              let jpegData = resized.jpegData(compressionQuality: 0.6)
        else { return }
        delegate?.videoCaptureService(self, didProduceJPEG: jpegData)
    }
}

final class AudioCaptureService {
    weak var delegate: AudioCaptureServiceDelegate?

    private let engine = AVAudioEngine()
    private let processingQueue = DispatchQueue(label: "com.aeye.audioProcessingQueue")
    private let targetFormat = AVAudioFormat(commonFormat: .pcmFormatInt16, sampleRate: 16_000, channels: 1, interleaved: true)!
    private var converter: AVAudioConverter?
    private let chunkFrameCount = 320

    func start() throws {
        let inputNode = engine.inputNode
        let inputFormat = inputNode.outputFormat(forBus: 0)
        converter = AVAudioConverter(from: inputFormat, to: targetFormat)

        inputNode.removeTap(onBus: 0)
        inputNode.installTap(onBus: 0, bufferSize: 1024, format: inputFormat) { [weak self] buffer, _ in
            self?.process(buffer: buffer)
        }

        if engine.isRunning == false {
            try engine.start()
        }
    }

    func stop() {
        let inputNode = engine.inputNode
        inputNode.removeTap(onBus: 0)
        if engine.isRunning {
            engine.stop()
        }
    }

    private func process(buffer: AVAudioPCMBuffer) {
        guard let converter else { return }
        processingQueue.async { [weak self] in
            guard let self else { return }
            let ratio = self.targetFormat.sampleRate / buffer.format.sampleRate
            let capacity = UInt32(Double(buffer.frameLength) * ratio) + 1024
            guard let convertedBuffer = AVAudioPCMBuffer(pcmFormat: self.targetFormat, frameCapacity: capacity) else { return }

            var conversionError: NSError?
            converter.convert(to: convertedBuffer, error: &conversionError) { _, outStatus in
                outStatus.pointee = .haveData
                return buffer
            }

            if let conversionError {
                print("Audio conversion error: \(conversionError)")
                return
            }

            convertedBuffer.frameLength = convertedBuffer.frameCapacity
            guard let channelData = convertedBuffer.int16ChannelData else { return }
            let totalFrames = Int(convertedBuffer.frameLength)
            let samples = channelData[0]
            let bytesPerSample = MemoryLayout<Int16>.size

            var offset = 0
            while offset < totalFrames {
                let framesToCopy = min(chunkFrameCount, totalFrames - offset)
                let byteCount = framesToCopy * bytesPerSample
                let chunk = Data(bytes: samples.advanced(by: offset), count: byteCount)
                self.delegate?.audioCaptureService(self, didProducePCMChunk: chunk)
                offset += framesToCopy
            }
        }
    }
}

final class AudioPlaybackService {
    private let engine = AVAudioEngine()
    private let player = AVAudioPlayerNode()
    private let queue = DispatchQueue(label: "com.aeye.audioPlaybackQueue")
    private let playbackFormat = AVAudioFormat(commonFormat: .pcmFormatInt16, sampleRate: 24_000, channels: 1, interleaved: true)!

    init() {
        engine.attach(player)
        engine.connect(player, to: engine.mainMixerNode, format: playbackFormat)
    }

    func start() throws {
        if engine.isRunning == false {
            try engine.start()
        }
        if player.isPlaying == false {
            player.play()
        }
    }

    func stop() {
        queue.async {
            self.player.stop()
            self.engine.stop()
        }
    }

    func enqueue(pcmData: Data) {
        queue.async { [weak self] in
            guard let self else { return }
            let frameCount = AVAudioFrameCount(pcmData.count / MemoryLayout<Int16>.size)
            guard frameCount > 0,
                  let buffer = AVAudioPCMBuffer(pcmFormat: self.playbackFormat, frameCapacity: frameCount) else { return }
            buffer.frameLength = frameCount
            pcmData.withUnsafeBytes { pointer in
                guard let baseAddress = pointer.bindMemory(to: Int16.self).baseAddress,
                      let destination = buffer.int16ChannelData?.pointee else { return }
                destination.assign(from: baseAddress, count: Int(frameCount))
            }
            if self.engine.isRunning == false {
                try? self.engine.start()
            }
            if self.player.isPlaying == false {
                self.player.play()
            }
            self.player.scheduleBuffer(buffer, at: nil, options: [], completionHandler: nil)
        }
    }
}

final class StreamingViewModel: NSObject, ObservableObject {
    @Published private(set) var isStreaming = false
    @Published private(set) var connectionStatus = "Disconnected"
    @Published private(set) var statusMessage = ""
    @Published private(set) var aiResponse = ""
    @Published private(set) var lastError: String?

    private let serverURL: URL
    private let manager: SocketManager
    private let socket: SocketIOClient
    private let videoService = VideoCaptureService()
    private let audioService = AudioCaptureService()
    private let playbackService = AudioPlaybackService()
    private let emitQueue = DispatchQueue(label: "com.aeye.socketEmitQueue")
    private var socketReady = false
    private var permissionsGranted = false

    override init() {
        if let urlString = Bundle.main.object(forInfoDictionaryKey: "STREAM_SERVER_URL") as? String,
           let url = URL(string: urlString) {
            serverURL = url
        } else {
            serverURL = URL(string: "http://localhost:8080")!
        }
        manager = SocketManager(
            socketURL: serverURL,
            config: [.log(false), .compress, .reconnects(true), .forceWebsockets(true)]
        )
        socket = manager.defaultSocket
        super.init()
        configureSocketHandlers()
        videoService.delegate = self
        audioService.delegate = self
    }

    func toggleStreaming() {
        if isStreaming {
            stopStreaming()
        } else {
            Task { await startStreaming() }
        }
    }

    func send(message: String) {
        guard isStreaming else {
            updateStatus("You need to start streaming before sending a message")
            return
        }
        guard message.trimmingCharacters(in: .whitespacesAndNewlines).isEmpty == false else { return }
        emitQueue.async { [weak self] in
            self?.socket.emit("send_message", ["message": message])
        }
    }

    func stopStreaming() {
        guard isStreaming else { return }
        emitQueue.async { [weak self] in
            self?.socket.emit("stop_streaming")
            self?.socket.disconnect()
        }
        audioService.stop()
        videoService.stop()
        playbackService.stop()
        try? AVAudioSession.sharedInstance().setActive(false, options: .notifyOthersOnDeactivation)
        socketReady = false
        updateMain {
            self.isStreaming = false
            self.connectionStatus = "Disconnected"
            self.statusMessage = "Stopped"
        }
    }

    private func startStreaming() async {
        do {
            try await requestPermissionsIfNeeded()
            try configureAudioSession()
            try playbackService.start()
            try videoService.start()
            try audioService.start()
            socket.connect()
            updateMain {
                self.connectionStatus = "Connecting"
                self.statusMessage = "Connecting to server..."
                self.isStreaming = true
            }
        } catch {
            handle(error: error)
        }
    }

    private func configureSocketHandlers() {
        socket.on(clientEvent: .connect) { [weak self] _, _ in
            guard let self else { return }
            self.socketReady = true
            self.updateStatus("Connected to server")
            self.updateConnection("Connected")
            self.emitQueue.async {
                self.socket.emit("start_streaming", ["mode": "camera"])
            }
        }

        socket.on(clientEvent: .disconnect) { [weak self] data, _ in
            self?.socketReady = false
            self?.updateConnection("Disconnected")
            if let reason = data.first as? String {
                self?.updateStatus("Disconnected: \(reason)")
            }
        }

        socket.on(clientEvent: .error) { [weak self] data, _ in
            if let error = data.first {
                self?.handle(error: error)
            }
        }

        socket.on("status") { [weak self] data, _ in
            guard
                let dict = data.first as? [String: Any],
                let message = dict["message"] as? String
            else { return }
            self?.updateStatus(message)
        }

        socket.on("error") { [weak self] data, _ in
            if let dict = data.first as? [String: Any], let message = dict["message"] as? String {
                self?.handle(error: message)
            }
        }

        socket.on("ai_response") { [weak self] data, _ in
            guard
                let dict = data.first as? [String: Any],
                let text = dict["text"] as? String
            else { return }
            self?.updateMain {
                self?.aiResponse = text
            }
        }

        socket.on("audio_data") { [weak self] data, _ in
            guard
                let dict = data.first as? [String: Any],
                let encoded = dict["data"] as? String,
                let pcmData = Data(base64Encoded: encoded)
            else { return }
            self?.playbackService.enqueue(pcmData: pcmData)
        }
    }

    private func requestPermissionsIfNeeded() async throws {
        guard permissionsGranted == false else { return }
        let cameraGranted = await AVCaptureDevice.requestAccess(for: .video)
        if cameraGranted == false {
            throw StreamingError.cameraPermissionDenied
        }
        let audioGranted = await withCheckedContinuation { continuation in
            AVAudioSession.sharedInstance().requestRecordPermission { granted in
                continuation.resume(returning: granted)
            }
        }
        if audioGranted == false {
            throw StreamingError.microphonePermissionDenied
        }
        permissionsGranted = true
    }

    private func configureAudioSession() throws {
        let session = AVAudioSession.sharedInstance()
        try session.setCategory(.playAndRecord, mode: .voiceChat, options: [.defaultToSpeaker, .allowBluetooth])
        try session.setPreferredSampleRate(48_000)
        try session.setActive(true, options: .notifyOthersOnDeactivation)
    }

    private func updateStatus(_ message: String) {
        updateMain {
            self.statusMessage = message
        }
    }

    private func updateConnection(_ state: String) {
        updateMain {
            self.connectionStatus = state
        }
    }

    private func handle(error: Any) {
        updateMain {
            if let swiftError = error as? Error {
                self.lastError = swiftError.localizedDescription
            } else if let message = error as? String {
                self.lastError = message
            } else {
                self.lastError = "Unknown error"
            }
            self.statusMessage = self.lastError ?? ""
        }
    }

    private func updateMain(_ block: @escaping () -> Void) {
        if Thread.isMainThread {
            block()
        } else {
            DispatchQueue.main.async(execute: block)
        }
    }
}

extension StreamingViewModel: VideoCaptureServiceDelegate {
    func videoCaptureService(_ service: VideoCaptureService, didProduceJPEG data: Data) {
        guard socketReady else { return }
        let encoded = data.base64EncodedString()
        emitQueue.async { [weak self] in
            self?.socket.emit("video_frame", ["mime_type": "image/jpeg", "data": encoded])
        }
    }
}

extension StreamingViewModel: AudioCaptureServiceDelegate {
    func audioCaptureService(_ service: AudioCaptureService, didProducePCMChunk data: Data) {
        guard socketReady else { return }
        let encoded = data.base64EncodedString()
        emitQueue.async { [weak self] in
            self?.socket.emit("audio_chunk", ["data": encoded, "sampleRate": 16_000])
        }
    }
}

enum StreamingError: LocalizedError {
    case cameraPermissionDenied
    case microphonePermissionDenied

    var errorDescription: String? {
        switch self {
        case .cameraPermissionDenied:
            return "Camera access is required to stream video."
        case .microphonePermissionDenied:
            return "Microphone access is required to send audio."
        }
    }
}

private extension UIImage {
    func scaled(toMaxDimension dimension: CGFloat) -> UIImage? {
        let maxSide = max(size.width, size.height)
        guard maxSide > dimension else { return self }
        let scale = dimension / maxSide
        let newSize = CGSize(width: size.width * scale, height: size.height * scale)
        UIGraphicsBeginImageContextWithOptions(newSize, false, 1.0)
        defer { UIGraphicsEndImageContext() }
        draw(in: CGRect(origin: .zero, size: newSize))
        return UIGraphicsGetImageFromCurrentImageContext()
    }
}
```

```swift
//
//  ContentView.swift
//  A-EYE
//
//  Created by Eirik Vavik on 08/10/2025.
//

import SwiftUI

struct ContentView: View {
    @StateObject private var viewModel: StreamingViewModel
    @State private var messageText: String = ""
    @State private var showingError = false

    init(viewModel: StreamingViewModel = StreamingViewModel()) {
        _viewModel = StateObject(wrappedValue: viewModel)
    }

    var body: some View {
        NavigationStack {
            VStack(alignment: .leading, spacing: 16) {
                statusSection
                responseSection
                inputSection
                Spacer()
                controlButton
            }
            .padding()
            .navigationTitle("A-EYE Assistant")
        }
        .alert("Error", isPresented: $showingError, presenting: viewModel.lastError) { _ in
            Button("OK", role: .cancel) { showingError = false }
        } message: { error in
            Text(error)
        }
        .onChange(of: viewModel.lastError) { _, newValue in
            showingError = newValue != nil
        }
    }

    private var statusSection: some View {
        VStack(alignment: .leading, spacing: 8) {
            Text("Connection: \(viewModel.connectionStatus)")
                .font(.headline)
            if viewModel.statusMessage.isEmpty == false {
                Text(viewModel.statusMessage)
                    .font(.subheadline)
                    .foregroundStyle(.secondary)
            }
        }
    }

    private var responseSection: some View {
        VStack(alignment: .leading, spacing: 8) {
            Text("Assistant")
                .font(.headline)
            ScrollView {
                Text(viewModel.aiResponse.isEmpty ? "No response yet." : viewModel.aiResponse)
                    .frame(maxWidth: .infinity, alignment: .leading)
                    .padding()
                    .background(.ultraThinMaterial, in: RoundedRectangle(cornerRadius: 12, style: .continuous))
            }
            .frame(minHeight: 160, idealHeight: 220)
        }
    }

    private var inputSection: some View {
        VStack(alignment: .leading, spacing: 8) {
            Text("Ask a question")
                .font(.headline)
            HStack {
                TextField("Where is the gluten free bread?", text: $messageText)
                    .textFieldStyle(.roundedBorder)
                    .disabled(viewModel.isStreaming == false)
                Button("Send") {
                    sendMessage()
                }
                .disabled(viewModel.isStreaming == false || messageText.trimmingCharacters(in: .whitespacesAndNewlines).isEmpty)
            }
        }
    }

    private var controlButton: some View {
        Button {
            viewModel.toggleStreaming()
        } label: {
            Text(viewModel.isStreaming ? "Stop Streaming" : "Start Streaming")
                .font(.title3.weight(.semibold))
                .frame(maxWidth: .infinity)
                .padding()
                .background(viewModel.isStreaming ? Color.red : Color.accentColor, in: Capsule())
                .foregroundStyle(.white)
        }
    }

    private func sendMessage() {
        let trimmed = messageText.trimmingCharacters(in: .whitespacesAndNewlines)
        guard trimmed.isEmpty == false else { return }
        viewModel.send(message: trimmed)
        messageText = ""
    }
}

#Preview {
    ContentView()
}

```

```plaintext
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
    <key>NSCameraUsageDescription</key>
    <string>This app needs camera access to understand what you are looking at.</string>
    <key>NSMicrophoneUsageDescription</key>
    <string>This app needs microphone access to send your voice to the assistant.</string>
    <key>NSAppTransportSecurity</key>
    <dict>
        <key>NSAllowsArbitraryLoads</key>
        <true/>
    </dict>
</dict>
</plist>
```

```plaintext
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
    <key>NSCameraUsageDescription</key>
    <string>Camera access lets the assistant understand what you point at.</string>
    <key>NSMicrophoneUsageDescription</key>
    <string>Microphone access lets the assistant hear your questions.</string>
    <key>NSAppTransportSecurity</key>
    <dict>
        <key>NSAllowsArbitraryLoads</key>
        <true/>
    </dict>
</dict>
</plist>

```

```python
"""
## Documentation
Quickstart: https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI.py

## Setup

To install the dependencies for this script, run:

```
pip install google-genai opencv-python sounddevice pillow mss python-dotenv numpy

Important: **Use headphones**. This script uses the system default audio
input and output, which often won't include echo cancellation. So to prevent
the model from interrupting itself it is important that you use headphones. 

```
To run the script:
```
python main.py
```

The script takes a video-mode flag `--mode`, this can be "camera", "screen", or "none".
The default is "camera". To share your screen run:

```
python Get_started_LiveAPI.py --mode screen
```
"""

import os
import asyncio
import base64
import io
import traceback
from typing import Optional
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

import cv2
import sounddevice as sd
import numpy as np
import PIL.Image
import mss

import argparse

from google import genai
from google.genai import types

CHANNELS = 1
SEND_SAMPLE_RATE = 16000
RECEIVE_SAMPLE_RATE = 24000
CHUNK_SIZE = 1024
DTYPE = np.int16

MODEL = "models/gemini-2.5-flash-preview-native-audio-dialog"

DEFAULT_MODE = "camera"

client = genai.Client(
    http_options={"api_version": "v1beta"},
    api_key=os.environ.get("GEMINI_API_KEY"),
)


CONFIG = types.LiveConnectConfig(
    response_modalities=[
        "AUDIO",
    ],
    media_resolution="MEDIA_RESOLUTION_MEDIUM",
    speech_config=types.SpeechConfig(
        voice_config=types.VoiceConfig(
            prebuilt_voice_config=types.PrebuiltVoiceConfig(voice_name="Zephyr")
        )
    ),
    context_window_compression=types.ContextWindowCompressionConfig(
        trigger_tokens=25600,
        sliding_window=types.SlidingWindow(target_tokens=12800),
    ),
)

# Audio device will be selected in the AudioLoop class


class AudioLoop:
    def __init__(
        self,
        video_mode=DEFAULT_MODE,
        *,
        external_media: bool = False,
        enable_local_playback: bool = True,
        enable_cli_text: bool = True,
    ):
        self.video_mode = video_mode
        self.external_media = external_media
        self.enable_local_playback = enable_local_playback
        self.enable_cli_text = enable_cli_text

        self.session: Optional[genai.live.LiveConnectSession] = None

        self.audio_in_queue: Optional[asyncio.Queue] = None
        self.out_queue: Optional[asyncio.Queue] = None
        self.text_queue: Optional[asyncio.Queue] = None

        self.input_stream = None
        self.output_stream = None

        self._shutdown_future: Optional[asyncio.Future] = None

    async def send_text(self):
        if self.enable_cli_text:
            while True:
                text = await asyncio.to_thread(
                    input,
                    "message > ",
                )
                if text.lower() == "q":
                    await self.shutdown()
                    break
                await self.session.send(input=text or ".", end_of_turn=True)
        else:
            while True:
                message = await self.text_queue.get()
                if message is None:
                    break
                await self.session.send(input=message, end_of_turn=True)

    def _get_frame(self, cap):
        # Read the frameq
        ret, frame = cap.read()
        # Check if the frame was read successfully
        if not ret:
            return None
        # Fix: Convert BGR to RGB color space
        # OpenCV captures in BGR but PIL expects RGB format
        # This prevents the blue tint in the video feed
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        img = PIL.Image.fromarray(frame_rgb)  # Now using RGB frame
        img.thumbnail([1024, 1024])

        image_io = io.BytesIO()
        img.save(image_io, format="jpeg")
        image_io.seek(0)

        mime_type = "image/jpeg"
        image_bytes = image_io.read()
        return {"mime_type": mime_type, "data": base64.b64encode(image_bytes).decode()}

    async def get_frames(self):
        # This takes about a second, and will block the whole program
        # causing the audio pipeline to overflow if you don't to_thread it.
        cap = await asyncio.to_thread(
            cv2.VideoCapture, 0
        )  # 0 represents the default camera

        while True:
            frame = await asyncio.to_thread(self._get_frame, cap)
            if frame is None:
                break

            await asyncio.sleep(1.0)

            await self.out_queue.put(frame)

        # Release the VideoCapture object
        cap.release()

    def _get_screen(self):
        sct = mss.mss()
        monitor = sct.monitors[0]

        i = sct.grab(monitor)

        mime_type = "image/jpeg"
        image_bytes = mss.tools.to_png(i.rgb, i.size)
        img = PIL.Image.open(io.BytesIO(image_bytes))

        image_io = io.BytesIO()
        img.save(image_io, format="jpeg")
        image_io.seek(0)

        image_bytes = image_io.read()
        return {"mime_type": mime_type, "data": base64.b64encode(image_bytes).decode()}

    async def get_screen(self):

        while True:
            frame = await asyncio.to_thread(self._get_screen)
            if frame is None:
                break

            await asyncio.sleep(1.0)

            await self.out_queue.put(frame)

    async def send_realtime(self):
        while True:
            msg = await self.out_queue.get()
            await self.session.send(input=msg)

    async def queue_realtime_input(self, payload):
        if not self.out_queue:
            raise RuntimeError("Realtime queue not ready")
        if self.out_queue.full():
            try:
                self.out_queue.get_nowait()
            except asyncio.QueueEmpty:
                pass
        await self.out_queue.put(payload)

    async def queue_text(self, message: str, *, end_of_turn: bool = True):
        if self.enable_cli_text:
            raise RuntimeError("Text queue not available in CLI mode")
        if not self.text_queue:
            raise RuntimeError("Text queue not ready")
        if end_of_turn:
            await self.text_queue.put(message)
        else:
            await self.session.send(input=message, end_of_turn=False)

    async def listen_audio(self):
        # Create a queue for audio data
        audio_queue = asyncio.Queue()
        
        def audio_callback(indata, frames, time, status):
            if status:
                print(f"Audio input status: {status}")
            # Convert float32 to int16 and get raw bytes
            audio_data = (indata[:, 0] * 32767).astype(DTYPE).tobytes()
            try:
                audio_queue.put_nowait({"data": audio_data, "mime_type": "audio/pcm"})
            except asyncio.QueueFull:
                pass  # Drop frames if queue is full
        
        # Start the input stream
        self.input_stream = sd.InputStream(
            samplerate=SEND_SAMPLE_RATE,
            channels=CHANNELS,
            dtype='float32',
            blocksize=CHUNK_SIZE,
            callback=audio_callback
        )
        
        self.input_stream.start()
        
        try:
            while True:
                audio_data = await audio_queue.get()
                await self.queue_realtime_input(audio_data)
        finally:
            if self.input_stream:
                self.input_stream.stop()
                self.input_stream.close()
                self.input_stream = None

    async def receive_audio(self):
        while True:
            turn = self.session.receive()
            async for response in turn:
                if data := response.data:
                    await self.handle_incoming_audio(response)
                    continue
                if text := response.text:
                    await self.handle_incoming_text(text)
                await self.handle_event(response)
            await self.on_turn_complete()

    async def handle_incoming_audio(self, response):
        if not self.enable_local_playback or not self.audio_in_queue:
            return
        try:
            self.audio_in_queue.put_nowait(response.data)
        except asyncio.QueueFull:
            while not self.audio_in_queue.empty():
                self.audio_in_queue.get_nowait()
            try:
                self.audio_in_queue.put_nowait(response.data)
            except asyncio.QueueFull:
                pass

    async def handle_incoming_text(self, text: str):
        print(text, end="")

    async def handle_event(self, response):
        model_turn = getattr(response, "model_turn", None)
        if model_turn and getattr(model_turn, "turn_complete", False):
            await self.on_turn_complete()

    async def on_turn_complete(self):
        if not self.enable_local_playback or not self.audio_in_queue:
            return
        while not self.audio_in_queue.empty():
            self.audio_in_queue.get_nowait()

    async def play_audio(self):
        # Create a queue for output audio data
        output_queue = asyncio.Queue()
        
        def audio_callback(outdata, frames, time, status):
            if status:
                print(f"Audio output status: {status}")
            try:
                # Get audio data from queue (non-blocking)
                audio_data = output_queue.get_nowait()
                # Convert bytes to numpy array and reshape
                audio_array = np.frombuffer(audio_data, dtype=DTYPE).astype(np.float32) / 32767.0
                # Ensure we have the right number of frames
                if len(audio_array) >= frames:
                    outdata[:, 0] = audio_array[:frames]
                else:
                    # Pad with zeros if not enough data
                    outdata[:len(audio_array), 0] = audio_array
                    outdata[len(audio_array):, 0] = 0
            except asyncio.QueueEmpty:
                # No audio data available, output silence
                outdata[:, 0] = 0
        
        # Start the output stream
        self.output_stream = sd.OutputStream(
            samplerate=RECEIVE_SAMPLE_RATE,
            channels=CHANNELS,
            dtype='float32',
            blocksize=CHUNK_SIZE,
            callback=audio_callback
        )
        
        self.output_stream.start()
        
        try:
            while True:
                bytestream = await self.audio_in_queue.get()
                await output_queue.put(bytestream)
        finally:
            if self.output_stream:
                self.output_stream.stop()
                self.output_stream.close()
            if self.output_stream:
                self.output_stream = None

    async def shutdown(self):
        if self._shutdown_future and not self._shutdown_future.done():
            self._shutdown_future.set_result(None)
        if not self.enable_cli_text and self.text_queue:
            await self.text_queue.put(None)
        await self._close_streams()

    async def _close_streams(self):
        if self.input_stream:
            self.input_stream.stop()
            self.input_stream.close()
            self.input_stream = None
        if self.output_stream:
            self.output_stream.stop()
            self.output_stream.close()
            self.output_stream = None

    async def run(self):
        try:
            async with (
                client.aio.live.connect(model=MODEL, config=CONFIG) as session,
                asyncio.TaskGroup() as tg,
            ):
                self.session = session
                loop = asyncio.get_running_loop()
                self._loop = loop
                self.audio_in_queue = asyncio.Queue(maxsize=50)
                self.out_queue = asyncio.Queue(maxsize=10)
                if not self.enable_cli_text:
                    self.text_queue = asyncio.Queue()
                else:
                    self.text_queue = None
                self._shutdown_future = loop.create_future()

                if self.enable_cli_text or self.text_queue is not None:
                    tg.create_task(self.send_text())

                tg.create_task(self.send_realtime())

                if not self.external_media:
                    tg.create_task(self.listen_audio())
                    if self.video_mode == "camera":
                        tg.create_task(self.get_frames())
                    elif self.video_mode == "screen":
                        tg.create_task(self.get_screen())

                tg.create_task(self.receive_audio())

                if self.enable_local_playback:
                    tg.create_task(self.play_audio())

                await self._shutdown_future
                raise asyncio.CancelledError("Shutdown requested")

        except asyncio.CancelledError:
            pass
        except ExceptionGroup as EG:
            traceback.print_exception(EG)
        finally:
            await self._close_streams()


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--mode",
        type=str,
        default=DEFAULT_MODE,
        help="pixels to stream from",
        choices=["camera", "screen", "none"],
    )
    args = parser.parse_args()
    main = AudioLoop(video_mode=args.mode)
    asyncio.run(main.run())

```

```python
"""
## Documentation
Quickstart: https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI.py

## Setup

To install the dependencies for this script, run:

```
pip install google-genai opencv-python sounddevice pillow mss python-dotenv numpy

Important: **Use headphones**. This script uses the system default audio
input and output, which often won't include echo cancellation. So to prevent
the model from interrupting itself it is important that you use headphones. 

```
To run the script:
```
python main.py
```

The script takes a video-mode flag `--mode`, this can be "camera", "screen", or "none".
The default is "camera". To share your screen run:

```
python Get_started_LiveAPI.py --mode screen
```
"""

import os
import asyncio
import base64
import traceback
from typing import Optional
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

import sounddevice as sd
import numpy as np

import argparse

from google import genai
from google.genai import types

CHANNELS = 1
SEND_SAMPLE_RATE = 16000
RECEIVE_SAMPLE_RATE = 24000
CHUNK_SIZE = 1024
DTYPE = np.int16

MODEL = "models/gemini-2.5-flash-preview-native-audio-dialog"

DEFAULT_MODE = "camera"

client = genai.Client(
    http_options={"api_version": "v1beta"},
    api_key=os.environ.get("GEMINI_API_KEY"),
)


CONFIG = types.LiveConnectConfig(
    response_modalities=[
        "AUDIO",
    ],
    media_resolution="MEDIA_RESOLUTION_MEDIUM",
    speech_config=types.SpeechConfig(
        voice_config=types.VoiceConfig(
            prebuilt_voice_config=types.PrebuiltVoiceConfig(voice_name="Zephyr")
        )
    ),
    context_window_compression=types.ContextWindowCompressionConfig(
        trigger_tokens=25600,
        sliding_window=types.SlidingWindow(target_tokens=12800),
    ),
)

# Audio device will be selected in the AudioLoop class


class AudioLoop:
    def __init__(
        self,
        video_mode=DEFAULT_MODE,
        *,
        external_media: bool = False,
        enable_local_playback: bool = True,
        enable_cli_text: bool = True,
    ):
        self.video_mode = video_mode
        self.external_media = external_media
        self.enable_local_playback = enable_local_playback
        self.enable_cli_text = enable_cli_text

        self.session: Optional[genai.live.LiveConnectSession] = None

        self.audio_in_queue: Optional[asyncio.Queue] = None
        self.out_queue: Optional[asyncio.Queue] = None
        self.text_queue: Optional[asyncio.Queue] = None

        self.input_stream = None
        self.output_stream = None

        self._shutdown_future: Optional[asyncio.Future] = None

    async def send_text(self):
        if self.enable_cli_text:
            while True:
                text = await asyncio.to_thread(
                    input,
                    "message > ",
                )
                if text.lower() == "q":
                    await self.shutdown()
                    break
                await self.session.send(input=text or ".", end_of_turn=True)
        else:
            while True:
                message = await self.text_queue.get()
                if message is None:
                    break
                await self.session.send(input=message, end_of_turn=True)

    def _get_frame(self, cap):
        # Read the frameq
        ret, frame = cap.read()
        # Check if the frame was read successfully
        if not ret:
            return None
        # Fix: Convert BGR to RGB color space
        # OpenCV captures in BGR but PIL expects RGB format
        # This prevents the blue tint in the video feed
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        img = PIL.Image.fromarray(frame_rgb)  # Now using RGB frame
        img.thumbnail([1024, 1024])

        image_io = io.BytesIO()
        img.save(image_io, format="jpeg")
        image_io.seek(0)

        mime_type = "image/jpeg"
        image_bytes = image_io.read()
        return {"mime_type": mime_type, "data": base64.b64encode(image_bytes).decode()}

    async def get_frames(self):
        # This takes about a second, and will block the whole program
        # causing the audio pipeline to overflow if you don't to_thread it.
        cap = await asyncio.to_thread(
            cv2.VideoCapture, 0
        )  # 0 represents the default camera

        while True:
            frame = await asyncio.to_thread(self._get_frame, cap)
            if frame is None:
                break

            await asyncio.sleep(1.0)

            await self.out_queue.put(frame)

        # Release the VideoCapture object
        cap.release()

    def _get_screen(self):
        sct = mss.mss()
        monitor = sct.monitors[0]

        i = sct.grab(monitor)

        mime_type = "image/jpeg"
        image_bytes = mss.tools.to_png(i.rgb, i.size)
        img = PIL.Image.open(io.BytesIO(image_bytes))

        image_io = io.BytesIO()
        img.save(image_io, format="jpeg")
        image_io.seek(0)

        image_bytes = image_io.read()
        return {"mime_type": mime_type, "data": base64.b64encode(image_bytes).decode()}

    async def get_screen(self):

        while True:
            frame = await asyncio.to_thread(self._get_screen)
            if frame is None:
                break

            await asyncio.sleep(1.0)

            await self.out_queue.put(frame)

    async def send_realtime(self):
        while True:
            msg = await self.out_queue.get()
            await self.session.send(input=msg)

    async def queue_realtime_input(self, payload):
        if not self.out_queue:
            raise RuntimeError("Realtime queue not ready")
        if self.out_queue.full():
            try:
                self.out_queue.get_nowait()
            except asyncio.QueueEmpty:
                pass
        await self.out_queue.put(payload)

    async def queue_text(self, message: str, *, end_of_turn: bool = True):
        if self.enable_cli_text:
            raise RuntimeError("Text queue not available in CLI mode")
        if not self.text_queue:
            raise RuntimeError("Text queue not ready")
        if end_of_turn:
            await self.text_queue.put(message)
        else:
            await self.session.send(input=message, end_of_turn=False)

    async def listen_audio(self):
        # Create a queue for audio data
        audio_queue = asyncio.Queue()
        
        def audio_callback(indata, frames, time, status):
            if status:
                print(f"Audio input status: {status}")
            # Convert float32 to int16 and get raw bytes
            audio_data = (indata[:, 0] * 32767).astype(DTYPE).tobytes()
            try:
                audio_queue.put_nowait({"data": audio_data, "mime_type": "audio/pcm"})
            except asyncio.QueueFull:
                pass  # Drop frames if queue is full
        
        # Start the input stream
        self.input_stream = sd.InputStream(
            samplerate=SEND_SAMPLE_RATE,
            channels=CHANNELS,
            dtype='float32',
            blocksize=CHUNK_SIZE,
            callback=audio_callback
        )
        
        self.input_stream.start()
        
        try:
            while True:
                audio_data = await audio_queue.get()
                await self.queue_realtime_input(audio_data)
        finally:
            if self.input_stream:
                self.input_stream.stop()
                self.input_stream.close()
                self.input_stream = None

    async def receive_audio(self):
        while True:
            turn = self.session.receive()
            async for response in turn:
                if data := response.data:
                    await self.handle_incoming_audio(response)
                    continue
                if text := response.text:
                    await self.handle_incoming_text(text)
                await self.handle_event(response)
            await self.on_turn_complete()

    async def handle_incoming_audio(self, response):
        if not self.enable_local_playback or not self.audio_in_queue:
            return
        try:
            self.audio_in_queue.put_nowait(response.data)
        except asyncio.QueueFull:
            while not self.audio_in_queue.empty():
                self.audio_in_queue.get_nowait()
            try:
                self.audio_in_queue.put_nowait(response.data)
            except asyncio.QueueFull:
                pass

    async def handle_incoming_text(self, text: str):
        print(text, end="")

    async def handle_event(self, response):
        model_turn = getattr(response, "model_turn", None)
        if model_turn and getattr(model_turn, "turn_complete", False):
            await self.on_turn_complete()

    async def on_turn_complete(self):
        if not self.enable_local_playback or not self.audio_in_queue:
            return
        while not self.audio_in_queue.empty():
            self.audio_in_queue.get_nowait()

    async def play_audio(self):
        # Create a queue for output audio data
        output_queue = asyncio.Queue()
        
        def audio_callback(outdata, frames, time, status):
            if status:
                print(f"Audio output status: {status}")
            try:
                # Get audio data from queue (non-blocking)
                audio_data = output_queue.get_nowait()
                # Convert bytes to numpy array and reshape
                audio_array = np.frombuffer(audio_data, dtype=DTYPE).astype(np.float32) / 32767.0
                # Ensure we have the right number of frames
                if len(audio_array) >= frames:
                    outdata[:, 0] = audio_array[:frames]
                else:
                    # Pad with zeros if not enough data
                    outdata[:len(audio_array), 0] = audio_array
                    outdata[len(audio_array):, 0] = 0
            except asyncio.QueueEmpty:
                # No audio data available, output silence
                outdata[:, 0] = 0
        
        # Start the output stream
        self.output_stream = sd.OutputStream(
            samplerate=RECEIVE_SAMPLE_RATE,
            channels=CHANNELS,
            dtype='float32',
            blocksize=CHUNK_SIZE,
            callback=audio_callback
        )
        
        self.output_stream.start()
        
        try:
            while True:
                bytestream = await self.audio_in_queue.get()
                await output_queue.put(bytestream)
        finally:
            if self.output_stream:
                self.output_stream.stop()
                self.output_stream.close()
            if self.output_stream:
                self.output_stream = None

    async def shutdown(self):
        if self._shutdown_future and not self._shutdown_future.done():
            self._shutdown_future.set_result(None)
        if not self.enable_cli_text and self.text_queue:
            await self.text_queue.put(None)
        await self._close_streams()

    async def _close_streams(self):
        if self.input_stream:
            self.input_stream.stop()
            self.input_stream.close()
            self.input_stream = None
        if self.output_stream:
            self.output_stream.stop()
            self.output_stream.close()
            self.output_stream = None

    async def run(self):
        try:
            async with (
                client.aio.live.connect(model=MODEL, config=CONFIG) as session,
                asyncio.TaskGroup() as tg,
            ):
                self.session = session
                loop = asyncio.get_running_loop()
                self._loop = loop
                self.audio_in_queue = asyncio.Queue(maxsize=50)
                self.out_queue = asyncio.Queue(maxsize=10)
                if not self.enable_cli_text:
                    self.text_queue = asyncio.Queue()
                else:
                    self.text_queue = None
                self._shutdown_future = loop.create_future()

                if self.enable_cli_text or self.text_queue is not None:
                    tg.create_task(self.send_text())

                tg.create_task(self.send_realtime())

                if not self.external_media:
                    tg.create_task(self.listen_audio())
                    if self.video_mode == "camera":
                        tg.create_task(self.get_frames())
                    elif self.video_mode == "screen":
                        tg.create_task(self.get_screen())

                tg.create_task(self.receive_audio())

                if self.enable_local_playback:
                    tg.create_task(self.play_audio())

                await self._shutdown_future
                raise asyncio.CancelledError("Shutdown requested")

        except asyncio.CancelledError:
            pass
        except ExceptionGroup as EG:
            traceback.print_exception(EG)
        finally:
            await self._close_streams()


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--mode",
        type=str,
        default=DEFAULT_MODE,
        help="pixels to stream from",
        choices=["camera", "screen", "none"],
    )
    args = parser.parse_args()
    main = AudioLoop(video_mode=args.mode)
    asyncio.run(main.run())

```

```python
"""
## Documentation
Quickstart: https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI.py

## Setup

To install the dependencies for this script, run:

```
pip install google-genai opencv-python sounddevice pillow mss python-dotenv numpy

Important: **Use headphones**. This script uses the system default audio
input and output, which often won't include echo cancellation. So to prevent
the model from interrupting itself it is important that you use headphones. 

```
To run the script:
```
python main.py
```

The script takes a video-mode flag `--mode`, this can be "camera", "screen", or "none".
The default is "camera". To share your screen run:

```
python Get_started_LiveAPI.py --mode screen
```
"""

import os
import asyncio
import base64
import io
import traceback
from typing import Optional
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

import cv2
import sounddevice as sd
import numpy as np
import PIL.Image
import mss

import argparse

from google import genai
from google.genai import types

CHANNELS = 1
SEND_SAMPLE_RATE = 16000
RECEIVE_SAMPLE_RATE = 24000
CHUNK_SIZE = 1024
DTYPE = np.int16

MODEL = "models/gemini-2.5-flash-preview-native-audio-dialog"

DEFAULT_MODE = "camera"

client = genai.Client(
    http_options={"api_version": "v1beta"},
    api_key=os.environ.get("GEMINI_API_KEY"),
)


CONFIG = types.LiveConnectConfig(
    response_modalities=[
        "AUDIO",
    ],
    media_resolution="MEDIA_RESOLUTION_MEDIUM",
    speech_config=types.SpeechConfig(
        voice_config=types.VoiceConfig(
            prebuilt_voice_config=types.PrebuiltVoiceConfig(voice_name="Zephyr")
        )
    ),
    context_window_compression=types.ContextWindowCompressionConfig(
        trigger_tokens=25600,
        sliding_window=types.SlidingWindow(target_tokens=12800),
    ),
)

# Audio device will be selected in the AudioLoop class


class AudioLoop:
    def __init__(
        self,
        video_mode=DEFAULT_MODE,
        *,
        external_media: bool = False,
        enable_local_playback: bool = True,
        enable_cli_text: bool = True,
    ):
        self.video_mode = video_mode
        self.external_media = external_media
        self.enable_local_playback = enable_local_playback
        self.enable_cli_text = enable_cli_text

        self.session: Optional[genai.live.LiveConnectSession] = None

        self.audio_in_queue: Optional[asyncio.Queue] = None
        self.out_queue: Optional[asyncio.Queue] = None
        self.text_queue: Optional[asyncio.Queue] = None

        self.input_stream = None
        self.output_stream = None

        self._shutdown_future: Optional[asyncio.Future] = None

    async def send_text(self):
        if self.enable_cli_text:
            while True:
                text = await asyncio.to_thread(
                    input,
                    "message > ",
                )
                if text.lower() == "q":
                    await self.shutdown()
                    break
                await self.session.send(input=text or ".", end_of_turn=True)
        else:
            while True:
                message = await self.text_queue.get()
                if message is None:
                    break
                await self.session.send(input=message, end_of_turn=True)

    def _get_frame(self, cap):
        # Read the frameq
        ret, frame = cap.read()
        # Check if the frame was read successfully
        if not ret:
            return None
        # Fix: Convert BGR to RGB color space
        # OpenCV captures in BGR but PIL expects RGB format
        # This prevents the blue tint in the video feed
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        img = PIL.Image.fromarray(frame_rgb)  # Now using RGB frame
        img.thumbnail([1024, 1024])

        image_io = io.BytesIO()
        img.save(image_io, format="jpeg")
        image_io.seek(0)

        mime_type = "image/jpeg"
        image_bytes = image_io.read()
        return {"mime_type": mime_type, "data": base64.b64encode(image_bytes).decode()}

    async def get_frames(self):
        # This takes about a second, and will block the whole program
        # causing the audio pipeline to overflow if you don't to_thread it.
        cap = await asyncio.to_thread(
            cv2.VideoCapture, 0
        )  # 0 represents the default camera

        while True:
            frame = await asyncio.to_thread(self._get_frame, cap)
            if frame is None:
                break

            await asyncio.sleep(1.0)

            await self.out_queue.put(frame)

        # Release the VideoCapture object
        cap.release()

    def _get_screen(self):
        sct = mss.mss()
        monitor = sct.monitors[0]

        i = sct.grab(monitor)

        mime_type = "image/jpeg"
        image_bytes = mss.tools.to_png(i.rgb, i.size)
        img = PIL.Image.open(io.BytesIO(image_bytes))

        image_io = io.BytesIO()
        img.save(image_io, format="jpeg")
        image_io.seek(0)

        image_bytes = image_io.read()
        return {"mime_type": mime_type, "data": base64.b64encode(image_bytes).decode()}

    async def get_screen(self):

        while True:
            frame = await asyncio.to_thread(self._get_screen)
            if frame is None:
                break

            await asyncio.sleep(1.0)

            await self.out_queue.put(frame)

    async def send_realtime(self):
        while True:
            msg = await self.out_queue.get()
            await self.session.send(input=msg)

    async def queue_realtime_input(self, payload):
        if not self.out_queue:
            raise RuntimeError("Realtime queue not ready")
        if self.out_queue.full():
            try:
                self.out_queue.get_nowait()
            except asyncio.QueueEmpty:
                pass
        await self.out_queue.put(payload)

    async def queue_text(self, message: str, *, end_of_turn: bool = True):
        if self.enable_cli_text:
            raise RuntimeError("Text queue not available in CLI mode")
        if not self.text_queue:
            raise RuntimeError("Text queue not ready")
        if end_of_turn:
            await self.text_queue.put(message)
        else:
            await self.session.send(input=message, end_of_turn=False)

    async def listen_audio(self):
        # Create a queue for audio data
        audio_queue = asyncio.Queue()
        
        def audio_callback(indata, frames, time, status):
            if status:
                print(f"Audio input status: {status}")
            # Convert float32 to int16 and get raw bytes
            audio_data = (indata[:, 0] * 32767).astype(DTYPE).tobytes()
            try:
                audio_queue.put_nowait({"data": audio_data, "mime_type": "audio/pcm"})
            except asyncio.QueueFull:
                pass  # Drop frames if queue is full
        
        # Start the input stream
        self.input_stream = sd.InputStream(
            samplerate=SEND_SAMPLE_RATE,
            channels=CHANNELS,
            dtype='float32',
            blocksize=CHUNK_SIZE,
            callback=audio_callback
        )
        
        self.input_stream.start()
        
        try:
            while True:
                audio_data = await audio_queue.get()
                await self.queue_realtime_input(audio_data)
        finally:
            if self.input_stream:
                self.input_stream.stop()
                self.input_stream.close()
                self.input_stream = None

    async def receive_audio(self):
        while True:
            turn = self.session.receive()
            async for response in turn:
                if data := response.data:
                    await self.handle_incoming_audio(response)
                    continue
                if text := response.text:
                    await self.handle_incoming_text(text)
                await self.handle_event(response)
            await self.on_turn_complete()

    async def handle_incoming_audio(self, response):
        if not self.enable_local_playback or not self.audio_in_queue:
            return
        try:
            self.audio_in_queue.put_nowait(response.data)
        except asyncio.QueueFull:
            while not self.audio_in_queue.empty():
                self.audio_in_queue.get_nowait()
            try:
                self.audio_in_queue.put_nowait(response.data)
            except asyncio.QueueFull:
                pass

    async def handle_incoming_text(self, text: str):
        print(text, end="")

    async def handle_event(self, response):
        model_turn = getattr(response, "model_turn", None)
        if model_turn and getattr(model_turn, "turn_complete", False):
            await self.on_turn_complete()

    async def on_turn_complete(self):
        if not self.enable_local_playback or not self.audio_in_queue:
            return
        while not self.audio_in_queue.empty():
            self.audio_in_queue.get_nowait()

    async def play_audio(self):
        # Create a queue for output audio data
        output_queue = asyncio.Queue()
        
        def audio_callback(outdata, frames, time, status):
            if status:
                print(f"Audio output status: {status}")
            try:
                # Get audio data from queue (non-blocking)
                audio_data = output_queue.get_nowait()
                # Convert bytes to numpy array and reshape
                audio_array = np.frombuffer(audio_data, dtype=DTYPE).astype(np.float32) / 32767.0
                # Ensure we have the right number of frames
                if len(audio_array) >= frames:
                    outdata[:, 0] = audio_array[:frames]
                else:
                    # Pad with zeros if not enough data
                    outdata[:len(audio_array), 0] = audio_array
                    outdata[len(audio_array):, 0] = 0
            except asyncio.QueueEmpty:
                # No audio data available, output silence
                outdata[:, 0] = 0
        
        # Start the output stream
        self.output_stream = sd.OutputStream(
            samplerate=RECEIVE_SAMPLE_RATE,
            channels=CHANNELS,
            dtype='float32',
            blocksize=CHUNK_SIZE,
            callback=audio_callback
        )
        
        self.output_stream.start()
        
        try:
            while True:
                bytestream = await self.audio_in_queue.get()
                await output_queue.put(bytestream)
        finally:
            if self.output_stream:
                self.output_stream.stop()
                self.output_stream.close()
            if self.output_stream:
                self.output_stream = None

    async def shutdown(self):
        if self._shutdown_future and not self._shutdown_future.done():
            self._shutdown_future.set_result(None)
        if not self.enable_cli_text and self.text_queue:
            await self.text_queue.put(None)
        await self._close_streams()

    async def _close_streams(self):
        if self.input_stream:
            self.input_stream.stop()
            self.input_stream.close()
            self.input_stream = None
        if self.output_stream:
            self.output_stream.stop()
            self.output_stream.close()
            self.output_stream = None

    async def run(self):
        try:
            async with (
                client.aio.live.connect(model=MODEL, config=CONFIG) as session,
                asyncio.TaskGroup() as tg,
            ):
                self.session = session
                loop = asyncio.get_running_loop()
                self._loop = loop
                self.audio_in_queue = asyncio.Queue(maxsize=50)
                self.out_queue = asyncio.Queue(maxsize=10)
                if not self.enable_cli_text:
                    self.text_queue = asyncio.Queue()
                else:
                    self.text_queue = None
                self._shutdown_future = loop.create_future()

                if self.enable_cli_text or self.text_queue is not None:
                    tg.create_task(self.send_text())

                tg.create_task(self.send_realtime())

                if not self.external_media:
                    tg.create_task(self.listen_audio())
                    if self.video_mode == "camera":
                        tg.create_task(self.get_frames())
                    elif self.video_mode == "screen":
                        tg.create_task(self.get_screen())

                tg.create_task(self.receive_audio())

                if self.enable_local_playback:
                    tg.create_task(self.play_audio())

                await self._shutdown_future
                raise asyncio.CancelledError("Shutdown requested")

        except asyncio.CancelledError:
            pass
        except ExceptionGroup as EG:
            traceback.print_exception(EG)
        finally:
            await self._close_streams()


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--mode",
        type=str,
        default=DEFAULT_MODE,
        help="pixels to stream from",
        choices=["camera", "screen", "none"],
    )
    args = parser.parse_args()
    main = AudioLoop(video_mode=args.mode)
    asyncio.run(main.run())

```

## Update Summary

- Reworked `WebInterface/main.py` so `AudioLoop` accepts external media, exposes queue helpers, and skips local capture when driven by a client. Added safe shutdown, optional typing, and resilient handling of Gemini Live responses.
- Replaced legacy Flask bridge with `LoopController`/`WebAudioLoop` in `WebInterface/app.py`: per-socket sessions, new `video_frame` and `audio_chunk` events, and Socket.IO emissions scoped to the originating client.
- Added Swift-side capture/playback pipeline (`StreamingViewModel.swift`) with AVCapture/AVAudioEngine, Socket.IO wiring, and simple SwiftUI UI updated in `ContentView.swift`.
- Declared camera/mic usage descriptions and temporary ATS allowance in `A-EYE/Info.plist`.
- Created an asset `Info.plist` override and supporting utilities (image scaling, error handling, etc.).

## Files Touched

- `WebInterface/main.py`
- `WebInterface/app.py`
- `A-EYE/StreamingViewModel.swift` (new)
- `A-EYE/ContentView.swift`
- `A-EYE/Info.plist`

## Next Steps

- Backend: `pip install -r WebInterface/requirements.txt`; run `python app.py` to verify Socket.IO events. Ensure TLS (ngrok/HTTPS proxy) before device testing.
- iOS: populate `STREAM_SERVER_URL` in Xcode build settings (Debug: `http://127.0.0.1:8080` for tunnel; Release: `wss://...`). Remove ATS exception before shipping.
- Testing: run Flask server, then launch the app on device; confirm mic/camera permissions, frame throttling, audio roundtrip. Clean up `Inject` package if unused.
- When ready, remove ATS exceptions and disable `NSAllowsArbitraryLoads` for production.

