# Adjust UI for visual sphere shader
_Exported on 15/11/2025 at 2:50:38 CET from Cursor (1.7.44)_

---

**User**

This is a project that lets the user experience real-time voice chat with an AI model that uses your back camera to look at things.

I want to focus ONLY on changing the UI for now:

Make the whole bottom half (the half that has the visual sphere shader) one much "shorter". I only want to have it cover 1/3 of the screen, not half. Also try and make the sphere shader bigger/zoomed in.

---

**Cursor**

Proposed plan (pseudocode-style):

```
1. Update layout split:
   - in index.tsx → static styles:
     • set #video-background height to roughly 67vh so it covers the upper two-thirds.
     • change .speak-area height to calc(33vh + env(safe-area-inset-bottom)) so the panel is only one-third tall.
     • ensure border radius/positioning stay untouched so the glass panel still hugs the bottom.

2. Keep interaction affordances intact:
   - verify the speak button still anchors to the bottom with existing padding.
   - confirm the video continues to cover the remaining space without gaps.

3. Enlarge/zoom sphere:
   - in visual-3d.ts → animation loop:
     • bump the base scale factor from 1 to ~1.35 before adding audio-driven growth.
     • shorten the camera’s orbit radius vector from length 5 to about 3.6 so the camera sits closer.
   - keep rotation/math logic unchanged so motion stays identical, just visually larger.

4. Quick sanity pass:
   - rebuild to ensure the layout change looks correct on both desktop and mobile viewport sizes.
```

Let me know if you’d like any tweaks before I implement this.

```plaintext
1. Update layout split:
   - in index.tsx → static styles:
     • set #video-background height to roughly 67vh so it covers the upper two-thirds.
     • change .speak-area height to calc(33vh + env(safe-area-inset-bottom)) so the panel is only one-third tall.
     • ensure border radius/positioning stay untouched so the glass panel still hugs the bottom.

2. Keep interaction affordances intact:
   - verify the speak button still anchors to the bottom with existing padding.
   - confirm the video continues to cover the remaining space without gaps.

3. Enlarge/zoom sphere:
   - in visual-3d.ts → animation loop:
     • bump the base scale factor from 1 to ~1.35 before adding audio-driven growth.
     • shorten the camera’s orbit radius vector from length 5 to about 3.6 so the camera sits closer.
   - keep rotation/math logic unchanged so motion stays identical, just visually larger.

4. Quick sanity pass:
   - rebuild to ensure the layout change looks correct on both desktop and mobile viewport sizes.
```

---

**User**

Proceed

---

**Cursor**

```typescriptreact
/* tslint:disable */
/**
 * @license
 * SPDX-License-Identifier: Apache-2.0
 */

import {GoogleGenAI, LiveServerMessage, Modality, Session} from '@google/genai';
import {LitElement, css, html} from 'lit';
import {customElement, state} from 'lit/decorators.js';
import {createBlob, decode, decodeAudioData} from './utils';
import './visual-3d';
// PWA: register service worker if available
try {
  // dynamic import optional; vite-plugin-pwa injects virtual module
  // eslint-disable-next-line @typescript-eslint/ban-ts-comment
  // @ts-ignore
  import('virtual:pwa-register').then(({ registerSW }) => {
    registerSW({ immediate: true });
  }).catch(() => {});
} catch {}

@customElement('gdm-live-audio')
export class GdmLiveAudio extends LitElement {
  @state() isRecording = false;
  @state() status = '';
  @state() error = '';

  private client: GoogleGenAI;
  private sessionPromise: Promise<Session>;
  // FIX: Cast window to `any` to access prefixed `webkitAudioContext` for older browsers.
  private inputAudioContext = new (window.AudioContext ||
    (window as any).webkitAudioContext)({sampleRate: 16000});
  // FIX: Cast window to `any` to access prefixed `webkitAudioContext` for older browsers.
  private outputAudioContext = new (window.AudioContext ||
    (window as any).webkitAudioContext)({sampleRate: 24000});
  @state() inputNode = this.inputAudioContext.createGain();
  @state() outputNode = this.outputAudioContext.createGain();
  private nextStartTime = 0;
  private mediaStream: MediaStream;
  private sourceNode: AudioNode;
  private scriptProcessorNode: ScriptProcessorNode;
  private sources = new Set<AudioBufferSourceNode>();
  private frameInterval: number;
  private videoElement: HTMLVideoElement;
  private canvasElement: HTMLCanvasElement;
  private framesSent = 0;

  static styles = css`
    :host {
      display: block;
      position: relative;
      width: 100vw;
      height: 100vh;
      min-height: 100dvh;
      background: #000000;
      color: #ffffff;
      overflow: hidden;
    }

    @supports (height: 100dvh) {
      :host {
        height: 100dvh;
      }
    }

    .layout {
      position: relative;
      width: 100%;
      height: 100%;
    }

    #video-background {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 66.667vh;
      object-fit: cover;
    }

    .speak-area {
      position: absolute;
      bottom: 0;
      left: 0;
      width: 100%;
      height: calc(33.333vh + env(safe-area-inset-bottom));
      border-top-left-radius: 32px;
      border-top-right-radius: 32px;
      border: 1px solid rgba(255, 255, 255, 0.08);
      border-bottom: none;
      overflow: hidden;
      backdrop-filter: blur(6px);
    }

    gdm-live-audio-visuals-3d {
      display: block;
      width: 100%;
      height: 100%;
      pointer-events: none;
      border-top-left-radius: inherit;
      border-top-right-radius: inherit;
    }

    .speak-button {
      position: absolute;
      inset: 0;
      width: 100%;
      height: 100%;
      border: none;
      background: transparent;
      cursor: pointer;
      display: flex;
      align-items: flex-end;
      justify-content: center;
      padding: 24px 24px calc(24px + env(safe-area-inset-bottom));
      color: inherit;
      transition: background 0.2s ease;
    }

    .speak-button:hover,
    .speak-button:focus-visible {
      background: rgba(255, 255, 255, 0.04);
    }

    .speak-button:focus-visible {
      outline: 2px solid rgba(255, 255, 255, 0.25);
      outline-offset: 4px;
    }

    .speak-label {
      font-size: 18px;
      letter-spacing: 0.08em;
      text-transform: uppercase;
      color: rgba(255, 255, 255, 0.92);
      text-shadow: 0 0 6px rgba(0, 0, 0, 0.6);
    }

    .error-banner {
      position: absolute;
      top: calc(env(safe-area-inset-top) + 12px);
      left: 16px;
      right: 16px;
      z-index: 10;
      padding: 12px 16px;
      background: rgba(220, 38, 38, 0.85);
      color: #ffffff;
      border-radius: 12px;
      text-align: center;
      font-size: 14px;
      line-height: 1.4;
      box-shadow: 0 12px 24px rgba(0, 0, 0, 0.35);
      backdrop-filter: blur(12px);
    }
  `;

  constructor() {
    super();
    this.initClient();
  }

  protected firstUpdated() {
    this.videoElement = this.shadowRoot!.querySelector('#video-background');
    this.canvasElement = this.shadowRoot!.querySelector('#frame-canvas');
    this.debug(
      `Boot: secure=${window.isSecureContext} ua=${navigator.userAgent}`,
    );
    this.checkPermissions();
    this.addEventListener('click', async () => {
      try {
        await this.outputAudioContext.resume();
        await this.inputAudioContext.resume();
      } catch {}
    });
  }

  private async checkPermissions() {
    const parts: string[] = [];
    const anyNav: any = navigator as any;
    if (anyNav.permissions?.query) {
      try {
        const mic = await anyNav.permissions.query({name: 'microphone'});
        parts.push(`mic=${mic.state}`);
      } catch {}
      try {
        const cam = await anyNav.permissions.query({name: 'camera'});
        parts.push(`cam=${cam.state}`);
      } catch {}
    }
    if (parts.length) this.updateStatus(`Permissions: ${parts.join(' ')}`);
  }

  private debug(_msg: string, _data?: unknown) {
    // Debug logging disabled for production UI clarity.
  }

  private initAudio() {
    this.nextStartTime = this.outputAudioContext.currentTime;
  }

  private async initClient() {
    this.initAudio();

    if (!process.env.API_KEY || process.env.API_KEY === 'undefined') {
      this.updateError('Missing GEMINI_API_KEY');
    }

    this.client = new GoogleGenAI({apiKey: process.env.API_KEY});

    this.outputNode.connect(this.outputAudioContext.destination);

    this.sessionPromise = this.initSession();
  }

  private initSession(): Promise<Session> {
    const model = 'gemini-2.5-flash-native-audio-preview-09-2025';

    try {
      return this.client.live.connect({
        model: model,
        callbacks: {
          onopen: () => {
            this.debug('Live session opened');
          },
          onmessage: async (message: LiveServerMessage) => {
            this.debug('Live message received');
            const audio =
              message.serverContent?.modelTurn?.parts[0]?.inlineData;

            if (audio) {
              this.nextStartTime = Math.max(
                this.nextStartTime,
                this.outputAudioContext.currentTime,
              );

              const audioBuffer = await decodeAudioData(
                decode(audio.data),
                this.outputAudioContext,
                24000,
                1,
              );
              const source = this.outputAudioContext.createBufferSource();
              source.buffer = audioBuffer;
              source.connect(this.outputNode);
              source.addEventListener('ended', () => {
                this.sources.delete(source);
              });

              source.start(this.nextStartTime);
              this.nextStartTime = this.nextStartTime + audioBuffer.duration;
              this.sources.add(source);
            }

            const interrupted = message.serverContent?.interrupted;
            if (interrupted) {
              for (const source of this.sources.values()) {
                source.stop();
                this.sources.delete(source);
              }
              this.nextStartTime = 0;
            }
          },
          onerror: (e: ErrorEvent) => {
            this.updateError('Live error: ' + e.message);
            console.error('Live error event', e);
          },
          onclose: (e: CloseEvent) => {
            this.updateStatus('Live close: ' + e.reason + ' (' + e.code + ')');
            console.warn('Live close', e);
          },
        },
        config: {
          responseModalities: [Modality.AUDIO],
          speechConfig: {
            voiceConfig: {prebuiltVoiceConfig: {voiceName: 'Zephyr'}},
            // languageCode: 'en-GB'
          },
        },
      });
    } catch (e) {
      console.error(e);
      return Promise.reject(e);
    }
  }

  private updateStatus(msg: string) {
    this.status = msg;
  }

  private updateError(msg: string) {
    this.error = msg;
  }

  private blobToBase64(blob: Blob): Promise<string> {
    return new Promise((resolve, reject) => {
      const reader = new FileReader();
      reader.onloadend = () => {
        if (typeof reader.result === 'string') {
          resolve(reader.result.split(',')[1]);
        } else {
          reject(new Error('Failed to read blob as base64 string.'));
        }
      };
      reader.onerror = reject;
      reader.readAsDataURL(blob);
    });
  }

  private async startRecording() {
    if (this.isRecording) {
      return;
    }

    await this.inputAudioContext.resume();
    await this.outputAudioContext.resume();

    this.updateStatus('Requesting camera/mic access...');

    try {
      const preferBackCamera = async (): Promise<MediaStream> => {
        const constraints: MediaStreamConstraints = {
          audio: {
            channelCount: 1,
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true,
          },
          video: {
            facingMode: { ideal: 'environment' },
          },
        };
        this.debug('getUserMedia with constraints', constraints);
        try {
          const stream = await navigator.mediaDevices.getUserMedia(constraints);
          const track = stream.getVideoTracks()[0];
          const settings = track?.getSettings?.() || {} as MediaTrackSettings;
          this.debug('Acquired stream video settings', settings);
          if (settings.facingMode !== 'environment') {
            // Try enumerate fallback to enforce back camera
            const devices = await navigator.mediaDevices.enumerateDevices();
            const back = devices.find(
              (d) => d.kind === 'videoinput' && /back|rear|environment/i.test(d.label),
            );
            if (back) {
              try {
                const enforced = await navigator.mediaDevices.getUserMedia({
                  audio: constraints.audio,
                  video: { deviceId: { exact: back.deviceId } },
                });
                this.debug('Enforced back camera via deviceId', back);
                stream.getTracks().forEach((t) => t.stop());
                return enforced;
              } catch {
                // fall through to original stream
              }
            }
          }
          return stream;
        } catch (e) {
          this.updateError('getUserMedia failed: ' + (e as Error).message);
          const devices = await navigator.mediaDevices.enumerateDevices();
          const back = devices.find(
            (d) => d.kind === 'videoinput' && /back|rear|environment/i.test(d.label),
          );
          if (back) {
            return await navigator.mediaDevices.getUserMedia({
              audio: constraints.audio,
              video: { deviceId: { exact: back.deviceId } },
            });
          }
          throw e;
        }
      };

      this.mediaStream = await preferBackCamera();

      this.videoElement.srcObject = this.mediaStream;
      try {
        await this.videoElement.play();
      } catch (e) {
        console.warn('video.play() failed', e);
      }

      const vt = this.mediaStream.getVideoTracks()[0];
      const at = this.mediaStream.getAudioTracks()[0];
      this.updateStatus('Media access granted. v=' + (!!vt) + ' a=' + (!!at));

      this.sourceNode = this.inputAudioContext.createMediaStreamSource(
        this.mediaStream,
      );
      this.sourceNode.connect(this.inputNode);

      const bufferSize = 4096;
      this.scriptProcessorNode = this.inputAudioContext.createScriptProcessor(
        bufferSize,
        1,
        1,
      );

      this.scriptProcessorNode.onaudioprocess = (audioProcessingEvent) => {
        if (!this.isRecording) return;

        const inputBuffer = audioProcessingEvent.inputBuffer;
        const pcmData = inputBuffer.getChannelData(0);
        const srcRate = this.inputAudioContext.sampleRate;
        const downsampleTo16k = (float32: Float32Array, inRate: number) => {
          if (inRate === 16000) return float32;
          const ratio = inRate / 16000;
          const newLen = Math.floor(float32.length / ratio);
          const result = new Float32Array(newLen);
          let idx = 0, pos = 0;
          while (idx < newLen) {
            const nextPos = Math.min(float32.length - 1, (idx + 1) * ratio);
            let sum = 0, count = 0;
            for (let i = pos; i < nextPos; i++) { sum += float32[i]; count++; }
            result[idx++] = sum / Math.max(1, count);
            pos = nextPos;
          }
          return result;
        };
        const mono16k = srcRate === 16000 ? pcmData : downsampleTo16k(pcmData, srcRate);

        this.sessionPromise.then((session) => {
          session.sendRealtimeInput({media: createBlob(mono16k)});
        }).catch((e) => this.updateError('Session send failed: ' + e.message));
      };

      this.sourceNode.connect(this.scriptProcessorNode);
      this.scriptProcessorNode.connect(this.inputAudioContext.destination);

      this.frameInterval = window.setInterval(() => {
        if (!this.isRecording) return;
        const context = this.canvasElement.getContext('2d');
        this.canvasElement.width = this.videoElement.videoWidth;
        this.canvasElement.height = this.videoElement.videoHeight;
        context.drawImage(
          this.videoElement,
          0,
          0,
          this.canvasElement.width,
          this.canvasElement.height,
        );
        this.canvasElement.toBlob(
          async (blob) => {
            if (blob) {
              const base64Data = await this.blobToBase64(blob);
              this.sessionPromise
                .then((session) => {
                  session.sendRealtimeInput({
                    media: {data: base64Data, mimeType: 'image/jpeg'},
                  });
                  this.framesSent++;
                  if (this.framesSent % 10 === 0) {
                    this.debug(`Frames sent: ${this.framesSent}`);
                  }
                })
                .catch((e) =>
                  this.updateError('Session send (image) failed: ' + e.message),
                );
            }
          },
          'image/jpeg',
          0.8,
        );
      }, 500); // 2 frames per second

      this.isRecording = true;
      this.updateStatus('');
    } catch (err) {
      console.error('Error starting recording:', err);
      this.updateStatus(`Error: ${err.message}`);
      this.stopRecording();
    }
  }

  private stopRecording() {
    if (!this.isRecording && !this.mediaStream && !this.inputAudioContext)
      return;

    this.updateStatus('');

    this.isRecording = false;

    if (this.frameInterval) {
      clearInterval(this.frameInterval);
      this.frameInterval = null;
    }

    if (this.scriptProcessorNode && this.sourceNode && this.inputAudioContext) {
      this.scriptProcessorNode.disconnect();
      this.sourceNode.disconnect();
    }

    this.scriptProcessorNode = null;
    this.sourceNode = null;

    if (this.mediaStream) {
      this.mediaStream.getTracks().forEach((track) => track.stop());
      this.mediaStream = null;
    }

    if (this.videoElement) {
      this.videoElement.srcObject = null;
    }

    this.updateStatus('');
  }

  private reset() {
    this.sessionPromise?.then((session) => session.close());
    this.sessionPromise = this.initSession();
    this.updateStatus('');
  }

  private async handleSpeakTap() {
    if (this.isRecording) {
      this.stopRecording();
      return;
    }

    try {
      await this.startRecording();
    } catch (err) {
      console.error('Error handling speak tap:', err);
    }
  }

  render() {
    return html`
      <div class="layout">
        <video id="video-background" autoplay muted playsinline></video>
        <canvas id="frame-canvas" style="display:none"></canvas>
        ${this.error
          ? html`<div class="error-banner" role="alert">${this.error}</div>`
          : null}
        <div class="speak-area">
          <gdm-live-audio-visuals-3d
            .inputNode=${this.inputNode}
            .outputNode=${this.outputNode}></gdm-live-audio-visuals-3d>
          <button
            class="speak-button"
            type="button"
            aria-pressed=${this.isRecording}
            aria-label=${this.isRecording ? 'Stop speaking' : 'Tap to speak'}
            @click=${this.handleSpeakTap}>
            <span class="speak-label">Tap to Speak</span>
          </button>
        </div>
      </div>
    `;
  }
}

```

```typescript
/**
 * @license
 * SPDX-License-Identifier: Apache-2.0
 */

// tslint:disable:organize-imports
// tslint:disable:ban-malformed-import-paths
// tslint:disable:no-new-decorators

import {LitElement, css, html} from 'lit';
import {customElement, property} from 'lit/decorators.js';
import {Analyser} from './analyser';

import * as THREE from 'three';
import {EXRLoader} from 'three/addons/loaders/EXRLoader.js';
import {EffectComposer} from 'three/addons/postprocessing/EffectComposer.js';
import {RenderPass} from 'three/addons/postprocessing/RenderPass.js';
import {ShaderPass} from 'three/addons/postprocessing/ShaderPass.js';
import {UnrealBloomPass} from 'three/addons/postprocessing/UnrealBloomPass.js';
import {FXAAShader} from 'three/addons/shaders/FXAAShader.js';
import {fs as backdropFS, vs as backdropVS} from './backdrop-shader';
import {vs as sphereVS} from './sphere-shader';

/**
 * 3D live audio visual.
 */
@customElement('gdm-live-audio-visuals-3d')
export class GdmLiveAudioVisuals3D extends LitElement {
  private inputAnalyser!: Analyser;
  private outputAnalyser!: Analyser;
  private camera!: THREE.PerspectiveCamera;
  private backdrop!: THREE.Mesh;
  private composer!: EffectComposer;
  private sphere!: THREE.Mesh;
  private prevTime = 0;
  private rotation = new THREE.Vector3(0, 0, 0);

  private _outputNode!: AudioNode;

  @property()
  set outputNode(node: AudioNode) {
    this._outputNode = node;
    this.outputAnalyser = new Analyser(this._outputNode);
  }

  get outputNode() {
    return this._outputNode;
  }

  private _inputNode!: AudioNode;

  @property()
  set inputNode(node: AudioNode) {
    this._inputNode = node;
    this.inputAnalyser = new Analyser(this._inputNode);
  }

  get inputNode() {
    return this._inputNode;
  }

  private canvas!: HTMLCanvasElement;

  static styles = css`
    canvas {
      width: 100% !important;
      height: 100% !important;
      position: absolute;
      inset: 0;
      image-rendering: pixelated;
    }
  `;

  connectedCallback() {
    super.connectedCallback();
  }

  private init() {
    const scene = new THREE.Scene();
    scene.background = new THREE.Color(0x100c14);

    const backdrop = new THREE.Mesh(
      new THREE.IcosahedronGeometry(10, 5),
      new THREE.RawShaderMaterial({
        uniforms: {
          resolution: {value: new THREE.Vector2(1, 1)},
          rand: {value: 0},
        },
        vertexShader: backdropVS,
        fragmentShader: backdropFS,
        glslVersion: THREE.GLSL3,
      }),
    );
    backdrop.material.side = THREE.BackSide;
    scene.add(backdrop);
    this.backdrop = backdrop;

    const camera = new THREE.PerspectiveCamera(
      75,
      window.innerWidth / window.innerHeight,
      0.1,
      1000,
    );
    camera.position.set(1.8, -1.8, 3.6);
    this.camera = camera;

    const renderer = new THREE.WebGLRenderer({
      canvas: this.canvas,
      antialias: !true,
    });
    renderer.setSize(window.innerWidth, window.innerHeight);
    renderer.setPixelRatio(window.devicePixelRatio / 1);

    const geometry = new THREE.IcosahedronGeometry(1, 10);

    new EXRLoader().load('piz_compressed.exr', (texture: THREE.Texture) => {
      texture.mapping = THREE.EquirectangularReflectionMapping;
      const exrCubeRenderTarget = pmremGenerator.fromEquirectangular(texture);
      sphereMaterial.envMap = exrCubeRenderTarget.texture;
      sphere.visible = true;
    });

    const pmremGenerator = new THREE.PMREMGenerator(renderer);
    pmremGenerator.compileEquirectangularShader();

    const sphereMaterial = new THREE.MeshStandardMaterial({
      color: 0x000010,
      metalness: 0.5,
      roughness: 0.1,
      emissive: 0x000010,
      emissiveIntensity: 1.5,
    });

    sphereMaterial.onBeforeCompile = (shader) => {
      shader.uniforms.time = {value: 0};
      shader.uniforms.inputData = {value: new THREE.Vector4()};
      shader.uniforms.outputData = {value: new THREE.Vector4()};

      sphereMaterial.userData.shader = shader;

      shader.vertexShader = sphereVS;
    };

    const sphere = new THREE.Mesh(geometry, sphereMaterial);
    scene.add(sphere);
    sphere.visible = false;

    this.sphere = sphere;

    const renderPass = new RenderPass(scene, camera);

    const bloomPass = new UnrealBloomPass(
      new THREE.Vector2(window.innerWidth, window.innerHeight),
      5,
      0.5,
      0,
    );

    const fxaaPass = new ShaderPass(FXAAShader);

    const composer = new EffectComposer(renderer);
    composer.addPass(renderPass);
    // composer.addPass(fxaaPass);
    composer.addPass(bloomPass);

    this.composer = composer;

    const onWindowResize = () => {
      if (!this.canvas) return;
      const w = this.canvas.clientWidth;
      const h = this.canvas.clientHeight;

      camera.aspect = w / h;
      camera.updateProjectionMatrix();

      const dPR = renderer.getPixelRatio();
      backdrop.material.uniforms.resolution.value.set(w * dPR, h * dPR);

      renderer.setSize(w, h);
      composer.setSize(w, h);

      fxaaPass.material.uniforms['resolution'].value.set(
        1 / (w * dPR),
        1 / (h * dPR),
      );
    };

    window.addEventListener('resize', onWindowResize);
    onWindowResize();

    this.animation();
  }

  private animation() {
    requestAnimationFrame(() => this.animation());

    this.inputAnalyser.update();
    this.outputAnalyser.update();

    const t = performance.now();
    const dt = (t - this.prevTime) / (1000 / 60);
    this.prevTime = t;
    const backdropMaterial = this.backdrop.material as THREE.RawShaderMaterial;
    const sphereMaterial = this.sphere.material as THREE.MeshStandardMaterial;

    backdropMaterial.uniforms.rand.value = Math.random() * 10000;

    if (sphereMaterial.userData.shader) {
      this.sphere.scale.setScalar(
        1.35 + (0.25 * this.outputAnalyser.data[1]) / 255,
      );

      const f = 0.001;
      this.rotation.x += (dt * f * 0.5 * this.outputAnalyser.data[1]) / 255;
      this.rotation.z += (dt * f * 0.5 * this.inputAnalyser.data[1]) / 255;
      this.rotation.y += (dt * f * 0.25 * this.inputAnalyser.data[2]) / 255;
      this.rotation.y += (dt * f * 0.25 * this.outputAnalyser.data[2]) / 255;

      const euler = new THREE.Euler(
        this.rotation.x,
        this.rotation.y,
        this.rotation.z,
      );
      const quaternion = new THREE.Quaternion().setFromEuler(euler);
      const vector = new THREE.Vector3(0, 0, 3.6);
      vector.applyQuaternion(quaternion);
      this.camera.position.copy(vector);
      this.camera.lookAt(this.sphere.position);

      sphereMaterial.userData.shader.uniforms.time.value +=
        (dt * 0.1 * this.outputAnalyser.data[0]) / 255;
      sphereMaterial.userData.shader.uniforms.inputData.value.set(
        (1 * this.inputAnalyser.data[0]) / 255,
        (0.1 * this.inputAnalyser.data[1]) / 255,
        (10 * this.inputAnalyser.data[2]) / 255,
        0,
      );
      sphereMaterial.userData.shader.uniforms.outputData.value.set(
        (2 * this.outputAnalyser.data[0]) / 255,
        (0.1 * this.outputAnalyser.data[1]) / 255,
        (10 * this.outputAnalyser.data[2]) / 255,
        0,
      );
    }

    this.composer.render();
  }

  protected firstUpdated() {
    this.canvas = this.shadowRoot!.querySelector('canvas') as HTMLCanvasElement;
    this.init();
  }

  protected render() {
    return html`<canvas></canvas>`;
  }
}

declare global {
  interface HTMLElementTagNameMap {
    'gdm-live-audio-visuals-3d': GdmLiveAudioVisuals3D;
  }
}

```

## UI Update

- Shortened `speak-area` panel to one-third height and kept camera feed filling the rest, preserving rounded glass styling while respecting safe areas.
- Brought the sphere closer and increased its base scale so the shader now feels zoomed-in within the tighter canvas.

Tests: not run (please verify in app).

```1:552:/Users/eirvav/PythonProjects/A-EYE-Vite/index.tsx
/* tslint:disable */
/**
 * @license
 * SPDX-License-Identifier: Apache-2.0
 */

import {GoogleGenAI, LiveServerMessage, Modality, Session} from '@google/genai';
import {LitElement, css, html} from 'lit';
import {customElement, state} from 'lit/decorators.js';
import {createBlob, decode, decodeAudioData} from './utils';
import './visual-3d';
// PWA: register service worker if available
try {
  // dynamic import optional; vite-plugin-pwa injects virtual module
  // eslint-disable-next-line @typescript-eslint/ban-ts-comment
  // @ts-ignore
  import('virtual:pwa-register').then(({ registerSW }) => {
    registerSW({ immediate: true });
  }).catch(() => {});
} catch {}

@customElement('gdm-live-audio')
export class GdmLiveAudio extends LitElement {
  @state() isRecording = false;
  @state() status = '';
  @state() error = '';

  private client: GoogleGenAI;
  private sessionPromise: Promise<Session>;
  // FIX: Cast window to `any` to access prefixed `webkitAudioContext` for older browsers.
  private inputAudioContext = new (window.AudioContext ||
    (window as any).webkitAudioContext)({sampleRate: 16000});
  // FIX: Cast window to `any` to access prefixed `webkitAudioContext` for older browsers.
  private outputAudioContext = new (window.AudioContext ||
    (window as any).webkitAudioContext)({sampleRate: 24000});
  @state() inputNode = this.inputAudioContext.createGain();
  @state() outputNode = this.outputAudioContext.createGain();
  private nextStartTime = 0;
  private mediaStream: MediaStream;
  private sourceNode: AudioNode;
  private scriptProcessorNode: ScriptProcessorNode;
  private sources = new Set<AudioBufferSourceNode>();
  private frameInterval: number;
  private videoElement: HTMLVideoElement;
  private canvasElement: HTMLCanvasElement;
  private framesSent = 0;

  static styles = css`
    :host {
      display: block;
      position: relative;
      width: 100vw;
      height: 100vh;
      min-height: 100dvh;
      background: #000000;
      color: #ffffff;
      overflow: hidden;
    }

    @supports (height: 100dvh) {
      :host {
        height: 100dvh;
      }
    }

    .layout {
      position: relative;
      width: 100%;
      height: 100%;
    }

    #video-background {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 66.667vh;
      object-fit: cover;
    }

    .speak-area {
      position: absolute;
      bottom: 0;
      left: 0;
      width: 100%;
      height: calc(33.333vh + env(safe-area-inset-bottom));
      border-top-left-radius: 32px;
      border-top-right-radius: 32px;
      border: 1px solid rgba(255, 255, 255, 0.08);
      border-bottom: none;
      overflow: hidden;
      backdrop-filter: blur(6px);
    }

    gdm-live-audio-visuals-3d {
      display: block;
      width: 100%;
      height: 100%;
      pointer-events: none;
      border-top-left-radius: inherit;
      border-top-right-radius: inherit;
    }

    .speak-button {
      position: absolute;
      inset: 0;
      width: 100%;
      height: 100%;
      border: none;
      background: transparent;
      cursor: pointer;
      display: flex;
      align-items: flex-end;
      justify-content: center;
      padding: 24px 24px calc(24px + env(safe-area-inset-bottom));
      color: inherit;
      transition: background 0.2s ease;
    }

    .speak-button:hover,
    .speak-button:focus-visible {
      background: rgba(255, 255, 255, 0.04);
    }

    .speak-button:focus-visible {
      outline: 2px solid rgba(255, 255, 255, 0.25);
      outline-offset: 4px;
    }

    .speak-label {
      font-size: 18px;
      letter-spacing: 0.08em;
      text-transform: uppercase;
      color: rgba(255, 255, 255, 0.92);
      text-shadow: 0 0 6px rgba(0, 0, 0, 0.6);
    }

    .error-banner {
      position: absolute;
      top: calc(env(safe-area-inset-top) + 12px);
      left: 16px;
      right: 16px;
      z-index: 10;
      padding: 12px 16px;
      background: rgba(220, 38, 38, 0.85);
      color: #ffffff;
      border-radius: 12px;
      text-align: center;
      font-size: 14px;
      line-height: 1.4;
      box-shadow: 0 12px 24px rgba(0, 0, 0, 0.35);
      backdrop-filter: blur(12px);
    }
  `;

  constructor() {
    super();
    this.initClient();
  }

  protected firstUpdated() {
    this.videoElement = this.shadowRoot!.querySelector('#video-background');
    this.canvasElement = this.shadowRoot!.querySelector('#frame-canvas');
    this.debug(
      `Boot: secure=${window.isSecureContext} ua=${navigator.userAgent}`,
    );
    this.checkPermissions();
    this.addEventListener('click', async () => {
      try {
        await this.outputAudioContext.resume();
        await this.inputAudioContext.resume();
      } catch {}
    });
  }

  private async checkPermissions() {
    const parts: string[] = [];
    const anyNav: any = navigator as any;
    if (anyNav.permissions?.query) {
      try {
        const mic = await anyNav.permissions.query({name: 'microphone'});
        parts.push(`mic=${mic.state}`);
      } catch {}
      try {
        const cam = await anyNav.permissions.query({name: 'camera'});
        parts.push(`cam=${cam.state}`);
      } catch {}
    }
    if (parts.length) this.updateStatus(`Permissions: ${parts.join(' ')}`);
  }

  private debug(_msg: string, _data?: unknown) {
    // Debug logging disabled for production UI clarity.
  }

  private initAudio() {
    this.nextStartTime = this.outputAudioContext.currentTime;
  }

  private async initClient() {
    this.initAudio();

    if (!process.env.API_KEY || process.env.API_KEY === 'undefined') {
      this.updateError('Missing GEMINI_API_KEY');
    }

    this.client = new GoogleGenAI({apiKey: process.env.API_KEY});

    this.outputNode.connect(this.outputAudioContext.destination);

    this.sessionPromise = this.initSession();
  }

  private initSession(): Promise<Session> {
    const model = 'gemini-2.5-flash-native-audio-preview-09-2025';

    try {
      return this.client.live.connect({
        model: model,
        callbacks: {
          onopen: () => {
            this.debug('Live session opened');
          },
          onmessage: async (message: LiveServerMessage) => {
            this.debug('Live message received');
            const audio =
              message.serverContent?.modelTurn?.parts[0]?.inlineData;

            if (audio) {
              this.nextStartTime = Math.max(
                this.nextStartTime,
                this.outputAudioContext.currentTime,
              );

              const audioBuffer = await decodeAudioData(
                decode(audio.data),
                this.outputAudioContext,
                24000,
                1,
              );
              const source = this.outputAudioContext.createBufferSource();
              source.buffer = audioBuffer;
              source.connect(this.outputNode);
              source.addEventListener('ended', () => {
                this.sources.delete(source);
              });

              source.start(this.nextStartTime);
              this.nextStartTime = this.nextStartTime + audioBuffer.duration;
              this.sources.add(source);
            }

            const interrupted = message.serverContent?.interrupted;
            if (interrupted) {
              for (const source of this.sources.values()) {
                source.stop();
                this.sources.delete(source);
              }
              this.nextStartTime = 0;
            }
          },
          onerror: (e: ErrorEvent) => {
            this.updateError('Live error: ' + e.message);
            console.error('Live error event', e);
          },
          onclose: (e: CloseEvent) => {
            this.updateStatus('Live close: ' + e.reason + ' (' + e.code + ')');
            console.warn('Live close', e);
          },
        },
        config: {
          responseModalities: [Modality.AUDIO],
          speechConfig: {
            voiceConfig: {prebuiltVoiceConfig: {voiceName: 'Zephyr'}},
            // languageCode: 'en-GB'
          },
        },
      });
    } catch (e) {
      console.error(e);
      return Promise.reject(e);
    }
  }

  private updateStatus(msg: string) {
    this.status = msg;
  }

  private updateError(msg: string) {
    this.error = msg;
  }

  private blobToBase64(blob: Blob): Promise<string> {
    return new Promise((resolve, reject) => {
      const reader = new FileReader();
      reader.onloadend = () => {
        if (typeof reader.result === 'string') {
          resolve(reader.result.split(',')[1]);
        } else {
          reject(new Error('Failed to read blob as base64 string.'));
        }
      };
      reader.onerror = reject;
      reader.readAsDataURL(blob);
    });
  }

  private async startRecording() {
    if (this.isRecording) {
      return;
    }

    await this.inputAudioContext.resume();
    await this.outputAudioContext.resume();

    this.updateStatus('Requesting camera/mic access...');

    try {
      const preferBackCamera = async (): Promise<MediaStream> => {
        const constraints: MediaStreamConstraints = {
          audio: {
            channelCount: 1,
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true,
          },
          video: {
            facingMode: { ideal: 'environment' },
          },
        };
        this.debug('getUserMedia with constraints', constraints);
        try {
          const stream = await navigator.mediaDevices.getUserMedia(constraints);
          const track = stream.getVideoTracks()[0];
          const settings = track?.getSettings?.() || {} as MediaTrackSettings;
          this.debug('Acquired stream video settings', settings);
          if (settings.facingMode !== 'environment') {
            // Try enumerate fallback to enforce back camera
            const devices = await navigator.mediaDevices.enumerateDevices();
            const back = devices.find(
              (d) => d.kind === 'videoinput' && /back|rear|environment/i.test(d.label),
            );
            if (back) {
              try {
                const enforced = await navigator.mediaDevices.getUserMedia({
                  audio: constraints.audio,
                  video: { deviceId: { exact: back.deviceId } },
                });
                this.debug('Enforced back camera via deviceId', back);
                stream.getTracks().forEach((t) => t.stop());
                return enforced;
              } catch {
                // fall through to original stream
              }
            }
          }
          return stream;
        } catch (e) {
          this.updateError('getUserMedia failed: ' + (e as Error).message);
          const devices = await navigator.mediaDevices.enumerateDevices();
          const back = devices.find(
            (d) => d.kind === 'videoinput' && /back|rear|environment/i.test(d.label),
          );
          if (back) {
            return await navigator.mediaDevices.getUserMedia({
              audio: constraints.audio,
              video: { deviceId: { exact: back.deviceId } },
            });
          }
          throw e;
        }
      };

      this.mediaStream = await preferBackCamera();

      this.videoElement.srcObject = this.mediaStream;
      try {
        await this.videoElement.play();
      } catch (e) {
        console.warn('video.play() failed', e);
      }

      const vt = this.mediaStream.getVideoTracks()[0];
      const at = this.mediaStream.getAudioTracks()[0];
      this.updateStatus('Media access granted. v=' + (!!vt) + ' a=' + (!!at));

      this.sourceNode = this.inputAudioContext.createMediaStreamSource(
        this.mediaStream,
      );
      this.sourceNode.connect(this.inputNode);

      const bufferSize = 4096;
      this.scriptProcessorNode = this.inputAudioContext.createScriptProcessor(
        bufferSize,
        1,
        1,
      );

      this.scriptProcessorNode.onaudioprocess = (audioProcessingEvent) => {
        if (!this.isRecording) return;

        const inputBuffer = audioProcessingEvent.inputBuffer;
        const pcmData = inputBuffer.getChannelData(0);
        const srcRate = this.inputAudioContext.sampleRate;
        const downsampleTo16k = (float32: Float32Array, inRate: number) => {
          if (inRate === 16000) return float32;
          const ratio = inRate / 16000;
          const newLen = Math.floor(float32.length / ratio);
          const result = new Float32Array(newLen);
          let idx = 0, pos = 0;
          while (idx < newLen) {
            const nextPos = Math.min(float32.length - 1, (idx + 1) * ratio);
            let sum = 0, count = 0;
            for (let i = pos; i < nextPos; i++) { sum += float32[i]; count++; }
            result[idx++] = sum / Math.max(1, count);
            pos = nextPos;
          }
          return result;
        };
        const mono16k = srcRate === 16000 ? pcmData : downsampleTo16k(pcmData, srcRate);

        this.sessionPromise.then((session) => {
          session.sendRealtimeInput({media: createBlob(mono16k)});
        }).catch((e) => this.updateError('Session send failed: ' + e.message));
      };

      this.sourceNode.connect(this.scriptProcessorNode);
      this.scriptProcessorNode.connect(this.inputAudioContext.destination);

      this.frameInterval = window.setInterval(() => {
        if (!this.isRecording) return;
        const context = this.canvasElement.getContext('2d');
        this.canvasElement.width = this.videoElement.videoWidth;
        this.canvasElement.height = this.videoElement.videoHeight;
        context.drawImage(
          this.videoElement,
          0,
          0,
          this.canvasElement.width,
          this.canvasElement.height,
        );
        this.canvasElement.toBlob(
          async (blob) => {
            if (blob) {
              const base64Data = await this.blobToBase64(blob);
              this.sessionPromise
                .then((session) => {
                  session.sendRealtimeInput({
                    media: {data: base64Data, mimeType: 'image/jpeg'},
                  });
                  this.framesSent++;
                  if (this.framesSent % 10 === 0) {
                    this.debug(`Frames sent: ${this.framesSent}`);
                  }
                })
                .catch((e) =>
                  this.updateError('Session send (image) failed: ' + e.message),
                );
            }
          },
          'image/jpeg',
          0.8,
        );
      }, 500); // 2 frames per second

      this.isRecording = true;
      this.updateStatus('');
    } catch (err) {
      console.error('Error starting recording:', err);
      this.updateStatus(`Error: ${err.message}`);
      this.stopRecording();
    }
  }

  private stopRecording() {
    if (!this.isRecording && !this.mediaStream && !this.inputAudioContext)
      return;

    this.updateStatus('');

    this.isRecording = false;

    if (this.frameInterval) {
      clearInterval(this.frameInterval);
      this.frameInterval = null;
    }

    if (this.scriptProcessorNode && this.sourceNode && this.inputAudioContext) {
      this.scriptProcessorNode.disconnect();
      this.sourceNode.disconnect();
    }

    this.scriptProcessorNode = null;
    this.sourceNode = null;

    if (this.mediaStream) {
      this.mediaStream.getTracks().forEach((track) => track.stop());
      this.mediaStream = null;
    }

    if (this.videoElement) {
      this.videoElement.srcObject = null;
    }

    this.updateStatus('');
  }

  private reset() {
    this.sessionPromise?.then((session) => session.close());
    this.sessionPromise = this.initSession();
    this.updateStatus('');
  }

  private async handleSpeakTap() {
    if (this.isRecording) {
      this.stopRecording();
      return;
    }

    try {
      await this.startRecording();
    } catch (err) {
      console.error('Error handling speak tap:', err);
    }
  }

  render() {
    return html`
      <div class="layout">
        <video id="video-background" autoplay muted playsinline></video>
        <canvas id="frame-canvas" style="display:none"></canvas>
        ${this.error
          ? html`<div class="error-banner" role="alert">${this.error}</div>`
          : null}
        <div class="speak-area">
          <gdm-live-audio-visuals-3d
            .inputNode=${this.inputNode}
            .outputNode=${this.outputNode}></gdm-live-audio-visuals-3d>
          <button
            class="speak-button"
            type="button"
            aria-pressed=${this.isRecording}
            aria-label=${this.isRecording ? 'Stop speaking' : 'Tap to speak'}
            @click=${this.handleSpeakTap}>
            <span class="speak-label">Tap to Speak</span>
          </button>
        </div>
      </div>
    `;
  }
}
```

```1:264:/Users/eirvav/PythonProjects/A-EYE-Vite/visual-3d.ts
/**
 * @license
 * SPDX-License-Identifier: Apache-2.0
 */

// tslint:disable:organize-imports
// tslint:disable:ban-malformed-import-paths
// tslint:disable:no-new-decorators

import {LitElement, css, html} from 'lit';
import {customElement, property} from 'lit/decorators.js';
import {Analyser} from './analyser';

import * as THREE from 'three';
import {EXRLoader} from 'three/addons/loaders/EXRLoader.js';
import {EffectComposer} from 'three/addons/postprocessing/EffectComposer.js';
import {RenderPass} from 'three/addons/postprocessing/RenderPass.js';
import {ShaderPass} from 'three/addons/postprocessing/ShaderPass.js';
import {UnrealBloomPass} from 'three/addons/postprocessing/UnrealBloomPass.js';
import {FXAAShader} from 'three/addons/shaders/FXAAShader.js';
import {fs as backdropFS, vs as backdropVS} from './backdrop-shader';
import {vs as sphereVS} from './sphere-shader';

/**
 * 3D live audio visual.
 */
@customElement('gdm-live-audio-visuals-3d')
export class GdmLiveAudioVisuals3D extends LitElement {
  private inputAnalyser!: Analyser;
  private outputAnalyser!: Analyser;
  private camera!: THREE.PerspectiveCamera;
  private backdrop!: THREE.Mesh;
  private composer!: EffectComposer;
  private sphere!: THREE.Mesh;
  private prevTime = 0;
  private rotation = new THREE.Vector3(0, 0, 0);

  private _outputNode!: AudioNode;

  @property()
  set outputNode(node: AudioNode) {
    this._outputNode = node;
    this.outputAnalyser = new Analyser(this._outputNode);
  }

  get outputNode() {
    return this._outputNode;
  }

  private _inputNode!: AudioNode;

  @property()
  set inputNode(node: AudioNode) {
    this._inputNode = node;
    this.inputAnalyser = new Analyser(this._inputNode);
  }

  get inputNode() {
    return this._inputNode;
  }

  private canvas!: HTMLCanvasElement;

  static styles = css`
    canvas {
      width: 100% !important;
      height: 100% !important;
      position: absolute;
      inset: 0;
      image-rendering: pixelated;
    }
  `;

  connectedCallback() {
    super.connectedCallback();
  }

  private init() {
    const scene = new THREE.Scene();
    scene.background = new THREE.Color(0x100c14);

    const backdrop = new THREE.Mesh(
      new THREE.IcosahedronGeometry(10, 5),
      new THREE.RawShaderMaterial({
        uniforms: {
          resolution: {value: new THREE.Vector2(1, 1)},
          rand: {value: 0},
        },
        vertexShader: backdropVS,
        fragmentShader: backdropFS,
        glslVersion: THREE.GLSL3,
      }),
    );
    backdrop.material.side = THREE.BackSide;
    scene.add(backdrop);
    this.backdrop = backdrop;

    const camera = new THREE.PerspectiveCamera(
      75,
      window.innerWidth / window.innerHeight,
      0.1,
      1000,
    );
    camera.position.set(1.8, -1.8, 3.6);
    this.camera = camera;

    const renderer = new THREE.WebGLRenderer({
      canvas: this.canvas,
      antialias: !true,
    });
    renderer.setSize(window.innerWidth, window.innerHeight);
    renderer.setPixelRatio(window.devicePixelRatio / 1);

    const geometry = new THREE.IcosahedronGeometry(1, 10);

    new EXRLoader().load('piz_compressed.exr', (texture: THREE.Texture) => {
      texture.mapping = THREE.EquirectangularReflectionMapping;
      const exrCubeRenderTarget = pmremGenerator.fromEquirectangular(texture);
      sphereMaterial.envMap = exrCubeRenderTarget.texture;
      sphere.visible = true;
    });

    const pmremGenerator = new THREE.PMREMGenerator(renderer);
    pmremGenerator.compileEquirectangularShader();

    const sphereMaterial = new THREE.MeshStandardMaterial({
      color: 0x000010,
      metalness: 0.5,
      roughness: 0.1,
      emissive: 0x000010,
      emissiveIntensity: 1.5,
    });

    sphereMaterial.onBeforeCompile = (shader) => {
      shader.uniforms.time = {value: 0};
      shader.uniforms.inputData = {value: new THREE.Vector4()};
      shader.uniforms.outputData = {value: new THREE.Vector4()};

      sphereMaterial.userData.shader = shader;

      shader.vertexShader = sphereVS;
    };

    const sphere = new THREE.Mesh(geometry, sphereMaterial);
    scene.add(sphere);
    sphere.visible = false;

    this.sphere = sphere;

    const renderPass = new RenderPass(scene, camera);

    const bloomPass = new UnrealBloomPass(
      new THREE.Vector2(window.innerWidth, window.innerHeight),
      5,
      0.5,
      0,
    );

    const fxaaPass = new ShaderPass(FXAAShader);

    const composer = new EffectComposer(renderer);
    composer.addPass(renderPass);
    // composer.addPass(fxaaPass);
    composer.addPass(bloomPass);

    this.composer = composer;

    const onWindowResize = () => {
      if (!this.canvas) return;
      const w = this.canvas.clientWidth;
      const h = this.canvas.clientHeight;

      camera.aspect = w / h;
      camera.updateProjectionMatrix();

      const dPR = renderer.getPixelRatio();
      backdrop.material.uniforms.resolution.value.set(w * dPR, h * dPR);

      renderer.setSize(w, h);
      composer.setSize(w, h);

      fxaaPass.material.uniforms['resolution'].value.set(
        1 / (w * dPR),
        1 / (h * dPR),
      );
    };

    window.addEventListener('resize', onWindowResize);
    onWindowResize();

    this.animation();
  }

  private animation() {
    requestAnimationFrame(() => this.animation());

    this.inputAnalyser.update();
    this.outputAnalyser.update();

    const t = performance.now();
    const dt = (t - this.prevTime) / (1000 / 60);
    this.prevTime = t;
    const backdropMaterial = this.backdrop.material as THREE.RawShaderMaterial;
    const sphereMaterial = this.sphere.material as THREE.MeshStandardMaterial;

    backdropMaterial.uniforms.rand.value = Math.random() * 10000;

    if (sphereMaterial.userData.shader) {
      this.sphere.scale.setScalar(
        1.35 + (0.25 * this.outputAnalyser.data[1]) / 255,
      );

      const f = 0.001;
      this.rotation.x += (dt * f * 0.5 * this.outputAnalyser.data[1]) / 255;
      this.rotation.z += (dt * f * 0.5 * this.inputAnalyser.data[1]) / 255;
      this.rotation.y += (dt * f * 0.25 * this.inputAnalyser.data[2]) / 255;
      this.rotation.y += (dt * f * 0.25 * this.outputAnalyser.data[2]) / 255;

      const euler = new THREE.Euler(
        this.rotation.x,
        this.rotation.y,
        this.rotation.z,
      );
      const quaternion = new THREE.Quaternion().setFromEuler(euler);
      const vector = new THREE.Vector3(0, 0, 3.6);
      vector.applyQuaternion(quaternion);
      this.camera.position.copy(vector);
      this.camera.lookAt(this.sphere.position);

      sphereMaterial.userData.shader.uniforms.time.value +=
        (dt * 0.1 * this.outputAnalyser.data[0]) / 255;
      sphereMaterial.userData.shader.uniforms.inputData.value.set(
        (1 * this.inputAnalyser.data[0]) / 255,
        (0.1 * this.inputAnalyser.data[1]) / 255,
        (10 * this.inputAnalyser.data[2]) / 255,
        0,
      );
      sphereMaterial.userData.shader.uniforms.outputData.value.set(
        (2 * this.outputAnalyser.data[0]) / 255,
        (0.1 * this.outputAnalyser.data[1]) / 255,
        (10 * this.outputAnalyser.data[2]) / 255,
        0,
      );
    }

    this.composer.render();
  }

  protected firstUpdated() {
    this.canvas = this.shadowRoot!.querySelector('canvas') as HTMLCanvasElement;
    this.init();
  }

  protected render() {
    return html`<canvas></canvas>`;
  }
}

declare global {
  interface HTMLElementTagNameMap {
    'gdm-live-audio-visuals-3d': GdmLiveAudioVisuals3D;
  }
}
```

```typescriptreact
/* tslint:disable */
/**
 * @license
 * SPDX-License-Identifier: Apache-2.0
 */

import {GoogleGenAI, LiveServerMessage, Modality, Session} from '@google/genai';
import {LitElement, css, html} from 'lit';
import {customElement, state} from 'lit/decorators.js';
import {createBlob, decode, decodeAudioData} from './utils';
import './visual-3d';
// PWA: register service worker if available
try {
  // dynamic import optional; vite-plugin-pwa injects virtual module
  // eslint-disable-next-line @typescript-eslint/ban-ts-comment
  // @ts-ignore
  import('virtual:pwa-register').then(({ registerSW }) => {
    registerSW({ immediate: true });
  }).catch(() => {});
} catch {}

@customElement('gdm-live-audio')
export class GdmLiveAudio extends LitElement {
  @state() isRecording = false;
  @state() status = '';
  @state() error = '';

  private client: GoogleGenAI;
  private sessionPromise: Promise<Session>;
  // FIX: Cast window to `any` to access prefixed `webkitAudioContext` for older browsers.
  private inputAudioContext = new (window.AudioContext ||
    (window as any).webkitAudioContext)({sampleRate: 16000});
  // FIX: Cast window to `any` to access prefixed `webkitAudioContext` for older browsers.
  private outputAudioContext = new (window.AudioContext ||
    (window as any).webkitAudioContext)({sampleRate: 24000});
  @state() inputNode = this.inputAudioContext.createGain();
  @state() outputNode = this.outputAudioContext.createGain();
  private nextStartTime = 0;
  private mediaStream: MediaStream;
  private sourceNode: AudioNode;
  private scriptProcessorNode: ScriptProcessorNode;
  private sources = new Set<AudioBufferSourceNode>();
  private frameInterval: number;
  private videoElement: HTMLVideoElement;
  private canvasElement: HTMLCanvasElement;
  private framesSent = 0;

  static styles = css`
    :host {
      display: block;
      position: relative;
      width: 100vw;
      height: 100vh;
      min-height: 100dvh;
      background: #000000;
      color: #ffffff;
      overflow: hidden;
    }

    @supports (height: 100dvh) {
      :host {
        height: 100dvh;
      }
    }

    .layout {
      position: relative;
      width: 100%;
      height: 100%;
    }

    #video-background {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 66.667vh;
      object-fit: cover;
    }

    .speak-area {
      position: absolute;
      bottom: 0;
      left: 0;
      width: 100%;
      height: calc(33.333vh + env(safe-area-inset-bottom));
      border-top-left-radius: 32px;
      border-top-right-radius: 32px;
      border: 1px solid rgba(255, 255, 255, 0.08);
      border-bottom: none;
      overflow: hidden;
      backdrop-filter: blur(6px);
    }

    gdm-live-audio-visuals-3d {
      display: block;
      width: 100%;
      height: 100%;
      pointer-events: none;
      border-top-left-radius: inherit;
      border-top-right-radius: inherit;
    }

    .speak-button {
      position: absolute;
      inset: 0;
      width: 100%;
      height: 100%;
      border: none;
      background: transparent;
      cursor: pointer;
      display: flex;
      align-items: flex-end;
      justify-content: center;
      padding: 24px 24px calc(24px + env(safe-area-inset-bottom));
      color: inherit;
      transition: background 0.2s ease;
    }

    .speak-button:hover,
    .speak-button:focus-visible {
      background: rgba(255, 255, 255, 0.04);
    }

    .speak-button:focus-visible {
      outline: 2px solid rgba(255, 255, 255, 0.25);
      outline-offset: 4px;
    }

    .speak-label {
      font-size: 18px;
      letter-spacing: 0.08em;
      text-transform: uppercase;
      color: rgba(255, 255, 255, 0.92);
      text-shadow: 0 0 6px rgba(0, 0, 0, 0.6);
    }

    .error-banner {
      position: absolute;
      top: calc(env(safe-area-inset-top) + 12px);
      left: 16px;
      right: 16px;
      z-index: 10;
      padding: 12px 16px;
      background: rgba(220, 38, 38, 0.85);
      color: #ffffff;
      border-radius: 12px;
      text-align: center;
      font-size: 14px;
      line-height: 1.4;
      box-shadow: 0 12px 24px rgba(0, 0, 0, 0.35);
      backdrop-filter: blur(12px);
    }
  `;

  constructor() {
    super();
    this.initClient();
  }

  protected firstUpdated() {
    this.videoElement = this.shadowRoot!.querySelector('#video-background');
    this.canvasElement = this.shadowRoot!.querySelector('#frame-canvas');
    this.debug(
      `Boot: secure=${window.isSecureContext} ua=${navigator.userAgent}`,
    );
    this.checkPermissions();
    this.addEventListener('click', async () => {
      try {
        await this.outputAudioContext.resume();
        await this.inputAudioContext.resume();
      } catch {}
    });
  }

  private async checkPermissions() {
    const parts: string[] = [];
    const anyNav: any = navigator as any;
    if (anyNav.permissions?.query) {
      try {
        const mic = await anyNav.permissions.query({name: 'microphone'});
        parts.push(`mic=${mic.state}`);
      } catch {}
      try {
        const cam = await anyNav.permissions.query({name: 'camera'});
        parts.push(`cam=${cam.state}`);
      } catch {}
    }
    if (parts.length) this.updateStatus(`Permissions: ${parts.join(' ')}`);
  }

  private debug(_msg: string, _data?: unknown) {
    // Debug logging disabled for production UI clarity.
  }

  private initAudio() {
    this.nextStartTime = this.outputAudioContext.currentTime;
  }

  private async initClient() {
    this.initAudio();

    if (!process.env.API_KEY || process.env.API_KEY === 'undefined') {
      this.updateError('Missing GEMINI_API_KEY');
    }

    this.client = new GoogleGenAI({apiKey: process.env.API_KEY});

    this.outputNode.connect(this.outputAudioContext.destination);

    this.sessionPromise = this.initSession();
  }

  private initSession(): Promise<Session> {
    const model = 'gemini-2.5-flash-native-audio-preview-09-2025';

    try {
      return this.client.live.connect({
        model: model,
        callbacks: {
          onopen: () => {
            this.debug('Live session opened');
          },
          onmessage: async (message: LiveServerMessage) => {
            this.debug('Live message received');
            const audio =
              message.serverContent?.modelTurn?.parts[0]?.inlineData;

            if (audio) {
              this.nextStartTime = Math.max(
                this.nextStartTime,
                this.outputAudioContext.currentTime,
              );

              const audioBuffer = await decodeAudioData(
                decode(audio.data),
                this.outputAudioContext,
                24000,
                1,
              );
              const source = this.outputAudioContext.createBufferSource();
              source.buffer = audioBuffer;
              source.connect(this.outputNode);
              source.addEventListener('ended', () => {
                this.sources.delete(source);
              });

              source.start(this.nextStartTime);
              this.nextStartTime = this.nextStartTime + audioBuffer.duration;
              this.sources.add(source);
            }

            const interrupted = message.serverContent?.interrupted;
            if (interrupted) {
              for (const source of this.sources.values()) {
                source.stop();
                this.sources.delete(source);
              }
              this.nextStartTime = 0;
            }
          },
          onerror: (e: ErrorEvent) => {
            this.updateError('Live error: ' + e.message);
            console.error('Live error event', e);
          },
          onclose: (e: CloseEvent) => {
            this.updateStatus('Live close: ' + e.reason + ' (' + e.code + ')');
            console.warn('Live close', e);
          },
        },
        config: {
          responseModalities: [Modality.AUDIO],
          speechConfig: {
            voiceConfig: {prebuiltVoiceConfig: {voiceName: 'Zephyr'}},
            // languageCode: 'en-GB'
          },
        },
      });
    } catch (e) {
      console.error(e);
      return Promise.reject(e);
    }
  }

  private updateStatus(msg: string) {
    this.status = msg;
  }

  private updateError(msg: string) {
    this.error = msg;
  }

  private blobToBase64(blob: Blob): Promise<string> {
    return new Promise((resolve, reject) => {
      const reader = new FileReader();
      reader.onloadend = () => {
        if (typeof reader.result === 'string') {
          resolve(reader.result.split(',')[1]);
        } else {
          reject(new Error('Failed to read blob as base64 string.'));
        }
      };
      reader.onerror = reject;
      reader.readAsDataURL(blob);
    });
  }

  private async startRecording() {
    if (this.isRecording) {
      return;
    }

    await this.inputAudioContext.resume();
    await this.outputAudioContext.resume();

    this.updateStatus('Requesting camera/mic access...');

    try {
      const preferBackCamera = async (): Promise<MediaStream> => {
        const constraints: MediaStreamConstraints = {
          audio: {
            channelCount: 1,
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true,
          },
          video: {
            facingMode: { ideal: 'environment' },
          },
        };
        this.debug('getUserMedia with constraints', constraints);
        try {
          const stream = await navigator.mediaDevices.getUserMedia(constraints);
          const track = stream.getVideoTracks()[0];
          const settings = track?.getSettings?.() || {} as MediaTrackSettings;
          this.debug('Acquired stream video settings', settings);
          if (settings.facingMode !== 'environment') {
            // Try enumerate fallback to enforce back camera
            const devices = await navigator.mediaDevices.enumerateDevices();
            const back = devices.find(
              (d) => d.kind === 'videoinput' && /back|rear|environment/i.test(d.label),
            );
            if (back) {
              try {
                const enforced = await navigator.mediaDevices.getUserMedia({
                  audio: constraints.audio,
                  video: { deviceId: { exact: back.deviceId } },
                });
                this.debug('Enforced back camera via deviceId', back);
                stream.getTracks().forEach((t) => t.stop());
                return enforced;
              } catch {
                // fall through to original stream
              }
            }
          }
          return stream;
        } catch (e) {
          this.updateError('getUserMedia failed: ' + (e as Error).message);
          const devices = await navigator.mediaDevices.enumerateDevices();
          const back = devices.find(
            (d) => d.kind === 'videoinput' && /back|rear|environment/i.test(d.label),
          );
          if (back) {
            return await navigator.mediaDevices.getUserMedia({
              audio: constraints.audio,
              video: { deviceId: { exact: back.deviceId } },
            });
          }
          throw e;
        }
      };

      this.mediaStream = await preferBackCamera();

      this.videoElement.srcObject = this.mediaStream;
      try {
        await this.videoElement.play();
      } catch (e) {
        console.warn('video.play() failed', e);
      }

      const vt = this.mediaStream.getVideoTracks()[0];
      const at = this.mediaStream.getAudioTracks()[0];
      this.updateStatus('Media access granted. v=' + (!!vt) + ' a=' + (!!at));

      this.sourceNode = this.inputAudioContext.createMediaStreamSource(
        this.mediaStream,
      );
      this.sourceNode.connect(this.inputNode);

      const bufferSize = 4096;
      this.scriptProcessorNode = this.inputAudioContext.createScriptProcessor(
        bufferSize,
        1,
        1,
      );

      this.scriptProcessorNode.onaudioprocess = (audioProcessingEvent) => {
        if (!this.isRecording) return;

        const inputBuffer = audioProcessingEvent.inputBuffer;
        const pcmData = inputBuffer.getChannelData(0);
        const srcRate = this.inputAudioContext.sampleRate;
        const downsampleTo16k = (float32: Float32Array, inRate: number) => {
          if (inRate === 16000) return float32;
          const ratio = inRate / 16000;
          const newLen = Math.floor(float32.length / ratio);
          const result = new Float32Array(newLen);
          let idx = 0, pos = 0;
          while (idx < newLen) {
            const nextPos = Math.min(float32.length - 1, (idx + 1) * ratio);
            let sum = 0, count = 0;
            for (let i = pos; i < nextPos; i++) { sum += float32[i]; count++; }
            result[idx++] = sum / Math.max(1, count);
            pos = nextPos;
          }
          return result;
        };
        const mono16k = srcRate === 16000 ? pcmData : downsampleTo16k(pcmData, srcRate);

        this.sessionPromise.then((session) => {
          session.sendRealtimeInput({media: createBlob(mono16k)});
        }).catch((e) => this.updateError('Session send failed: ' + e.message));
      };

      this.sourceNode.connect(this.scriptProcessorNode);
      this.scriptProcessorNode.connect(this.inputAudioContext.destination);

      this.frameInterval = window.setInterval(() => {
        if (!this.isRecording) return;
        const context = this.canvasElement.getContext('2d');
        this.canvasElement.width = this.videoElement.videoWidth;
        this.canvasElement.height = this.videoElement.videoHeight;
        context.drawImage(
          this.videoElement,
          0,
          0,
          this.canvasElement.width,
          this.canvasElement.height,
        );
        this.canvasElement.toBlob(
          async (blob) => {
            if (blob) {
              const base64Data = await this.blobToBase64(blob);
              this.sessionPromise
                .then((session) => {
                  session.sendRealtimeInput({
                    media: {data: base64Data, mimeType: 'image/jpeg'},
                  });
                  this.framesSent++;
                  if (this.framesSent % 10 === 0) {
                    this.debug(`Frames sent: ${this.framesSent}`);
                  }
                })
                .catch((e) =>
                  this.updateError('Session send (image) failed: ' + e.message),
                );
            }
          },
          'image/jpeg',
          0.8,
        );
      }, 500); // 2 frames per second

      this.isRecording = true;
      this.updateStatus('');
    } catch (err) {
      console.error('Error starting recording:', err);
      this.updateStatus(`Error: ${err.message}`);
      this.stopRecording();
    }
  }

  private stopRecording() {
    if (!this.isRecording && !this.mediaStream && !this.inputAudioContext)
      return;

    this.updateStatus('');

    this.isRecording = false;

    if (this.frameInterval) {
      clearInterval(this.frameInterval);
      this.frameInterval = null;
    }

    if (this.scriptProcessorNode && this.sourceNode && this.inputAudioContext) {
      this.scriptProcessorNode.disconnect();
      this.sourceNode.disconnect();
    }

    this.scriptProcessorNode = null;
    this.sourceNode = null;

    if (this.mediaStream) {
      this.mediaStream.getTracks().forEach((track) => track.stop());
      this.mediaStream = null;
    }

    if (this.videoElement) {
      this.videoElement.srcObject = null;
    }

    this.updateStatus('');
  }

  private reset() {
    this.sessionPromise?.then((session) => session.close());
    this.sessionPromise = this.initSession();
    this.updateStatus('');
  }

  private async handleSpeakTap() {
    if (this.isRecording) {
      this.stopRecording();
      return;
    }

    try {
      await this.startRecording();
    } catch (err) {
      console.error('Error handling speak tap:', err);
    }
  }

  render() {
    return html`
      <div class="layout">
        <video id="video-background" autoplay muted playsinline></video>
        <canvas id="frame-canvas" style="display:none"></canvas>
        ${this.error
          ? html`<div class="error-banner" role="alert">${this.error}</div>`
          : null}
        <div class="speak-area">
          <gdm-live-audio-visuals-3d
            .inputNode=${this.inputNode}
            .outputNode=${this.outputNode}></gdm-live-audio-visuals-3d>
          <button
            class="speak-button"
            type="button"
            aria-pressed=${this.isRecording}
            aria-label=${this.isRecording ? 'Stop speaking' : 'Tap to speak'}
            @click=${this.handleSpeakTap}>
            <span class="speak-label">Tap to Speak</span>
          </button>
        </div>
      </div>
    `;
  }
}
```

```typescript
/**
 * @license
 * SPDX-License-Identifier: Apache-2.0
 */

// tslint:disable:organize-imports
// tslint:disable:ban-malformed-import-paths
// tslint:disable:no-new-decorators

import {LitElement, css, html} from 'lit';
import {customElement, property} from 'lit/decorators.js';
import {Analyser} from './analyser';

import * as THREE from 'three';
import {EXRLoader} from 'three/addons/loaders/EXRLoader.js';
import {EffectComposer} from 'three/addons/postprocessing/EffectComposer.js';
import {RenderPass} from 'three/addons/postprocessing/RenderPass.js';
import {ShaderPass} from 'three/addons/postprocessing/ShaderPass.js';
import {UnrealBloomPass} from 'three/addons/postprocessing/UnrealBloomPass.js';
import {FXAAShader} from 'three/addons/shaders/FXAAShader.js';
import {fs as backdropFS, vs as backdropVS} from './backdrop-shader';
import {vs as sphereVS} from './sphere-shader';

/**
 * 3D live audio visual.
 */
@customElement('gdm-live-audio-visuals-3d')
export class GdmLiveAudioVisuals3D extends LitElement {
  private inputAnalyser!: Analyser;
  private outputAnalyser!: Analyser;
  private camera!: THREE.PerspectiveCamera;
  private backdrop!: THREE.Mesh;
  private composer!: EffectComposer;
  private sphere!: THREE.Mesh;
  private prevTime = 0;
  private rotation = new THREE.Vector3(0, 0, 0);

  private _outputNode!: AudioNode;

  @property()
  set outputNode(node: AudioNode) {
    this._outputNode = node;
    this.outputAnalyser = new Analyser(this._outputNode);
  }

  get outputNode() {
    return this._outputNode;
  }

  private _inputNode!: AudioNode;

  @property()
  set inputNode(node: AudioNode) {
    this._inputNode = node;
    this.inputAnalyser = new Analyser(this._inputNode);
  }

  get inputNode() {
    return this._inputNode;
  }

  private canvas!: HTMLCanvasElement;

  static styles = css`
    canvas {
      width: 100% !important;
      height: 100% !important;
      position: absolute;
      inset: 0;
      image-rendering: pixelated;
    }
  `;

  connectedCallback() {
    super.connectedCallback();
  }

  private init() {
    const scene = new THREE.Scene();
    scene.background = new THREE.Color(0x100c14);

    const backdrop = new THREE.Mesh(
      new THREE.IcosahedronGeometry(10, 5),
      new THREE.RawShaderMaterial({
        uniforms: {
          resolution: {value: new THREE.Vector2(1, 1)},
          rand: {value: 0},
        },
        vertexShader: backdropVS,
        fragmentShader: backdropFS,
        glslVersion: THREE.GLSL3,
      }),
    );
    backdrop.material.side = THREE.BackSide;
    scene.add(backdrop);
    this.backdrop = backdrop;

    const camera = new THREE.PerspectiveCamera(
      75,
      window.innerWidth / window.innerHeight,
      0.1,
      1000,
    );
    camera.position.set(1.8, -1.8, 3.6);
    this.camera = camera;

    const renderer = new THREE.WebGLRenderer({
      canvas: this.canvas,
      antialias: !true,
    });
    renderer.setSize(window.innerWidth, window.innerHeight);
    renderer.setPixelRatio(window.devicePixelRatio / 1);

    const geometry = new THREE.IcosahedronGeometry(1, 10);

    new EXRLoader().load('piz_compressed.exr', (texture: THREE.Texture) => {
      texture.mapping = THREE.EquirectangularReflectionMapping;
      const exrCubeRenderTarget = pmremGenerator.fromEquirectangular(texture);
      sphereMaterial.envMap = exrCubeRenderTarget.texture;
      sphere.visible = true;
    });

    const pmremGenerator = new THREE.PMREMGenerator(renderer);
    pmremGenerator.compileEquirectangularShader();

    const sphereMaterial = new THREE.MeshStandardMaterial({
      color: 0x000010,
      metalness: 0.5,
      roughness: 0.1,
      emissive: 0x000010,
      emissiveIntensity: 1.5,
    });

    sphereMaterial.onBeforeCompile = (shader) => {
      shader.uniforms.time = {value: 0};
      shader.uniforms.inputData = {value: new THREE.Vector4()};
      shader.uniforms.outputData = {value: new THREE.Vector4()};

      sphereMaterial.userData.shader = shader;

      shader.vertexShader = sphereVS;
    };

    const sphere = new THREE.Mesh(geometry, sphereMaterial);
    scene.add(sphere);
    sphere.visible = false;

    this.sphere = sphere;

    const renderPass = new RenderPass(scene, camera);

    const bloomPass = new UnrealBloomPass(
      new THREE.Vector2(window.innerWidth, window.innerHeight),
      5,
      0.5,
      0,
    );

    const fxaaPass = new ShaderPass(FXAAShader);

    const composer = new EffectComposer(renderer);
    composer.addPass(renderPass);
    // composer.addPass(fxaaPass);
    composer.addPass(bloomPass);

    this.composer = composer;

    const onWindowResize = () => {
      if (!this.canvas) return;
      const w = this.canvas.clientWidth;
      const h = this.canvas.clientHeight;

      camera.aspect = w / h;
      camera.updateProjectionMatrix();

      const dPR = renderer.getPixelRatio();
      backdrop.material.uniforms.resolution.value.set(w * dPR, h * dPR);

      renderer.setSize(w, h);
      composer.setSize(w, h);

      fxaaPass.material.uniforms['resolution'].value.set(
        1 / (w * dPR),
        1 / (h * dPR),
      );
    };

    window.addEventListener('resize', onWindowResize);
    onWindowResize();

    this.animation();
  }

  private animation() {
    requestAnimationFrame(() => this.animation());

    this.inputAnalyser.update();
    this.outputAnalyser.update();

    const t = performance.now();
    const dt = (t - this.prevTime) / (1000 / 60);
    this.prevTime = t;
    const backdropMaterial = this.backdrop.material as THREE.RawShaderMaterial;
    const sphereMaterial = this.sphere.material as THREE.MeshStandardMaterial;

    backdropMaterial.uniforms.rand.value = Math.random() * 10000;

    if (sphereMaterial.userData.shader) {
      this.sphere.scale.setScalar(
        1.35 + (0.25 * this.outputAnalyser.data[1]) / 255,
      );

      const f = 0.001;
      this.rotation.x += (dt * f * 0.5 * this.outputAnalyser.data[1]) / 255;
      this.rotation.z += (dt * f * 0.5 * this.inputAnalyser.data[1]) / 255;
      this.rotation.y += (dt * f * 0.25 * this.inputAnalyser.data[2]) / 255;
      this.rotation.y += (dt * f * 0.25 * this.outputAnalyser.data[2]) / 255;

      const euler = new THREE.Euler(
        this.rotation.x,
        this.rotation.y,
        this.rotation.z,
      );
      const quaternion = new THREE.Quaternion().setFromEuler(euler);
      const vector = new THREE.Vector3(0, 0, 3.6);
      vector.applyQuaternion(quaternion);
      this.camera.position.copy(vector);
      this.camera.lookAt(this.sphere.position);

      sphereMaterial.userData.shader.uniforms.time.value +=
        (dt * 0.1 * this.outputAnalyser.data[0]) / 255;
      sphereMaterial.userData.shader.uniforms.inputData.value.set(
        (1 * this.inputAnalyser.data[0]) / 255,
        (0.1 * this.inputAnalyser.data[1]) / 255,
        (10 * this.inputAnalyser.data[2]) / 255,
        0,
      );
      sphereMaterial.userData.shader.uniforms.outputData.value.set(
        (2 * this.outputAnalyser.data[0]) / 255,
        (0.1 * this.outputAnalyser.data[1]) / 255,
        (10 * this.outputAnalyser.data[2]) / 255,
        0,
      );
    }

    this.composer.render();
  }

  protected firstUpdated() {
    this.canvas = this.shadowRoot!.querySelector('canvas') as HTMLCanvasElement;
    this.init();
  }

  protected render() {
    return html`<canvas></canvas>`;
  }
}

declare global {
  interface HTMLElementTagNameMap {
    'gdm-live-audio-visuals-3d': GdmLiveAudioVisuals3D;
  }
}
```

