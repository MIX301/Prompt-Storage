# Debugging PWA mic and camera access
_Exported on 15/11/2025 at 3:00:13 CET from Cursor (1.7.44)_

---

**User**

I am building a PWA. Help me debug why my app wont accept the mic and camera access as a PWA (when added to my home page on iPhone), but it works flawlessly in Safari. I have a button that triggers the popup, but clicking accept works 50% of the time. 

Read the @DOCUMENTATION.md for more context. 

---

**Cursor**

### Plan (pseudocode-style)

```
1. Inspect `index.tsx`
   1.1 Locate handler wired to “Tap to Speak” button.
   1.2 Follow call chain to media-permission request logic.
   1.3 Note any async gaps between user gesture and first `getUserMedia()` or `AudioContext.resume()` call.

2. Review permission / session utilities
   2.1 Check if `navigator.permissions.query` is invoked before the user gesture.
   2.2 Confirm media tracks are properly stopped on teardown to avoid `NotReadableError` when reopening.

3. Compare Safari vs PWA runtime constraints
   3.1 Pin down any PWA-only code paths (e.g., display mode detection, service worker hooks).
   3.2 Verify HTTPS certificate / origin consistency for installed mode.

4. Cross-check known iOS PWA quirks
   4.1 Identify spots requiring direct user interaction (AudioContext resume, getUserMedia).
   4.2 Look for mitigations: preflight permission prompts, one-shot gestures, error retries.

5. Formulate fixes / suggestions
   5.1 Ensure everything runs inside a single synchronous tap handler (use `void button.onclick = async (...) => { await resumeAudio(); await getUserMedia(); }`).
   5.2 Add defensive teardown and retry logic tailored to iOS PWA.
   5.3 Provide testing steps to validate on-device, including logging.
```

Please confirm this approach (or adjust it), and I’ll dive in.

```plaintext
1. Inspect `index.tsx`
   1.1 Locate handler wired to “Tap to Speak” button.
   1.2 Follow call chain to media-permission request logic.
   1.3 Note any async gaps between user gesture and first `getUserMedia()` or `AudioContext.resume()` call.

2. Review permission / session utilities
   2.1 Check if `navigator.permissions.query` is invoked before the user gesture.
   2.2 Confirm media tracks are properly stopped on teardown to avoid `NotReadableError` when reopening.

3. Compare Safari vs PWA runtime constraints
   3.1 Pin down any PWA-only code paths (e.g., display mode detection, service worker hooks).
   3.2 Verify HTTPS certificate / origin consistency for installed mode.

4. Cross-check known iOS PWA quirks
   4.1 Identify spots requiring direct user interaction (AudioContext resume, getUserMedia).
   4.2 Look for mitigations: preflight permission prompts, one-shot gestures, error retries.

5. Formulate fixes / suggestions
   5.1 Ensure everything runs inside a single synchronous tap handler (use `void button.onclick = async (...) => { await resumeAudio(); await getUserMedia(); }`).
   5.2 Add defensive teardown and retry logic tailored to iOS PWA.
   5.3 Provide testing steps to validate on-device, including logging.
```

---

**User**

Proceed

---

**Cursor**

## Findings

- In `startRecording` the code resumes both audio contexts before the `getUserMedia` call (`await this.inputAudioContext.resume(); await this.outputAudioContext.resume();`). On iOS the WKWebView that backs installed PWAs is stricter about “user activation”: the first awaited promise inside the tap handler consumes the gesture, so by the time `getUserMedia` runs, the browser sometimes treats it as coming from outside the gesture and returns `NotAllowedError`, which matches the “works ~50%” behavior when the contexts happen to already be running and `resume()` resolves synchronously.  
```581:616:index.tsx
    await this.inputAudioContext.resume();
    await this.outputAudioContext.resume();
    this.updateStatus('Requesting camera/mic access...');
    try {
      const preferBackCamera = async (): Promise<MediaStream> => {
        const constraints: MediaStreamConstraints = {
          audio: {
            channelCount: 1,
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true,
          },
          video: {
            facingMode: { ideal: 'environment' },
          },
        };
        this.debug('getUserMedia with constraints', constraints);
        try {
          const stream = await navigator.mediaDevices.getUserMedia(constraints);
```
- The same pattern appears later in `startRecordingWithStream`, so even if the initial session succeeds, retrying after the app is suspended/backgrounded will hit the same user-activation wall when the contexts auto-suspend.  
```430:437:index.tsx
    if (this.inputAudioContext.state !== 'running') {
      await this.inputAudioContext.resume();
    }
    if (this.outputAudioContext.state !== 'running') {
      await this.outputAudioContext.resume();
    }
```
- Because you also call the flow from `touchstart` with `preventDefault`, there is no follow-up `click` event that would grant a fresh activation if the first promise consumed it. Safari’s tab view is more forgiving, which is why the bug only shows up in standalone mode.

## Recommendations

- Re-order the gesture handler so that `navigator.mediaDevices.getUserMedia` runs immediately, and only then `await` the audio context resumes. A common pattern is:
  1. Capture timestamps / log the gesture.
  2. Kick off the `resume()` calls without awaiting them yet (`const resumeInput = this.inputAudioContext.state !== 'running' ? this.inputAudioContext.resume() : Promise.resolve();`).
  3. Call `getUserMedia` right away.
  4. `await Promise.all([resumeInput, resumeOutput]);` after the stream promise resolves.
  5. Proceed with `startRecordingWithStream`.
- Apply the same ordering inside `startRecordingWithStream`: if the contexts are still suspended after the earlier resume attempt, use the already-created promises rather than issuing fresh `await` calls that again need user activation.
- Add logging of `err.name`/`err.message` to `updateError` so you can confirm whether the failure path is `NotAllowedError` or `NotReadableError`.
- Test in an installed build with Safari’s Web Inspector attached (`Develop > [device] > [app name]`) to verify the promises resolve in the expected order.

## Next Steps

- Rework the two spots above to defer awaiting `AudioContext.resume()` until after `getUserMedia`.
- Capture the exact exception names in the catch blocks while testing on-device.
- Once the flow is stable, rerun manual tests: fresh install, background/resume, lock/unlock, and repeated hold/release cycles.

```typescriptreact
    await this.inputAudioContext.resume();
    await this.outputAudioContext.resume();
    this.updateStatus('Requesting camera/mic access...');
    try {
      const preferBackCamera = async (): Promise<MediaStream> => {
        const constraints: MediaStreamConstraints = {
          audio: {
            channelCount: 1,
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true,
          },
          video: {
            facingMode: { ideal: 'environment' },
          },
        };
        this.debug('getUserMedia with constraints', constraints);
        try {
          const stream = await navigator.mediaDevices.getUserMedia(constraints);
```

```typescriptreact
    if (this.inputAudioContext.state !== 'running') {
      await this.inputAudioContext.resume();
    }
    if (this.outputAudioContext.state !== 'running') {
      await this.outputAudioContext.resume();
    }
```

---

**User**

Implement the recommended changes

---

**Cursor**

```typescriptreact
/* tslint:disable */
/**
 * @license
 * SPDX-License-Identifier: Apache-2.0
 */

import {GoogleGenAI, LiveServerMessage, Modality, Session} from '@google/genai';
import {LitElement, css, html} from 'lit';
import {customElement, state} from 'lit/decorators.js';
import {createBlob, decode, decodeAudioData} from './utils';
import './visual-3d';
// PWA: register service worker if available
try {
  // dynamic import optional; vite-plugin-pwa injects virtual module
  // eslint-disable-next-line @typescript-eslint/ban-ts-GB
  // @ts-ignore
  import('virtual:pwa-register').then(({ registerSW }) => {
    registerSW({ immediate: true });
  }).catch(() => {});
} catch {}

@customElement('gdm-live-audio')
export class GdmLiveAudio extends LitElement {
  @state() isRecording = false;
  @state() isHolding = false;
  @state() status = '';
  @state() error = '';
  @state() appStopped = false;
  @state() sessionStarted = false;

  private client: GoogleGenAI;
  private sessionPromise?: Promise<Session>;
  // Audio contexts created on user gesture for iOS PWA compatibility
  private inputAudioContext?: AudioContext;
  private outputAudioContext?: AudioContext;
  @state() inputNode?: GainNode;
  @state() outputNode?: GainNode;
  private nextStartTime = 0;
  private mediaStream: MediaStream;
  private sourceNode: AudioNode;
  private scriptProcessorNode: ScriptProcessorNode;
  private sources = new Set<AudioBufferSourceNode>();
  private frameInterval: number;
  private videoElement: HTMLVideoElement;
  private canvasElement: HTMLCanvasElement;
  private framesSent = 0;

  static styles = css`
    :host {
      display: block;
      position: relative;
      width: 100vw;
      height: 100vh;
      min-height: 100dvh;
      background: #000000;
      color: #ffffff;
      overflow: hidden;
    }

    @supports (height: 100dvh) {
      :host {
        height: 100dvh;
      }
    }

    .layout {
      position: relative;
      width: 100%;
      height: 100%;
    }

    #video-background {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 66.667vh;
      object-fit: cover;
    }

    .speak-area {
      position: absolute;
      bottom: 0;
      left: 0;
      width: 100%;
      height: calc(33.333vh + env(safe-area-inset-bottom));
      border-top-left-radius: 32px;
      border-top-right-radius: 32px;
      border: 1px solid rgba(255, 255, 255, 0.08);
      border-bottom: none;
      overflow: hidden;
      background: rgba(0, 0, 0, 0.3);
      backdrop-filter: blur(6px);
    }

    .session-overlay {
      position: absolute;
      inset: 0;
      display: flex;
      align-items: center;
      justify-content: center;
      padding: 24px;
      font-size: 18px;
      letter-spacing: 0.1em;
      text-transform: uppercase;
      color: rgba(255, 255, 255, 0.92);
      background: linear-gradient(
        180deg,
        rgba(17, 24, 39, 0.6) 0%,
        rgba(15, 23, 42, 0.85) 100%
      );
      pointer-events: none;
      text-align: center;
    }

    gdm-live-audio-visuals-3d {
      display: block;
      width: 100%;
      height: 100%;
      pointer-events: none;
      border-top-left-radius: inherit;
      border-top-right-radius: inherit;
    }

    .speak-button {
      position: absolute;
      inset: 0;
      width: 100%;
      height: 100%;
      border: none;
      background: transparent;
      cursor: pointer;
      display: flex;
      align-items: flex-end;
      justify-content: center;
      padding: 24px 24px calc(24px + env(safe-area-inset-bottom));
      color: inherit;
      transition: background 0.2s ease;
      -webkit-touch-callout: none;
      -webkit-user-select: none;
      -moz-user-select: none;
      -ms-user-select: none;
      user-select: none;
      -webkit-tap-highlight-color: transparent;
      touch-action: manipulation;
    }

    .speak-button:hover,
    .speak-button:focus-visible {
      background: rgba(255, 255, 255, 0.04);
    }

    .speak-button:focus-visible {
      outline: 2px solid rgba(255, 255, 255, 0.25);
      outline-offset: 4px;
    }

    .speak-label {
      font-size: 18px;
      letter-spacing: 0.08em;
      text-transform: uppercase;
      color: rgba(255, 255, 255, 0.92);
      text-shadow: 0 0 6px rgba(0, 0, 0, 0.6);
      -webkit-touch-callout: none;
      -webkit-user-select: none;
      -moz-user-select: none;
      -ms-user-select: none;
      user-select: none;
      pointer-events: none;
    }

    .error-banner {
      position: absolute;
      top: calc(env(safe-area-inset-top) + 12px);
      left: 16px;
      right: 16px;
      z-index: 10;
      padding: 12px 16px;
      background: rgba(220, 38, 38, 0.85);
      color: #ffffff;
      border-radius: 12px;
      text-align: center;
      font-size: 14px;
      line-height: 1.4;
      box-shadow: 0 12px 24px rgba(0, 0, 0, 0.35);
      backdrop-filter: blur(12px);
    }

    .start-button {
      position: absolute;
      top: 50%;
      left: 50%;
      transform: translate(-50%, -50%);
      z-index: 1000;
      padding: 20px 40px;
      background: rgba(255, 255, 255, 0.25);
      color: #ffffff;
      border: 2px solid rgba(255, 255, 255, 0.5);
      border-radius: 16px;
      font-size: 18px;
      font-weight: 500;
      letter-spacing: 0.05em;
      text-transform: uppercase;
      cursor: pointer;
      transition: all 0.2s ease;
      backdrop-filter: blur(12px);
      box-shadow: 0 8px 24px rgba(0, 0, 0, 0.6), 0 0 0 1px rgba(255, 255, 255, 0.1);
      -webkit-tap-highlight-color: transparent;
      touch-action: manipulation;
      white-space: nowrap;
    }

    .start-button:hover,
    .start-button:focus-visible {
      background: rgba(255, 255, 255, 0.25);
      border-color: rgba(255, 255, 255, 0.5);
      transform: translate(-50%, -50%) scale(1.05);
    }

    .start-button:active {
      transform: translate(-50%, -50%) scale(0.98);
    }

    .start-button:focus-visible {
      outline: 2px solid rgba(255, 255, 255, 0.5);
      outline-offset: 4px;
    }
  `;

  constructor() {
    super();
    this.initClient();
  }

  protected firstUpdated() {
    this.videoElement = this.shadowRoot!.querySelector('#video-background');
    this.canvasElement = this.shadowRoot!.querySelector('#frame-canvas');
    this.debug(
      `Boot: secure=${window.isSecureContext} ua=${navigator.userAgent}`,
    );
    this.checkPermissions();
    this.addEventListener('click', async () => {
      try {
        if (this.outputAudioContext) {
          await this.outputAudioContext.resume();
        }
        if (this.inputAudioContext) {
          await this.inputAudioContext.resume();
        }
      } catch {}
    });
  }

  private async checkPermissions() {
    const parts: string[] = [];
    const anyNav: any = navigator as any;
    if (anyNav.permissions?.query) {
      try {
        const mic = await anyNav.permissions.query({name: 'microphone'});
        parts.push(`mic=${mic.state}`);
      } catch {}
      try {
        const cam = await anyNav.permissions.query({name: 'camera'});
        parts.push(`cam=${cam.state}`);
      } catch {}
    }
    if (parts.length) this.updateStatus(`Permissions: ${parts.join(' ')}`);
  }

  private debug(_msg: string, _data?: unknown) {
    // Debug logging disabled for production UI clarity.
  }

  private initAudio() {
    if (this.outputAudioContext && this.inputNode) {
      this.nextStartTime = this.outputAudioContext.currentTime;
      this.inputNode.gain.value = 0.0; // Start with mic muted
    }
  }

  private async initClient() {
    if (!process.env.API_KEY || process.env.API_KEY === 'undefined') {
      this.updateError('Missing GEMINI_API_KEY');
    }

    this.client = new GoogleGenAI({apiKey: process.env.API_KEY});

    // Audio contexts and nodes will be created on user gesture
    // Session will be initialized when user starts session
  }

  private initSession(): Promise<Session> {
    const model = 'gemini-2.5-flash-native-audio-preview-09-2025';

    try {
      return this.client.live.connect({
        model: model,
        callbacks: {
          onopen: () => {
            this.debug('Live session opened');
          },
          onmessage: async (message: LiveServerMessage) => {
            this.debug('Live message received');
            const audio =
              message.serverContent?.modelTurn?.parts[0]?.inlineData;

            if (audio && this.outputAudioContext && this.outputNode) {
              this.nextStartTime = Math.max(
                this.nextStartTime,
                this.outputAudioContext.currentTime,
              );

              const audioBuffer = await decodeAudioData(
                decode(audio.data),
                this.outputAudioContext,
                24000,
                1,
              );
              const source = this.outputAudioContext.createBufferSource();
              source.buffer = audioBuffer;
              source.connect(this.outputNode);
              source.addEventListener('ended', () => {
                this.sources.delete(source);
              });

              source.start(this.nextStartTime);
              this.nextStartTime = this.nextStartTime + audioBuffer.duration;
              this.sources.add(source);
            }

            const interrupted = message.serverContent?.interrupted;
            if (interrupted) {
              for (const source of this.sources.values()) {
                source.stop();
                this.sources.delete(source);
              }
              this.nextStartTime = 0;
            }
          },
          onerror: (e: ErrorEvent) => {
            this.updateError('Live error: ' + e.message);
            console.error('Live error event', e);
          },
          onclose: (e: CloseEvent) => {
            this.updateStatus('Live close: ' + e.reason + ' (' + e.code + ')');
            console.warn('Live close', e);
          },
        },
        config: {
          responseModalities: [Modality.AUDIO],
          systemInstruction: {
            parts: [
              {
                text: `You are an AI assistant integrated into an application where the main interface is a live camera view. The user interacts with you by pressing and holding a large button at the bottom of the screen. When the button is active, you listen to the user's voice input and provide an answer based on what is visible through the camera.

Your task is to analyze the live camera feed and interpret the user’s spoken request in combination. Always respond with precise, spatially grounded instructions that reference items or locations visible on the screen.

When giving instructions:

Be specific and concise.

Refer to locations using clear spatial language (e.g., “third shelf from the top,” “top right corner,” “behind the red box,” etc.).

Only describe what is visible on the camera or can be reasonably inferred from it.

Do not invent objects or locations that are not present in the scene.

Keep answers straightforward and helpful.

Example:
If the user asks, “Where is the gluten-free bread?” and it is visible through the camera, respond with a description such as:
“The gluten-free bread is on the third shelf from the top, in the top right corner.”

Your goal is to help the user locate objects in the environment accurately and efficiently based on real-time visual input.`,
              },
            ],
          },
          speechConfig: {
            voiceConfig: {prebuiltVoiceConfig: {voiceName: 'Zephyr'}},
            // languageCode: 'en-GB'
          },
        },
      });
    } catch (e) {
      console.error(e);
      return Promise.reject(e);
    }
  }

  private updateStatus(msg: string) {
    this.status = msg;
  }

  private updateError(msg: string) {
    this.error = msg;
  }

  private blobToBase64(blob: Blob): Promise<string> {
    return new Promise((resolve, reject) => {
      const reader = new FileReader();
      reader.onloadend = () => {
        if (typeof reader.result === 'string') {
          resolve(reader.result.split(',')[1]);
        } else {
          reject(new Error('Failed to read blob as base64 string.'));
        }
      };
      reader.onerror = reject;
      reader.readAsDataURL(blob);
    });
  }

  private async startRecordingWithStream(stream: MediaStream) {
    if (this.isRecording) {
      stream.getTracks().forEach((t) => t.stop());
      return;
    }

    if (!this.inputAudioContext || !this.outputAudioContext || !this.inputNode || !this.outputNode) {
      throw new Error('Audio contexts not initialized');
    }

    if (!this.sessionPromise) {
      this.sessionPromise = this.initSession();
    }

    this.appStopped = false;
    this.framesSent = 0;

    this.mediaStream = stream;

    this.videoElement.srcObject = this.mediaStream;
    try {
      await this.videoElement.play();
    } catch (e) {
      console.warn('video.play() failed', e);
    }

    const vt = this.mediaStream.getVideoTracks()[0];
    const at = this.mediaStream.getAudioTracks()[0];
    this.updateStatus('Media access granted. v=' + (!!vt) + ' a=' + (!!at));

    // Ensure we have an audio track before proceeding
    if (!at) {
      throw new Error('No audio track available in media stream');
    }

    // Ensure audio track is enabled and ready
    if (!at.enabled) {
      at.enabled = true;
    }

    // Ensure audio track is ready - wait a bit if needed
    let trackReadyState = at.readyState;
    if (trackReadyState !== 'live') {
      // Wait for track to become live (max 1 second)
      let attempts = 0;
      while (trackReadyState !== 'live' && attempts < 10) {
        await new Promise(resolve => setTimeout(resolve, 100));
        trackReadyState = at.readyState;
        attempts++;
      }
      if (trackReadyState !== 'live') {
        throw new Error('Audio track is not live. State: ' + trackReadyState);
      }
    }

    // Create media stream source only after audio context is running
    if (this.inputAudioContext.state === 'running') {
      try {
        this.sourceNode = this.inputAudioContext.createMediaStreamSource(
          this.mediaStream,
        );
        this.sourceNode.connect(this.inputNode);
      } catch (e) {
        throw new Error('Failed to create media stream source: ' + (e as Error).message);
      }
    } else {
      throw new Error('Audio context is not running. State: ' + this.inputAudioContext.state);
    }

    const bufferSize = 4096;
    this.scriptProcessorNode = this.inputAudioContext.createScriptProcessor(
      bufferSize,
      1,
      1,
    );

    this.scriptProcessorNode.onaudioprocess = (audioProcessingEvent) => {
      if (!this.isRecording) return;
      if (!this.isHolding) return;

      const inputBuffer = audioProcessingEvent.inputBuffer;
      const pcmData = inputBuffer.getChannelData(0);
      const srcRate = this.inputAudioContext.sampleRate;
      const downsampleTo16k = (float32: Float32Array, inRate: number) => {
        if (inRate === 16000) return float32;
        const ratio = inRate / 16000;
        const newLen = Math.floor(float32.length / ratio);
        const result = new Float32Array(newLen);
        let idx = 0, pos = 0;
        while (idx < newLen) {
          const nextPos = Math.min(float32.length - 1, (idx + 1) * ratio);
          let sum = 0, count = 0;
          for (let i = pos; i < nextPos; i++) { sum += float32[i]; count++; }
          result[idx++] = sum / Math.max(1, count);
          pos = nextPos;
        }
        return result;
      };
      const mono16k = srcRate === 16000 ? pcmData : downsampleTo16k(pcmData, srcRate);

      const sessionPromise = this.sessionPromise;
      if (!sessionPromise) {
        return;
      }

      sessionPromise
        .then((session) => {
          session.sendRealtimeInput({media: createBlob(mono16k)});
        })
        .catch((e) => this.updateError('Session send failed: ' + e.message));
    };

    this.sourceNode.connect(this.scriptProcessorNode);
    this.scriptProcessorNode.connect(this.inputAudioContext.destination);

    this.frameInterval = window.setInterval(() => {
      if (!this.isRecording) return;
      const context = this.canvasElement.getContext('2d');
      this.canvasElement.width = this.videoElement.videoWidth;
      this.canvasElement.height = this.videoElement.videoHeight;
      context.drawImage(
        this.videoElement,
        0,
        0,
        this.canvasElement.width,
        this.canvasElement.height,
      );
      this.canvasElement.toBlob(
        async (blob) => {
          if (blob) {
            const base64Data = await this.blobToBase64(blob);
            const sessionPromise = this.sessionPromise;
            if (!sessionPromise) {
              return;
            }

            sessionPromise
              .then((session) => {
                session.sendRealtimeInput({
                  media: {data: base64Data, mimeType: 'image/jpeg'},
                });
                this.framesSent++;
                if (this.framesSent % 10 === 0) {
                  this.debug(`Frames sent: ${this.framesSent}`);
                }
              })
              .catch((e) =>
                this.updateError('Session send (image) failed: ' + e.message),
              );
          }
        },
        'image/jpeg',
        0.8,
      );
    }, 500); // 2 frames per second

    this.isRecording = true;
    this.updateStatus('');
  }

  private async startRecording() {
    if (this.isRecording) {
      return;
    }

    if (!this.inputAudioContext || !this.outputAudioContext) {
      throw new Error('Audio contexts not initialized');
    }

    const inputResumePromise =
      this.inputAudioContext.state === 'running'
        ? Promise.resolve()
        : this.inputAudioContext.resume();
    const outputResumePromise =
      this.outputAudioContext.state === 'running'
        ? Promise.resolve()
        : this.outputAudioContext.resume();

    this.updateStatus('Requesting camera/mic access...');

    try {
      const preferBackCamera = async (): Promise<MediaStream> => {
        const constraints: MediaStreamConstraints = {
          audio: {
            channelCount: 1,
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true,
          },
          video: {
            facingMode: { ideal: 'environment' },
          },
        };
        this.debug('getUserMedia with constraints', constraints);
        try {
          const stream = await navigator.mediaDevices.getUserMedia(constraints);
          const track = stream.getVideoTracks()[0];
          const settings = track?.getSettings?.() || {} as MediaTrackSettings;
          this.debug('Acquired stream video settings', settings);
          if (settings.facingMode !== 'environment') {
            // Try enumerate fallback to enforce back camera
            const devices = await navigator.mediaDevices.enumerateDevices();
            const back = devices.find(
              (d) => d.kind === 'videoinput' && /back|rear|environment/i.test(d.label),
            );
            if (back) {
              try {
                const enforced = await navigator.mediaDevices.getUserMedia({
                  audio: constraints.audio,
                  video: { deviceId: { exact: back.deviceId } },
                });
                this.debug('Enforced back camera via deviceId', back);
                stream.getTracks().forEach((t) => t.stop());
                return enforced;
              } catch {
                // fall through to original stream
              }
            }
          }
          return stream;
        } catch (e) {
          const err = e as DOMException;
          this.updateError(
            'getUserMedia failed: ' +
              (err?.name ?? 'Error') +
              ' - ' +
              (err?.message ?? err),
          );
          const devices = await navigator.mediaDevices.enumerateDevices();
          const back = devices.find(
            (d) => d.kind === 'videoinput' && /back|rear|environment/i.test(d.label),
          );
          if (back) {
            return await navigator.mediaDevices.getUserMedia({
              audio: constraints.audio,
              video: { deviceId: { exact: back.deviceId } },
            });
          }
          throw e;
        }
      };

      let stream: MediaStream | null = null;
      try {
        stream = await preferBackCamera();
        await Promise.all([inputResumePromise, outputResumePromise]);
        await this.startRecordingWithStream(stream);
      } catch (error) {
        if (stream) {
          stream.getTracks().forEach((t) => t.stop());
        }
        throw error;
      }
    } catch (err) {
      console.error('Error starting recording:', err);
      const error = err as DOMException;
      this.updateError(
        'Camera/mic activation failed: ' +
          (error?.name ?? 'Error') +
          ' - ' +
          (error?.message ?? error),
      );
      this.stopRecording();
    }
  }

  private stopRecording() {
    if (!this.isRecording && !this.mediaStream && !this.inputAudioContext)
      return;

    this.updateStatus('');

    this.isRecording = false;

    if (this.frameInterval) {
      clearInterval(this.frameInterval);
      this.frameInterval = null;
    }

    if (this.scriptProcessorNode && this.sourceNode && this.inputAudioContext) {
      this.scriptProcessorNode.disconnect();
      this.sourceNode.disconnect();
    }

    this.scriptProcessorNode = null;
    this.sourceNode = null;

    if (this.mediaStream) {
      this.mediaStream.getTracks().forEach((track) => track.stop());
      this.mediaStream = null;
    }

    if (this.videoElement) {
      this.videoElement.srcObject = null;
    }

    this.framesSent = 0;
    this.updateStatus('');
  }

  private stopApplication() {
    if (this.appStopped) {
      return;
    }

    this.stopRecording();

    for (const source of this.sources.values()) {
      try {
        source.stop();
      } catch {}
      this.sources.delete(source);
    }

    const sessionPromise = this.sessionPromise;
    this.sessionPromise = undefined;

    sessionPromise
      ?.then((session) => session.close())
      .catch((e) => this.updateError('Session close failed: ' + e.message));

    this.videoElement?.pause();
    if (this.videoElement) {
      this.videoElement.srcObject = null;
    }

    this.appStopped = true;
    this.updateStatus('Session ended');
  }

  private async restartApplication() {
    if (this.isRecording) {
      return;
    }

    this.updateStatus('Restarting session...');
    this.sessionPromise = this.initSession();
    this.appStopped = false;

    try {
      await this.startRecording();
    } catch (error) {
      this.updateError('Restart failed: ' + (error as Error).message);
    }
  }

  private reset() {
    this.sessionPromise?.then((session) => session.close());
    this.sessionPromise = this.initSession();
    this.updateStatus('');
  }

  private async handleStartSession() {
    if (this.sessionStarted) {
      return;
    }
    
    this.sessionStarted = true;
    this.updateStatus('Requesting camera/mic access...');
    
    let stream: MediaStream | null = null;
    
    try {
      // Create audio contexts within user gesture for iOS PWA compatibility
      // This MUST be done synchronously from the event handler
      if (!this.inputAudioContext) {
        this.inputAudioContext = new (window.AudioContext ||
          (window as any).webkitAudioContext)({sampleRate: 16000});
      }
      if (!this.outputAudioContext) {
        this.outputAudioContext = new (window.AudioContext ||
          (window as any).webkitAudioContext)({sampleRate: 24000});
      }
      
      // Create gain nodes if they don't exist
      if (!this.inputNode) {
        this.inputNode = this.inputAudioContext.createGain();
        this.inputNode.gain.value = 0.0; // Start with mic muted
      }
      if (!this.outputNode) {
        this.outputNode = this.outputAudioContext.createGain();
        this.outputNode.connect(this.outputAudioContext.destination);
      }
      
      // Initialize session if not already done
      if (!this.sessionPromise) {
        this.sessionPromise = this.initSession();
      }
      
      // Request permissions directly from user gesture handler
      // This MUST be called synchronously from the event handler on iOS
      const constraints: MediaStreamConstraints = {
        audio: {
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
        },
        video: {
          facingMode: { ideal: 'environment' },
        },
      };
      
      // Call getUserMedia immediately to preserve user gesture context
      stream = await navigator.mediaDevices.getUserMedia(constraints);
      
      // Ensure audio contexts are running (they should be already, but check)
      if (this.inputAudioContext.state !== 'running') {
        await this.inputAudioContext.resume();
      }
      if (this.outputAudioContext.state !== 'running') {
        await this.outputAudioContext.resume();
      }
      
      // Small delay to ensure audio contexts are fully ready
      await new Promise(resolve => setTimeout(resolve, 100));
      
      // Initialize audio timing
      this.initAudio();
      
      // Now start recording with the stream
      await this.startRecordingWithStream(stream);
    } catch (err) {
      console.error('Error starting session:', err);
      const errorMessage = err instanceof Error ? err.message : String(err);
      this.updateError('Failed to access camera/microphone: ' + errorMessage);
      this.sessionStarted = false; // Allow retry on error
      
      // Clean up stream if it was created
      if (stream) {
        stream.getTracks().forEach((t) => t.stop());
      }
    }
  }

  private async handleHoldStart() {
    if (!this.isRecording) {
      if (this.appStopped) {
        await this.restartApplication();
      } else {
        try {
          await this.startRecording();
        } catch (err) {
          console.error('Error starting recording:', err);
          return;
        }
      }
    }
    
    this.isHolding = true;
    if (this.inputNode) {
      this.inputNode.gain.value = 1.0;
    }
  }

  private handleHoldEnd() {
    this.isHolding = false;
    if (this.inputNode) {
      this.inputNode.gain.value = 0.0;
    }
  }

  render() {
    return html`
      <div class="layout">
        <video
          id="video-background"
          autoplay
          muted
          playsinline
          ?hidden=${this.appStopped}></video>
        <canvas id="frame-canvas" style="display:none"></canvas>
        ${this.error
          ? html`<div class="error-banner" role="alert">${this.error}</div>`
          : null}
        ${!this.sessionStarted
          ? html`
              <button
                class="start-button"
                type="button"
                aria-label="Start Session"
                @click=${this.handleStartSession}
                @touchstart=${(e: TouchEvent) => {
                  e.preventDefault();
                  this.handleStartSession();
                }}>
                Start Session
              </button>
            `
          : null}
        <div class="speak-area">
          ${this.inputNode && this.outputNode
            ? html`<gdm-live-audio-visuals-3d
                .inputNode=${this.inputNode}
                .outputNode=${this.outputNode}></gdm-live-audio-visuals-3d>`
            : null}
          ${this.appStopped
            ? html`<div class="session-overlay">Session Ended</div>`
            : null}
          <button
            class="speak-button"
            type="button"
            aria-pressed=${this.isHolding}
            aria-label=${this.isHolding ? 'Release to mute' : 'Hold to speak'}
            @touchstart=${(e: TouchEvent) => {
              e.preventDefault();
              this.handleHoldStart();
            }}
            @touchend=${(e: TouchEvent) => {
              e.preventDefault();
              this.handleHoldEnd();
            }}
            @touchcancel=${(e: TouchEvent) => {
              e.preventDefault();
              this.handleHoldEnd();
            }}
            @mousedown=${(e: MouseEvent) => {
              e.preventDefault();
              this.handleHoldStart();
            }}
            @mouseup=${(e: MouseEvent) => {
              e.preventDefault();
              this.handleHoldEnd();
            }}
            @mouseleave=${(e: MouseEvent) => {
              e.preventDefault();
              this.handleHoldEnd();
            }}>
            <span class="speak-label">
              ${this.isHolding ? 'Release to Mute' : 'Hold to Speak'}
            </span>
          </button>
        </div>
      </div>
    `;
  }
}

```

```typescriptreact
/* tslint:disable */
/**
 * @license
 * SPDX-License-Identifier: Apache-2.0
 */

import {GoogleGenAI, LiveServerMessage, Modality, Session} from '@google/genai';
import {LitElement, css, html} from 'lit';
import {customElement, state} from 'lit/decorators.js';
import {createBlob, decode, decodeAudioData} from './utils';
import './visual-3d';
// PWA: register service worker if available
try {
  // dynamic import optional; vite-plugin-pwa injects virtual module
  // eslint-disable-next-line @typescript-eslint/ban-ts-GB
  // @ts-ignore
  import('virtual:pwa-register').then(({ registerSW }) => {
    registerSW({ immediate: true });
  }).catch(() => {});
} catch {}

@customElement('gdm-live-audio')
export class GdmLiveAudio extends LitElement {
  @state() isRecording = false;
  @state() isHolding = false;
  @state() status = '';
  @state() error = '';
  @state() appStopped = false;
  @state() sessionStarted = false;

  private client: GoogleGenAI;
  private sessionPromise?: Promise<Session>;
  // Audio contexts created on user gesture for iOS PWA compatibility
  private inputAudioContext?: AudioContext;
  private outputAudioContext?: AudioContext;
  @state() inputNode?: GainNode;
  @state() outputNode?: GainNode;
  private nextStartTime = 0;
  private mediaStream: MediaStream;
  private sourceNode: AudioNode;
  private scriptProcessorNode: ScriptProcessorNode;
  private sources = new Set<AudioBufferSourceNode>();
  private frameInterval: number;
  private videoElement: HTMLVideoElement;
  private canvasElement: HTMLCanvasElement;
  private framesSent = 0;

  static styles = css`
    :host {
      display: block;
      position: relative;
      width: 100vw;
      height: 100vh;
      min-height: 100dvh;
      background: #000000;
      color: #ffffff;
      overflow: hidden;
    }

    @supports (height: 100dvh) {
      :host {
        height: 100dvh;
      }
    }

    .layout {
      position: relative;
      width: 100%;
      height: 100%;
    }

    #video-background {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 66.667vh;
      object-fit: cover;
    }

    .speak-area {
      position: absolute;
      bottom: 0;
      left: 0;
      width: 100%;
      height: calc(33.333vh + env(safe-area-inset-bottom));
      border-top-left-radius: 32px;
      border-top-right-radius: 32px;
      border: 1px solid rgba(255, 255, 255, 0.08);
      border-bottom: none;
      overflow: hidden;
      background: rgba(0, 0, 0, 0.3);
      backdrop-filter: blur(6px);
    }

    .session-overlay {
      position: absolute;
      inset: 0;
      display: flex;
      align-items: center;
      justify-content: center;
      padding: 24px;
      font-size: 18px;
      letter-spacing: 0.1em;
      text-transform: uppercase;
      color: rgba(255, 255, 255, 0.92);
      background: linear-gradient(
        180deg,
        rgba(17, 24, 39, 0.6) 0%,
        rgba(15, 23, 42, 0.85) 100%
      );
      pointer-events: none;
      text-align: center;
    }

    gdm-live-audio-visuals-3d {
      display: block;
      width: 100%;
      height: 100%;
      pointer-events: none;
      border-top-left-radius: inherit;
      border-top-right-radius: inherit;
    }

    .speak-button {
      position: absolute;
      inset: 0;
      width: 100%;
      height: 100%;
      border: none;
      background: transparent;
      cursor: pointer;
      display: flex;
      align-items: flex-end;
      justify-content: center;
      padding: 24px 24px calc(24px + env(safe-area-inset-bottom));
      color: inherit;
      transition: background 0.2s ease;
      -webkit-touch-callout: none;
      -webkit-user-select: none;
      -moz-user-select: none;
      -ms-user-select: none;
      user-select: none;
      -webkit-tap-highlight-color: transparent;
      touch-action: manipulation;
    }

    .speak-button:hover,
    .speak-button:focus-visible {
      background: rgba(255, 255, 255, 0.04);
    }

    .speak-button:focus-visible {
      outline: 2px solid rgba(255, 255, 255, 0.25);
      outline-offset: 4px;
    }

    .speak-label {
      font-size: 18px;
      letter-spacing: 0.08em;
      text-transform: uppercase;
      color: rgba(255, 255, 255, 0.92);
      text-shadow: 0 0 6px rgba(0, 0, 0, 0.6);
      -webkit-touch-callout: none;
      -webkit-user-select: none;
      -moz-user-select: none;
      -ms-user-select: none;
      user-select: none;
      pointer-events: none;
    }

    .error-banner {
      position: absolute;
      top: calc(env(safe-area-inset-top) + 12px);
      left: 16px;
      right: 16px;
      z-index: 10;
      padding: 12px 16px;
      background: rgba(220, 38, 38, 0.85);
      color: #ffffff;
      border-radius: 12px;
      text-align: center;
      font-size: 14px;
      line-height: 1.4;
      box-shadow: 0 12px 24px rgba(0, 0, 0, 0.35);
      backdrop-filter: blur(12px);
    }

    .start-button {
      position: absolute;
      top: 50%;
      left: 50%;
      transform: translate(-50%, -50%);
      z-index: 1000;
      padding: 20px 40px;
      background: rgba(255, 255, 255, 0.25);
      color: #ffffff;
      border: 2px solid rgba(255, 255, 255, 0.5);
      border-radius: 16px;
      font-size: 18px;
      font-weight: 500;
      letter-spacing: 0.05em;
      text-transform: uppercase;
      cursor: pointer;
      transition: all 0.2s ease;
      backdrop-filter: blur(12px);
      box-shadow: 0 8px 24px rgba(0, 0, 0, 0.6), 0 0 0 1px rgba(255, 255, 255, 0.1);
      -webkit-tap-highlight-color: transparent;
      touch-action: manipulation;
      white-space: nowrap;
    }

    .start-button:hover,
    .start-button:focus-visible {
      background: rgba(255, 255, 255, 0.25);
      border-color: rgba(255, 255, 255, 0.5);
      transform: translate(-50%, -50%) scale(1.05);
    }

    .start-button:active {
      transform: translate(-50%, -50%) scale(0.98);
    }

    .start-button:focus-visible {
      outline: 2px solid rgba(255, 255, 255, 0.5);
      outline-offset: 4px;
    }
  `;

  constructor() {
    super();
    this.initClient();
  }

  protected firstUpdated() {
    this.videoElement = this.shadowRoot!.querySelector('#video-background');
    this.canvasElement = this.shadowRoot!.querySelector('#frame-canvas');
    this.debug(
      `Boot: secure=${window.isSecureContext} ua=${navigator.userAgent}`,
    );
    this.checkPermissions();
    this.addEventListener('click', async () => {
      try {
        if (this.outputAudioContext) {
          await this.outputAudioContext.resume();
        }
        if (this.inputAudioContext) {
          await this.inputAudioContext.resume();
        }
      } catch {}
    });
  }

  private async checkPermissions() {
    const parts: string[] = [];
    const anyNav: any = navigator as any;
    if (anyNav.permissions?.query) {
      try {
        const mic = await anyNav.permissions.query({name: 'microphone'});
        parts.push(`mic=${mic.state}`);
      } catch {}
      try {
        const cam = await anyNav.permissions.query({name: 'camera'});
        parts.push(`cam=${cam.state}`);
      } catch {}
    }
    if (parts.length) this.updateStatus(`Permissions: ${parts.join(' ')}`);
  }

  private debug(_msg: string, _data?: unknown) {
    // Debug logging disabled for production UI clarity.
  }

  private initAudio() {
    if (this.outputAudioContext && this.inputNode) {
      this.nextStartTime = this.outputAudioContext.currentTime;
      this.inputNode.gain.value = 0.0; // Start with mic muted
    }
  }

  private async initClient() {
    if (!process.env.API_KEY || process.env.API_KEY === 'undefined') {
      this.updateError('Missing GEMINI_API_KEY');
    }

    this.client = new GoogleGenAI({apiKey: process.env.API_KEY});

    // Audio contexts and nodes will be created on user gesture
    // Session will be initialized when user starts session
  }

  private initSession(): Promise<Session> {
    const model = 'gemini-2.5-flash-native-audio-preview-09-2025';

    try {
      return this.client.live.connect({
        model: model,
        callbacks: {
          onopen: () => {
            this.debug('Live session opened');
          },
          onmessage: async (message: LiveServerMessage) => {
            this.debug('Live message received');
            const audio =
              message.serverContent?.modelTurn?.parts[0]?.inlineData;

            if (audio && this.outputAudioContext && this.outputNode) {
              this.nextStartTime = Math.max(
                this.nextStartTime,
                this.outputAudioContext.currentTime,
              );

              const audioBuffer = await decodeAudioData(
                decode(audio.data),
                this.outputAudioContext,
                24000,
                1,
              );
              const source = this.outputAudioContext.createBufferSource();
              source.buffer = audioBuffer;
              source.connect(this.outputNode);
              source.addEventListener('ended', () => {
                this.sources.delete(source);
              });

              source.start(this.nextStartTime);
              this.nextStartTime = this.nextStartTime + audioBuffer.duration;
              this.sources.add(source);
            }

            const interrupted = message.serverContent?.interrupted;
            if (interrupted) {
              for (const source of this.sources.values()) {
                source.stop();
                this.sources.delete(source);
              }
              this.nextStartTime = 0;
            }
          },
          onerror: (e: ErrorEvent) => {
            this.updateError('Live error: ' + e.message);
            console.error('Live error event', e);
          },
          onclose: (e: CloseEvent) => {
            this.updateStatus('Live close: ' + e.reason + ' (' + e.code + ')');
            console.warn('Live close', e);
          },
        },
        config: {
          responseModalities: [Modality.AUDIO],
          systemInstruction: {
            parts: [
              {
                text: `You are an AI assistant integrated into an application where the main interface is a live camera view. The user interacts with you by pressing and holding a large button at the bottom of the screen. When the button is active, you listen to the user's voice input and provide an answer based on what is visible through the camera.

Your task is to analyze the live camera feed and interpret the user’s spoken request in combination. Always respond with precise, spatially grounded instructions that reference items or locations visible on the screen.

When giving instructions:

Be specific and concise.

Refer to locations using clear spatial language (e.g., “third shelf from the top,” “top right corner,” “behind the red box,” etc.).

Only describe what is visible on the camera or can be reasonably inferred from it.

Do not invent objects or locations that are not present in the scene.

Keep answers straightforward and helpful.

Example:
If the user asks, “Where is the gluten-free bread?” and it is visible through the camera, respond with a description such as:
“The gluten-free bread is on the third shelf from the top, in the top right corner.”

Your goal is to help the user locate objects in the environment accurately and efficiently based on real-time visual input.`,
              },
            ],
          },
          speechConfig: {
            voiceConfig: {prebuiltVoiceConfig: {voiceName: 'Zephyr'}},
            // languageCode: 'en-GB'
          },
        },
      });
    } catch (e) {
      console.error(e);
      return Promise.reject(e);
    }
  }

  private updateStatus(msg: string) {
    this.status = msg;
  }

  private updateError(msg: string) {
    this.error = msg;
  }

  private formatErrorDescriptor(error: unknown): { name: string; message: string } {
    if (error instanceof DOMException) {
      return { name: error.name, message: error.message };
    }
    if (error instanceof Error) {
      return { name: error.name || 'Error', message: error.message };
    }
    return { name: 'Error', message: String(error) };
  }

  private blobToBase64(blob: Blob): Promise<string> {
    return new Promise((resolve, reject) => {
      const reader = new FileReader();
      reader.onloadend = () => {
        if (typeof reader.result === 'string') {
          resolve(reader.result.split(',')[1]);
        } else {
          reject(new Error('Failed to read blob as base64 string.'));
        }
      };
      reader.onerror = reject;
      reader.readAsDataURL(blob);
    });
  }

  private async startRecordingWithStream(stream: MediaStream) {
    if (this.isRecording) {
      stream.getTracks().forEach((t) => t.stop());
      return;
    }

    if (!this.inputAudioContext || !this.outputAudioContext || !this.inputNode || !this.outputNode) {
      throw new Error('Audio contexts not initialized');
    }

    if (!this.sessionPromise) {
      this.sessionPromise = this.initSession();
    }

    this.appStopped = false;
    this.framesSent = 0;

    this.mediaStream = stream;

    this.videoElement.srcObject = this.mediaStream;
    try {
      await this.videoElement.play();
    } catch (e) {
      console.warn('video.play() failed', e);
    }

    const vt = this.mediaStream.getVideoTracks()[0];
    const at = this.mediaStream.getAudioTracks()[0];
    this.updateStatus('Media access granted. v=' + (!!vt) + ' a=' + (!!at));

    // Ensure we have an audio track before proceeding
    if (!at) {
      throw new Error('No audio track available in media stream');
    }

    // Ensure audio track is enabled and ready
    if (!at.enabled) {
      at.enabled = true;
    }

    // Ensure audio track is ready - wait a bit if needed
    let trackReadyState = at.readyState;
    if (trackReadyState !== 'live') {
      // Wait for track to become live (max 1 second)
      let attempts = 0;
      while (trackReadyState !== 'live' && attempts < 10) {
        await new Promise(resolve => setTimeout(resolve, 100));
        trackReadyState = at.readyState;
        attempts++;
      }
      if (trackReadyState !== 'live') {
        throw new Error('Audio track is not live. State: ' + trackReadyState);
      }
    }

    // Create media stream source only after audio context is running
    if (this.inputAudioContext.state === 'running') {
      try {
        this.sourceNode = this.inputAudioContext.createMediaStreamSource(
          this.mediaStream,
        );
        this.sourceNode.connect(this.inputNode);
      } catch (e) {
        throw new Error('Failed to create media stream source: ' + (e as Error).message);
      }
    } else {
      throw new Error('Audio context is not running. State: ' + this.inputAudioContext.state);
    }

    const bufferSize = 4096;
    this.scriptProcessorNode = this.inputAudioContext.createScriptProcessor(
      bufferSize,
      1,
      1,
    );

    this.scriptProcessorNode.onaudioprocess = (audioProcessingEvent) => {
      if (!this.isRecording) return;
      if (!this.isHolding) return;

      const inputBuffer = audioProcessingEvent.inputBuffer;
      const pcmData = inputBuffer.getChannelData(0);
      const srcRate = this.inputAudioContext.sampleRate;
      const downsampleTo16k = (float32: Float32Array, inRate: number) => {
        if (inRate === 16000) return float32;
        const ratio = inRate / 16000;
        const newLen = Math.floor(float32.length / ratio);
        const result = new Float32Array(newLen);
        let idx = 0, pos = 0;
        while (idx < newLen) {
          const nextPos = Math.min(float32.length - 1, (idx + 1) * ratio);
          let sum = 0, count = 0;
          for (let i = pos; i < nextPos; i++) { sum += float32[i]; count++; }
          result[idx++] = sum / Math.max(1, count);
          pos = nextPos;
        }
        return result;
      };
      const mono16k = srcRate === 16000 ? pcmData : downsampleTo16k(pcmData, srcRate);

      const sessionPromise = this.sessionPromise;
      if (!sessionPromise) {
        return;
      }

      sessionPromise
        .then((session) => {
          session.sendRealtimeInput({media: createBlob(mono16k)});
        })
        .catch((e) => this.updateError('Session send failed: ' + e.message));
    };

    this.sourceNode.connect(this.scriptProcessorNode);
    this.scriptProcessorNode.connect(this.inputAudioContext.destination);

    this.frameInterval = window.setInterval(() => {
      if (!this.isRecording) return;
      const context = this.canvasElement.getContext('2d');
      this.canvasElement.width = this.videoElement.videoWidth;
      this.canvasElement.height = this.videoElement.videoHeight;
      context.drawImage(
        this.videoElement,
        0,
        0,
        this.canvasElement.width,
        this.canvasElement.height,
      );
      this.canvasElement.toBlob(
        async (blob) => {
          if (blob) {
            const base64Data = await this.blobToBase64(blob);
            const sessionPromise = this.sessionPromise;
            if (!sessionPromise) {
              return;
            }

            sessionPromise
              .then((session) => {
                session.sendRealtimeInput({
                  media: {data: base64Data, mimeType: 'image/jpeg'},
                });
                this.framesSent++;
                if (this.framesSent % 10 === 0) {
                  this.debug(`Frames sent: ${this.framesSent}`);
                }
              })
              .catch((e) =>
                this.updateError('Session send (image) failed: ' + e.message),
              );
          }
        },
        'image/jpeg',
        0.8,
      );
    }, 500); // 2 frames per second

    this.isRecording = true;
    this.updateStatus('');
  }

  private async startRecording() {
    if (this.isRecording) {
      return;
    }

    if (!this.inputAudioContext || !this.outputAudioContext) {
      throw new Error('Audio contexts not initialized');
    }

    const inputResumePromise =
      this.inputAudioContext.state === 'running'
        ? Promise.resolve()
        : this.inputAudioContext.resume();
    const outputResumePromise =
      this.outputAudioContext.state === 'running'
        ? Promise.resolve()
        : this.outputAudioContext.resume();

    this.updateStatus('Requesting camera/mic access...');

    try {
      const preferBackCamera = async (): Promise<MediaStream> => {
        const constraints: MediaStreamConstraints = {
          audio: {
            channelCount: 1,
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true,
          },
          video: {
            facingMode: { ideal: 'environment' },
          },
        };
        this.debug('getUserMedia with constraints', constraints);
        try {
          const stream = await navigator.mediaDevices.getUserMedia(constraints);
          const track = stream.getVideoTracks()[0];
          const settings = track?.getSettings?.() || {} as MediaTrackSettings;
          this.debug('Acquired stream video settings', settings);
          if (settings.facingMode !== 'environment') {
            // Try enumerate fallback to enforce back camera
            const devices = await navigator.mediaDevices.enumerateDevices();
            const back = devices.find(
              (d) => d.kind === 'videoinput' && /back|rear|environment/i.test(d.label),
            );
            if (back) {
              try {
                const enforced = await navigator.mediaDevices.getUserMedia({
                  audio: constraints.audio,
                  video: { deviceId: { exact: back.deviceId } },
                });
                this.debug('Enforced back camera via deviceId', back);
                stream.getTracks().forEach((t) => t.stop());
                return enforced;
              } catch {
                // fall through to original stream
              }
            }
          }
          return stream;
        } catch (e) {
          const err = e as DOMException;
          this.updateError(
            'getUserMedia failed: ' +
              (err?.name ?? 'Error') +
              ' - ' +
              (err?.message ?? err),
          );
          const devices = await navigator.mediaDevices.enumerateDevices();
          const back = devices.find(
            (d) => d.kind === 'videoinput' && /back|rear|environment/i.test(d.label),
          );
          if (back) {
            return await navigator.mediaDevices.getUserMedia({
              audio: constraints.audio,
              video: { deviceId: { exact: back.deviceId } },
            });
          }
          throw e;
        }
      };

      let stream: MediaStream | null = null;
      try {
        stream = await preferBackCamera();
        await Promise.all([inputResumePromise, outputResumePromise]);
        await this.startRecordingWithStream(stream);
      } catch (error) {
        if (stream) {
          stream.getTracks().forEach((t) => t.stop());
        }
        throw error;
      }
    } catch (err) {
      console.error('Error starting recording:', err);
      const error = err as DOMException;
      this.updateError(
        'Camera/mic activation failed: ' +
          (error?.name ?? 'Error') +
          ' - ' +
          (error?.message ?? error),
      );
      this.stopRecording();
    }
  }

  private stopRecording() {
    if (!this.isRecording && !this.mediaStream && !this.inputAudioContext)
      return;

    this.updateStatus('');

    this.isRecording = false;

    if (this.frameInterval) {
      clearInterval(this.frameInterval);
      this.frameInterval = null;
    }

    if (this.scriptProcessorNode && this.sourceNode && this.inputAudioContext) {
      this.scriptProcessorNode.disconnect();
      this.sourceNode.disconnect();
    }

    this.scriptProcessorNode = null;
    this.sourceNode = null;

    if (this.mediaStream) {
      this.mediaStream.getTracks().forEach((track) => track.stop());
      this.mediaStream = null;
    }

    if (this.videoElement) {
      this.videoElement.srcObject = null;
    }

    this.framesSent = 0;
    this.updateStatus('');
  }

  private stopApplication() {
    if (this.appStopped) {
      return;
    }

    this.stopRecording();

    for (const source of this.sources.values()) {
      try {
        source.stop();
      } catch {}
      this.sources.delete(source);
    }

    const sessionPromise = this.sessionPromise;
    this.sessionPromise = undefined;

    sessionPromise
      ?.then((session) => session.close())
      .catch((e) => this.updateError('Session close failed: ' + e.message));

    this.videoElement?.pause();
    if (this.videoElement) {
      this.videoElement.srcObject = null;
    }

    this.appStopped = true;
    this.updateStatus('Session ended');
  }

  private async restartApplication() {
    if (this.isRecording) {
      return;
    }

    this.updateStatus('Restarting session...');
    this.sessionPromise = this.initSession();
    this.appStopped = false;

    try {
      await this.startRecording();
    } catch (error) {
      this.updateError('Restart failed: ' + (error as Error).message);
    }
  }

  private reset() {
    this.sessionPromise?.then((session) => session.close());
    this.sessionPromise = this.initSession();
    this.updateStatus('');
  }

  private async handleStartSession() {
    if (this.sessionStarted) {
      return;
    }
    
    this.sessionStarted = true;
    this.updateStatus('Requesting camera/mic access...');
    
    let stream: MediaStream | null = null;
    
    try {
      // Create audio contexts within user gesture for iOS PWA compatibility
      // This MUST be done synchronously from the event handler
      if (!this.inputAudioContext) {
        this.inputAudioContext = new (window.AudioContext ||
          (window as any).webkitAudioContext)({sampleRate: 16000});
      }
      if (!this.outputAudioContext) {
        this.outputAudioContext = new (window.AudioContext ||
          (window as any).webkitAudioContext)({sampleRate: 24000});
      }
      
      // Create gain nodes if they don't exist
      if (!this.inputNode) {
        this.inputNode = this.inputAudioContext.createGain();
        this.inputNode.gain.value = 0.0; // Start with mic muted
      }
      if (!this.outputNode) {
        this.outputNode = this.outputAudioContext.createGain();
        this.outputNode.connect(this.outputAudioContext.destination);
      }
      
      // Initialize session if not already done
      if (!this.sessionPromise) {
        this.sessionPromise = this.initSession();
      }
      
      // Request permissions directly from user gesture handler
      // This MUST be called synchronously from the event handler on iOS
      const constraints: MediaStreamConstraints = {
        audio: {
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
        },
        video: {
          facingMode: { ideal: 'environment' },
        },
      };
      
      // Call getUserMedia immediately to preserve user gesture context
      stream = await navigator.mediaDevices.getUserMedia(constraints);
      
      // Ensure audio contexts are running (they should be already, but check)
      if (this.inputAudioContext.state !== 'running') {
        await this.inputAudioContext.resume();
      }
      if (this.outputAudioContext.state !== 'running') {
        await this.outputAudioContext.resume();
      }
      
      // Small delay to ensure audio contexts are fully ready
      await new Promise(resolve => setTimeout(resolve, 100));
      
      // Initialize audio timing
      this.initAudio();
      
      // Now start recording with the stream
      await this.startRecordingWithStream(stream);
    } catch (err) {
      console.error('Error starting session:', err);
      const errorMessage = err instanceof Error ? err.message : String(err);
      this.updateError('Failed to access camera/microphone: ' + errorMessage);
      this.sessionStarted = false; // Allow retry on error
      
      // Clean up stream if it was created
      if (stream) {
        stream.getTracks().forEach((t) => t.stop());
      }
    }
  }

  private async handleHoldStart() {
    if (!this.isRecording) {
      if (this.appStopped) {
        await this.restartApplication();
      } else {
        try {
          await this.startRecording();
        } catch (err) {
          console.error('Error starting recording:', err);
          return;
        }
      }
    }
    
    this.isHolding = true;
    if (this.inputNode) {
      this.inputNode.gain.value = 1.0;
    }
  }

  private handleHoldEnd() {
    this.isHolding = false;
    if (this.inputNode) {
      this.inputNode.gain.value = 0.0;
    }
  }

  render() {
    return html`
      <div class="layout">
        <video
          id="video-background"
          autoplay
          muted
          playsinline
          ?hidden=${this.appStopped}></video>
        <canvas id="frame-canvas" style="display:none"></canvas>
        ${this.error
          ? html`<div class="error-banner" role="alert">${this.error}</div>`
          : null}
        ${!this.sessionStarted
          ? html`
              <button
                class="start-button"
                type="button"
                aria-label="Start Session"
                @click=${this.handleStartSession}
                @touchstart=${(e: TouchEvent) => {
                  e.preventDefault();
                  this.handleStartSession();
                }}>
                Start Session
              </button>
            `
          : null}
        <div class="speak-area">
          ${this.inputNode && this.outputNode
            ? html`<gdm-live-audio-visuals-3d
                .inputNode=${this.inputNode}
                .outputNode=${this.outputNode}></gdm-live-audio-visuals-3d>`
            : null}
          ${this.appStopped
            ? html`<div class="session-overlay">Session Ended</div>`
            : null}
          <button
            class="speak-button"
            type="button"
            aria-pressed=${this.isHolding}
            aria-label=${this.isHolding ? 'Release to mute' : 'Hold to speak'}
            @touchstart=${(e: TouchEvent) => {
              e.preventDefault();
              this.handleHoldStart();
            }}
            @touchend=${(e: TouchEvent) => {
              e.preventDefault();
              this.handleHoldEnd();
            }}
            @touchcancel=${(e: TouchEvent) => {
              e.preventDefault();
              this.handleHoldEnd();
            }}
            @mousedown=${(e: MouseEvent) => {
              e.preventDefault();
              this.handleHoldStart();
            }}
            @mouseup=${(e: MouseEvent) => {
              e.preventDefault();
              this.handleHoldEnd();
            }}
            @mouseleave=${(e: MouseEvent) => {
              e.preventDefault();
              this.handleHoldEnd();
            }}>
            <span class="speak-label">
              ${this.isHolding ? 'Release to Mute' : 'Hold to Speak'}
            </span>
          </button>
        </div>
      </div>
    `;
  }
}

```

```typescriptreact
/* tslint:disable */
/**
 * @license
 * SPDX-License-Identifier: Apache-2.0
 */

import {GoogleGenAI, LiveServerMessage, Modality, Session} from '@google/genai';
import {LitElement, css, html} from 'lit';
import {customElement, state} from 'lit/decorators.js';
import {createBlob, decode, decodeAudioData} from './utils';
import './visual-3d';
// PWA: register service worker if available
try {
  // dynamic import optional; vite-plugin-pwa injects virtual module
  // eslint-disable-next-line @typescript-eslint/ban-ts-GB
  // @ts-ignore
  import('virtual:pwa-register').then(({ registerSW }) => {
    registerSW({ immediate: true });
  }).catch(() => {});
} catch {}

@customElement('gdm-live-audio')
export class GdmLiveAudio extends LitElement {
  @state() isRecording = false;
  @state() isHolding = false;
  @state() status = '';
  @state() error = '';
  @state() appStopped = false;
  @state() sessionStarted = false;

  private client: GoogleGenAI;
  private sessionPromise?: Promise<Session>;
  // Audio contexts created on user gesture for iOS PWA compatibility
  private inputAudioContext?: AudioContext;
  private outputAudioContext?: AudioContext;
  @state() inputNode?: GainNode;
  @state() outputNode?: GainNode;
  private nextStartTime = 0;
  private mediaStream: MediaStream;
  private sourceNode: AudioNode;
  private scriptProcessorNode: ScriptProcessorNode;
  private sources = new Set<AudioBufferSourceNode>();
  private frameInterval: number;
  private videoElement: HTMLVideoElement;
  private canvasElement: HTMLCanvasElement;
  private framesSent = 0;

  static styles = css`
    :host {
      display: block;
      position: relative;
      width: 100vw;
      height: 100vh;
      min-height: 100dvh;
      background: #000000;
      color: #ffffff;
      overflow: hidden;
    }

    @supports (height: 100dvh) {
      :host {
        height: 100dvh;
      }
    }

    .layout {
      position: relative;
      width: 100%;
      height: 100%;
    }

    #video-background {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 66.667vh;
      object-fit: cover;
    }

    .speak-area {
      position: absolute;
      bottom: 0;
      left: 0;
      width: 100%;
      height: calc(33.333vh + env(safe-area-inset-bottom));
      border-top-left-radius: 32px;
      border-top-right-radius: 32px;
      border: 1px solid rgba(255, 255, 255, 0.08);
      border-bottom: none;
      overflow: hidden;
      background: rgba(0, 0, 0, 0.3);
      backdrop-filter: blur(6px);
    }

    .session-overlay {
      position: absolute;
      inset: 0;
      display: flex;
      align-items: center;
      justify-content: center;
      padding: 24px;
      font-size: 18px;
      letter-spacing: 0.1em;
      text-transform: uppercase;
      color: rgba(255, 255, 255, 0.92);
      background: linear-gradient(
        180deg,
        rgba(17, 24, 39, 0.6) 0%,
        rgba(15, 23, 42, 0.85) 100%
      );
      pointer-events: none;
      text-align: center;
    }

    gdm-live-audio-visuals-3d {
      display: block;
      width: 100%;
      height: 100%;
      pointer-events: none;
      border-top-left-radius: inherit;
      border-top-right-radius: inherit;
    }

    .speak-button {
      position: absolute;
      inset: 0;
      width: 100%;
      height: 100%;
      border: none;
      background: transparent;
      cursor: pointer;
      display: flex;
      align-items: flex-end;
      justify-content: center;
      padding: 24px 24px calc(24px + env(safe-area-inset-bottom));
      color: inherit;
      transition: background 0.2s ease;
      -webkit-touch-callout: none;
      -webkit-user-select: none;
      -moz-user-select: none;
      -ms-user-select: none;
      user-select: none;
      -webkit-tap-highlight-color: transparent;
      touch-action: manipulation;
    }

    .speak-button:hover,
    .speak-button:focus-visible {
      background: rgba(255, 255, 255, 0.04);
    }

    .speak-button:focus-visible {
      outline: 2px solid rgba(255, 255, 255, 0.25);
      outline-offset: 4px;
    }

    .speak-label {
      font-size: 18px;
      letter-spacing: 0.08em;
      text-transform: uppercase;
      color: rgba(255, 255, 255, 0.92);
      text-shadow: 0 0 6px rgba(0, 0, 0, 0.6);
      -webkit-touch-callout: none;
      -webkit-user-select: none;
      -moz-user-select: none;
      -ms-user-select: none;
      user-select: none;
      pointer-events: none;
    }

    .error-banner {
      position: absolute;
      top: calc(env(safe-area-inset-top) + 12px);
      left: 16px;
      right: 16px;
      z-index: 10;
      padding: 12px 16px;
      background: rgba(220, 38, 38, 0.85);
      color: #ffffff;
      border-radius: 12px;
      text-align: center;
      font-size: 14px;
      line-height: 1.4;
      box-shadow: 0 12px 24px rgba(0, 0, 0, 0.35);
      backdrop-filter: blur(12px);
    }

    .start-button {
      position: absolute;
      top: 50%;
      left: 50%;
      transform: translate(-50%, -50%);
      z-index: 1000;
      padding: 20px 40px;
      background: rgba(255, 255, 255, 0.25);
      color: #ffffff;
      border: 2px solid rgba(255, 255, 255, 0.5);
      border-radius: 16px;
      font-size: 18px;
      font-weight: 500;
      letter-spacing: 0.05em;
      text-transform: uppercase;
      cursor: pointer;
      transition: all 0.2s ease;
      backdrop-filter: blur(12px);
      box-shadow: 0 8px 24px rgba(0, 0, 0, 0.6), 0 0 0 1px rgba(255, 255, 255, 0.1);
      -webkit-tap-highlight-color: transparent;
      touch-action: manipulation;
      white-space: nowrap;
    }

    .start-button:hover,
    .start-button:focus-visible {
      background: rgba(255, 255, 255, 0.25);
      border-color: rgba(255, 255, 255, 0.5);
      transform: translate(-50%, -50%) scale(1.05);
    }

    .start-button:active {
      transform: translate(-50%, -50%) scale(0.98);
    }

    .start-button:focus-visible {
      outline: 2px solid rgba(255, 255, 255, 0.5);
      outline-offset: 4px;
    }
  `;

  constructor() {
    super();
    this.initClient();
  }

  protected firstUpdated() {
    this.videoElement = this.shadowRoot!.querySelector('#video-background');
    this.canvasElement = this.shadowRoot!.querySelector('#frame-canvas');
    this.debug(
      `Boot: secure=${window.isSecureContext} ua=${navigator.userAgent}`,
    );
    this.checkPermissions();
    this.addEventListener('click', async () => {
      try {
        if (this.outputAudioContext) {
          await this.outputAudioContext.resume();
        }
        if (this.inputAudioContext) {
          await this.inputAudioContext.resume();
        }
      } catch {}
    });
  }

  private async checkPermissions() {
    const parts: string[] = [];
    const anyNav: any = navigator as any;
    if (anyNav.permissions?.query) {
      try {
        const mic = await anyNav.permissions.query({name: 'microphone'});
        parts.push(`mic=${mic.state}`);
      } catch {}
      try {
        const cam = await anyNav.permissions.query({name: 'camera'});
        parts.push(`cam=${cam.state}`);
      } catch {}
    }
    if (parts.length) this.updateStatus(`Permissions: ${parts.join(' ')}`);
  }

  private debug(_msg: string, _data?: unknown) {
    // Debug logging disabled for production UI clarity.
  }

  private initAudio() {
    if (this.outputAudioContext && this.inputNode) {
      this.nextStartTime = this.outputAudioContext.currentTime;
      this.inputNode.gain.value = 0.0; // Start with mic muted
    }
  }

  private async initClient() {
    if (!process.env.API_KEY || process.env.API_KEY === 'undefined') {
      this.updateError('Missing GEMINI_API_KEY');
    }

    this.client = new GoogleGenAI({apiKey: process.env.API_KEY});

    // Audio contexts and nodes will be created on user gesture
    // Session will be initialized when user starts session
  }

  private initSession(): Promise<Session> {
    const model = 'gemini-2.5-flash-native-audio-preview-09-2025';

    try {
      return this.client.live.connect({
        model: model,
        callbacks: {
          onopen: () => {
            this.debug('Live session opened');
          },
          onmessage: async (message: LiveServerMessage) => {
            this.debug('Live message received');
            const audio =
              message.serverContent?.modelTurn?.parts[0]?.inlineData;

            if (audio && this.outputAudioContext && this.outputNode) {
              this.nextStartTime = Math.max(
                this.nextStartTime,
                this.outputAudioContext.currentTime,
              );

              const audioBuffer = await decodeAudioData(
                decode(audio.data),
                this.outputAudioContext,
                24000,
                1,
              );
              const source = this.outputAudioContext.createBufferSource();
              source.buffer = audioBuffer;
              source.connect(this.outputNode);
              source.addEventListener('ended', () => {
                this.sources.delete(source);
              });

              source.start(this.nextStartTime);
              this.nextStartTime = this.nextStartTime + audioBuffer.duration;
              this.sources.add(source);
            }

            const interrupted = message.serverContent?.interrupted;
            if (interrupted) {
              for (const source of this.sources.values()) {
                source.stop();
                this.sources.delete(source);
              }
              this.nextStartTime = 0;
            }
          },
          onerror: (e: ErrorEvent) => {
            this.updateError('Live error: ' + e.message);
            console.error('Live error event', e);
          },
          onclose: (e: CloseEvent) => {
            this.updateStatus('Live close: ' + e.reason + ' (' + e.code + ')');
            console.warn('Live close', e);
          },
        },
        config: {
          responseModalities: [Modality.AUDIO],
          systemInstruction: {
            parts: [
              {
                text: `You are an AI assistant integrated into an application where the main interface is a live camera view. The user interacts with you by pressing and holding a large button at the bottom of the screen. When the button is active, you listen to the user's voice input and provide an answer based on what is visible through the camera.

Your task is to analyze the live camera feed and interpret the user’s spoken request in combination. Always respond with precise, spatially grounded instructions that reference items or locations visible on the screen.

When giving instructions:

Be specific and concise.

Refer to locations using clear spatial language (e.g., “third shelf from the top,” “top right corner,” “behind the red box,” etc.).

Only describe what is visible on the camera or can be reasonably inferred from it.

Do not invent objects or locations that are not present in the scene.

Keep answers straightforward and helpful.

Example:
If the user asks, “Where is the gluten-free bread?” and it is visible through the camera, respond with a description such as:
“The gluten-free bread is on the third shelf from the top, in the top right corner.”

Your goal is to help the user locate objects in the environment accurately and efficiently based on real-time visual input.`,
              },
            ],
          },
          speechConfig: {
            voiceConfig: {prebuiltVoiceConfig: {voiceName: 'Zephyr'}},
            // languageCode: 'en-GB'
          },
        },
      });
    } catch (e) {
      console.error(e);
      return Promise.reject(e);
    }
  }

  private updateStatus(msg: string) {
    this.status = msg;
  }

  private updateError(msg: string) {
    this.error = msg;
  }

  private formatErrorDescriptor(error: unknown): { name: string; message: string } {
    if (error instanceof DOMException) {
      return { name: error.name, message: error.message };
    }
    if (error instanceof Error) {
      return { name: error.name || 'Error', message: error.message };
    }
    return { name: 'Error', message: String(error) };
  }

  private blobToBase64(blob: Blob): Promise<string> {
    return new Promise((resolve, reject) => {
      const reader = new FileReader();
      reader.onloadend = () => {
        if (typeof reader.result === 'string') {
          resolve(reader.result.split(',')[1]);
        } else {
          reject(new Error('Failed to read blob as base64 string.'));
        }
      };
      reader.onerror = reject;
      reader.readAsDataURL(blob);
    });
  }

  private async startRecordingWithStream(stream: MediaStream) {
    if (this.isRecording) {
      stream.getTracks().forEach((t) => t.stop());
      return;
    }

    if (!this.inputAudioContext || !this.outputAudioContext || !this.inputNode || !this.outputNode) {
      throw new Error('Audio contexts not initialized');
    }

    if (!this.sessionPromise) {
      this.sessionPromise = this.initSession();
    }

    this.appStopped = false;
    this.framesSent = 0;

    this.mediaStream = stream;

    this.videoElement.srcObject = this.mediaStream;
    try {
      await this.videoElement.play();
    } catch (e) {
      console.warn('video.play() failed', e);
    }

    const vt = this.mediaStream.getVideoTracks()[0];
    const at = this.mediaStream.getAudioTracks()[0];
    this.updateStatus('Media access granted. v=' + (!!vt) + ' a=' + (!!at));

    // Ensure we have an audio track before proceeding
    if (!at) {
      throw new Error('No audio track available in media stream');
    }

    // Ensure audio track is enabled and ready
    if (!at.enabled) {
      at.enabled = true;
    }

    // Ensure audio track is ready - wait a bit if needed
    let trackReadyState = at.readyState;
    if (trackReadyState !== 'live') {
      // Wait for track to become live (max 1 second)
      let attempts = 0;
      while (trackReadyState !== 'live' && attempts < 10) {
        await new Promise(resolve => setTimeout(resolve, 100));
        trackReadyState = at.readyState;
        attempts++;
      }
      if (trackReadyState !== 'live') {
        throw new Error('Audio track is not live. State: ' + trackReadyState);
      }
    }

    // Create media stream source only after audio context is running
    if (this.inputAudioContext.state === 'running') {
      try {
        this.sourceNode = this.inputAudioContext.createMediaStreamSource(
          this.mediaStream,
        );
        this.sourceNode.connect(this.inputNode);
      } catch (e) {
        throw new Error('Failed to create media stream source: ' + (e as Error).message);
      }
    } else {
      throw new Error('Audio context is not running. State: ' + this.inputAudioContext.state);
    }

    const bufferSize = 4096;
    this.scriptProcessorNode = this.inputAudioContext.createScriptProcessor(
      bufferSize,
      1,
      1,
    );

    this.scriptProcessorNode.onaudioprocess = (audioProcessingEvent) => {
      if (!this.isRecording) return;
      if (!this.isHolding) return;

      const inputBuffer = audioProcessingEvent.inputBuffer;
      const pcmData = inputBuffer.getChannelData(0);
      const srcRate = this.inputAudioContext.sampleRate;
      const downsampleTo16k = (float32: Float32Array, inRate: number) => {
        if (inRate === 16000) return float32;
        const ratio = inRate / 16000;
        const newLen = Math.floor(float32.length / ratio);
        const result = new Float32Array(newLen);
        let idx = 0, pos = 0;
        while (idx < newLen) {
          const nextPos = Math.min(float32.length - 1, (idx + 1) * ratio);
          let sum = 0, count = 0;
          for (let i = pos; i < nextPos; i++) { sum += float32[i]; count++; }
          result[idx++] = sum / Math.max(1, count);
          pos = nextPos;
        }
        return result;
      };
      const mono16k = srcRate === 16000 ? pcmData : downsampleTo16k(pcmData, srcRate);

      const sessionPromise = this.sessionPromise;
      if (!sessionPromise) {
        return;
      }

      sessionPromise
        .then((session) => {
          session.sendRealtimeInput({media: createBlob(mono16k)});
        })
        .catch((e) => this.updateError('Session send failed: ' + e.message));
    };

    this.sourceNode.connect(this.scriptProcessorNode);
    this.scriptProcessorNode.connect(this.inputAudioContext.destination);

    this.frameInterval = window.setInterval(() => {
      if (!this.isRecording) return;
      const context = this.canvasElement.getContext('2d');
      this.canvasElement.width = this.videoElement.videoWidth;
      this.canvasElement.height = this.videoElement.videoHeight;
      context.drawImage(
        this.videoElement,
        0,
        0,
        this.canvasElement.width,
        this.canvasElement.height,
      );
      this.canvasElement.toBlob(
        async (blob) => {
          if (blob) {
            const base64Data = await this.blobToBase64(blob);
            const sessionPromise = this.sessionPromise;
            if (!sessionPromise) {
              return;
            }

            sessionPromise
              .then((session) => {
                session.sendRealtimeInput({
                  media: {data: base64Data, mimeType: 'image/jpeg'},
                });
                this.framesSent++;
                if (this.framesSent % 10 === 0) {
                  this.debug(`Frames sent: ${this.framesSent}`);
                }
              })
              .catch((e) =>
                this.updateError('Session send (image) failed: ' + e.message),
              );
          }
        },
        'image/jpeg',
        0.8,
      );
    }, 500); // 2 frames per second

    this.isRecording = true;
    this.updateStatus('');
  }

  private async startRecording() {
    if (this.isRecording) {
      return;
    }

    if (!this.inputAudioContext || !this.outputAudioContext) {
      throw new Error('Audio contexts not initialized');
    }

    const inputResumePromise =
      this.inputAudioContext.state === 'running'
        ? Promise.resolve()
        : this.inputAudioContext.resume();
    const outputResumePromise =
      this.outputAudioContext.state === 'running'
        ? Promise.resolve()
        : this.outputAudioContext.resume();

    this.updateStatus('Requesting camera/mic access...');

    try {
      const preferBackCamera = async (): Promise<MediaStream> => {
        const constraints: MediaStreamConstraints = {
          audio: {
            channelCount: 1,
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true,
          },
          video: {
            facingMode: { ideal: 'environment' },
          },
        };
        this.debug('getUserMedia with constraints', constraints);
        try {
          const stream = await navigator.mediaDevices.getUserMedia(constraints);
          const track = stream.getVideoTracks()[0];
          const settings = track?.getSettings?.() || {} as MediaTrackSettings;
          this.debug('Acquired stream video settings', settings);
          if (settings.facingMode !== 'environment') {
            // Try enumerate fallback to enforce back camera
            const devices = await navigator.mediaDevices.enumerateDevices();
            const back = devices.find(
              (d) => d.kind === 'videoinput' && /back|rear|environment/i.test(d.label),
            );
            if (back) {
              try {
                const enforced = await navigator.mediaDevices.getUserMedia({
                  audio: constraints.audio,
                  video: { deviceId: { exact: back.deviceId } },
                });
                this.debug('Enforced back camera via deviceId', back);
                stream.getTracks().forEach((t) => t.stop());
                return enforced;
              } catch {
                // fall through to original stream
              }
            }
          }
          return stream;
        } catch (e) {
          const { name, message } = this.formatErrorDescriptor(e);
          this.updateError(`getUserMedia failed: ${name} - ${message}`);
          const devices = await navigator.mediaDevices.enumerateDevices();
          const back = devices.find(
            (d) => d.kind === 'videoinput' && /back|rear|environment/i.test(d.label),
          );
          if (back) {
            return await navigator.mediaDevices.getUserMedia({
              audio: constraints.audio,
              video: { deviceId: { exact: back.deviceId } },
            });
          }
          throw e;
        }
      };

      let stream: MediaStream | null = null;
      try {
        stream = await preferBackCamera();
        await Promise.all([inputResumePromise, outputResumePromise]);
        await this.startRecordingWithStream(stream);
      } catch (error) {
        if (stream) {
          stream.getTracks().forEach((t) => t.stop());
        }
        throw error;
      }
    } catch (err) {
      console.error('Error starting recording:', err);
      const { name, message } = this.formatErrorDescriptor(err);
      this.updateError(`Camera/mic activation failed: ${name} - ${message}`);
      this.stopRecording();
    }
  }

  private stopRecording() {
    if (!this.isRecording && !this.mediaStream && !this.inputAudioContext)
      return;

    this.updateStatus('');

    this.isRecording = false;

    if (this.frameInterval) {
      clearInterval(this.frameInterval);
      this.frameInterval = null;
    }

    if (this.scriptProcessorNode && this.sourceNode && this.inputAudioContext) {
      this.scriptProcessorNode.disconnect();
      this.sourceNode.disconnect();
    }

    this.scriptProcessorNode = null;
    this.sourceNode = null;

    if (this.mediaStream) {
      this.mediaStream.getTracks().forEach((track) => track.stop());
      this.mediaStream = null;
    }

    if (this.videoElement) {
      this.videoElement.srcObject = null;
    }

    this.framesSent = 0;
    this.updateStatus('');
  }

  private stopApplication() {
    if (this.appStopped) {
      return;
    }

    this.stopRecording();

    for (const source of this.sources.values()) {
      try {
        source.stop();
      } catch {}
      this.sources.delete(source);
    }

    const sessionPromise = this.sessionPromise;
    this.sessionPromise = undefined;

    sessionPromise
      ?.then((session) => session.close())
      .catch((e) => this.updateError('Session close failed: ' + e.message));

    this.videoElement?.pause();
    if (this.videoElement) {
      this.videoElement.srcObject = null;
    }

    this.appStopped = true;
    this.updateStatus('Session ended');
  }

  private async restartApplication() {
    if (this.isRecording) {
      return;
    }

    this.updateStatus('Restarting session...');
    this.sessionPromise = this.initSession();
    this.appStopped = false;

    try {
      await this.startRecording();
    } catch (error) {
      this.updateError('Restart failed: ' + (error as Error).message);
    }
  }

  private reset() {
    this.sessionPromise?.then((session) => session.close());
    this.sessionPromise = this.initSession();
    this.updateStatus('');
  }

  private async handleStartSession() {
    if (this.sessionStarted) {
      return;
    }
    
    this.sessionStarted = true;
    this.updateStatus('Requesting camera/mic access...');
    
    let stream: MediaStream | null = null;
    
    try {
      // Create audio contexts within user gesture for iOS PWA compatibility
      // This MUST be done synchronously from the event handler
      if (!this.inputAudioContext) {
        this.inputAudioContext = new (window.AudioContext ||
          (window as any).webkitAudioContext)({sampleRate: 16000});
      }
      if (!this.outputAudioContext) {
        this.outputAudioContext = new (window.AudioContext ||
          (window as any).webkitAudioContext)({sampleRate: 24000});
      }
      
      // Create gain nodes if they don't exist
      if (!this.inputNode) {
        this.inputNode = this.inputAudioContext.createGain();
        this.inputNode.gain.value = 0.0; // Start with mic muted
      }
      if (!this.outputNode) {
        this.outputNode = this.outputAudioContext.createGain();
        this.outputNode.connect(this.outputAudioContext.destination);
      }
      
      // Initialize session if not already done
      if (!this.sessionPromise) {
        this.sessionPromise = this.initSession();
      }
      
      // Request permissions directly from user gesture handler
      // This MUST be called synchronously from the event handler on iOS
      const constraints: MediaStreamConstraints = {
        audio: {
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
        },
        video: {
          facingMode: { ideal: 'environment' },
        },
      };
      
      // Call getUserMedia immediately to preserve user gesture context
      stream = await navigator.mediaDevices.getUserMedia(constraints);
      
      // Ensure audio contexts are running (they should be already, but check)
      if (this.inputAudioContext.state !== 'running') {
        await this.inputAudioContext.resume();
      }
      if (this.outputAudioContext.state !== 'running') {
        await this.outputAudioContext.resume();
      }
      
      // Small delay to ensure audio contexts are fully ready
      await new Promise(resolve => setTimeout(resolve, 100));
      
      // Initialize audio timing
      this.initAudio();
      
      // Now start recording with the stream
      await this.startRecordingWithStream(stream);
    } catch (err) {
      console.error('Error starting session:', err);
      const errorMessage = err instanceof Error ? err.message : String(err);
      this.updateError('Failed to access camera/microphone: ' + errorMessage);
      this.sessionStarted = false; // Allow retry on error
      
      // Clean up stream if it was created
      if (stream) {
        stream.getTracks().forEach((t) => t.stop());
      }
    }
  }

  private async handleHoldStart() {
    if (!this.isRecording) {
      if (this.appStopped) {
        await this.restartApplication();
      } else {
        try {
          await this.startRecording();
        } catch (err) {
          console.error('Error starting recording:', err);
          return;
        }
      }
    }
    
    this.isHolding = true;
    if (this.inputNode) {
      this.inputNode.gain.value = 1.0;
    }
  }

  private handleHoldEnd() {
    this.isHolding = false;
    if (this.inputNode) {
      this.inputNode.gain.value = 0.0;
    }
  }

  render() {
    return html`
      <div class="layout">
        <video
          id="video-background"
          autoplay
          muted
          playsinline
          ?hidden=${this.appStopped}></video>
        <canvas id="frame-canvas" style="display:none"></canvas>
        ${this.error
          ? html`<div class="error-banner" role="alert">${this.error}</div>`
          : null}
        ${!this.sessionStarted
          ? html`
              <button
                class="start-button"
                type="button"
                aria-label="Start Session"
                @click=${this.handleStartSession}
                @touchstart=${(e: TouchEvent) => {
                  e.preventDefault();
                  this.handleStartSession();
                }}>
                Start Session
              </button>
            `
          : null}
        <div class="speak-area">
          ${this.inputNode && this.outputNode
            ? html`<gdm-live-audio-visuals-3d
                .inputNode=${this.inputNode}
                .outputNode=${this.outputNode}></gdm-live-audio-visuals-3d>`
            : null}
          ${this.appStopped
            ? html`<div class="session-overlay">Session Ended</div>`
            : null}
          <button
            class="speak-button"
            type="button"
            aria-pressed=${this.isHolding}
            aria-label=${this.isHolding ? 'Release to mute' : 'Hold to speak'}
            @touchstart=${(e: TouchEvent) => {
              e.preventDefault();
              this.handleHoldStart();
            }}
            @touchend=${(e: TouchEvent) => {
              e.preventDefault();
              this.handleHoldEnd();
            }}
            @touchcancel=${(e: TouchEvent) => {
              e.preventDefault();
              this.handleHoldEnd();
            }}
            @mousedown=${(e: MouseEvent) => {
              e.preventDefault();
              this.handleHoldStart();
            }}
            @mouseup=${(e: MouseEvent) => {
              e.preventDefault();
              this.handleHoldEnd();
            }}
            @mouseleave=${(e: MouseEvent) => {
              e.preventDefault();
              this.handleHoldEnd();
            }}>
            <span class="speak-label">
              ${this.isHolding ? 'Release to Mute' : 'Hold to Speak'}
            </span>
          </button>
        </div>
      </div>
    `;
  }
}

```

```typescriptreact
/* tslint:disable */
/**
 * @license
 * SPDX-License-Identifier: Apache-2.0
 */

import {GoogleGenAI, LiveServerMessage, Modality, Session} from '@google/genai';
import {LitElement, css, html} from 'lit';
import {customElement, state} from 'lit/decorators.js';
import {createBlob, decode, decodeAudioData} from './utils';
import './visual-3d';
// PWA: register service worker if available
try {
  // dynamic import optional; vite-plugin-pwa injects virtual module
  // eslint-disable-next-line @typescript-eslint/ban-ts-GB
  // @ts-ignore
  import('virtual:pwa-register').then(({ registerSW }) => {
    registerSW({ immediate: true });
  }).catch(() => {});
} catch {}

@customElement('gdm-live-audio')
export class GdmLiveAudio extends LitElement {
  @state() isRecording = false;
  @state() isHolding = false;
  @state() status = '';
  @state() error = '';
  @state() appStopped = false;
  @state() sessionStarted = false;

  private client: GoogleGenAI;
  private sessionPromise?: Promise<Session>;
  // Audio contexts created on user gesture for iOS PWA compatibility
  private inputAudioContext?: AudioContext;
  private outputAudioContext?: AudioContext;
  @state() inputNode?: GainNode;
  @state() outputNode?: GainNode;
  private nextStartTime = 0;
  private mediaStream: MediaStream;
  private sourceNode: AudioNode;
  private scriptProcessorNode: ScriptProcessorNode;
  private sources = new Set<AudioBufferSourceNode>();
  private frameInterval: number;
  private videoElement: HTMLVideoElement;
  private canvasElement: HTMLCanvasElement;
  private framesSent = 0;

  static styles = css`
    :host {
      display: block;
      position: relative;
      width: 100vw;
      height: 100vh;
      min-height: 100dvh;
      background: #000000;
      color: #ffffff;
      overflow: hidden;
    }

    @supports (height: 100dvh) {
      :host {
        height: 100dvh;
      }
    }

    .layout {
      position: relative;
      width: 100%;
      height: 100%;
    }

    #video-background {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 66.667vh;
      object-fit: cover;
    }

    .speak-area {
      position: absolute;
      bottom: 0;
      left: 0;
      width: 100%;
      height: calc(33.333vh + env(safe-area-inset-bottom));
      border-top-left-radius: 32px;
      border-top-right-radius: 32px;
      border: 1px solid rgba(255, 255, 255, 0.08);
      border-bottom: none;
      overflow: hidden;
      background: rgba(0, 0, 0, 0.3);
      backdrop-filter: blur(6px);
    }

    .session-overlay {
      position: absolute;
      inset: 0;
      display: flex;
      align-items: center;
      justify-content: center;
      padding: 24px;
      font-size: 18px;
      letter-spacing: 0.1em;
      text-transform: uppercase;
      color: rgba(255, 255, 255, 0.92);
      background: linear-gradient(
        180deg,
        rgba(17, 24, 39, 0.6) 0%,
        rgba(15, 23, 42, 0.85) 100%
      );
      pointer-events: none;
      text-align: center;
    }

    gdm-live-audio-visuals-3d {
      display: block;
      width: 100%;
      height: 100%;
      pointer-events: none;
      border-top-left-radius: inherit;
      border-top-right-radius: inherit;
    }

    .speak-button {
      position: absolute;
      inset: 0;
      width: 100%;
      height: 100%;
      border: none;
      background: transparent;
      cursor: pointer;
      display: flex;
      align-items: flex-end;
      justify-content: center;
      padding: 24px 24px calc(24px + env(safe-area-inset-bottom));
      color: inherit;
      transition: background 0.2s ease;
      -webkit-touch-callout: none;
      -webkit-user-select: none;
      -moz-user-select: none;
      -ms-user-select: none;
      user-select: none;
      -webkit-tap-highlight-color: transparent;
      touch-action: manipulation;
    }

    .speak-button:hover,
    .speak-button:focus-visible {
      background: rgba(255, 255, 255, 0.04);
    }

    .speak-button:focus-visible {
      outline: 2px solid rgba(255, 255, 255, 0.25);
      outline-offset: 4px;
    }

    .speak-label {
      font-size: 18px;
      letter-spacing: 0.08em;
      text-transform: uppercase;
      color: rgba(255, 255, 255, 0.92);
      text-shadow: 0 0 6px rgba(0, 0, 0, 0.6);
      -webkit-touch-callout: none;
      -webkit-user-select: none;
      -moz-user-select: none;
      -ms-user-select: none;
      user-select: none;
      pointer-events: none;
    }

    .error-banner {
      position: absolute;
      top: calc(env(safe-area-inset-top) + 12px);
      left: 16px;
      right: 16px;
      z-index: 10;
      padding: 12px 16px;
      background: rgba(220, 38, 38, 0.85);
      color: #ffffff;
      border-radius: 12px;
      text-align: center;
      font-size: 14px;
      line-height: 1.4;
      box-shadow: 0 12px 24px rgba(0, 0, 0, 0.35);
      backdrop-filter: blur(12px);
    }

    .start-button {
      position: absolute;
      top: 50%;
      left: 50%;
      transform: translate(-50%, -50%);
      z-index: 1000;
      padding: 20px 40px;
      background: rgba(255, 255, 255, 0.25);
      color: #ffffff;
      border: 2px solid rgba(255, 255, 255, 0.5);
      border-radius: 16px;
      font-size: 18px;
      font-weight: 500;
      letter-spacing: 0.05em;
      text-transform: uppercase;
      cursor: pointer;
      transition: all 0.2s ease;
      backdrop-filter: blur(12px);
      box-shadow: 0 8px 24px rgba(0, 0, 0, 0.6), 0 0 0 1px rgba(255, 255, 255, 0.1);
      -webkit-tap-highlight-color: transparent;
      touch-action: manipulation;
      white-space: nowrap;
    }

    .start-button:hover,
    .start-button:focus-visible {
      background: rgba(255, 255, 255, 0.25);
      border-color: rgba(255, 255, 255, 0.5);
      transform: translate(-50%, -50%) scale(1.05);
    }

    .start-button:active {
      transform: translate(-50%, -50%) scale(0.98);
    }

    .start-button:focus-visible {
      outline: 2px solid rgba(255, 255, 255, 0.5);
      outline-offset: 4px;
    }
  `;

  constructor() {
    super();
    this.initClient();
  }

  protected firstUpdated() {
    this.videoElement = this.shadowRoot!.querySelector('#video-background');
    this.canvasElement = this.shadowRoot!.querySelector('#frame-canvas');
    this.debug(
      `Boot: secure=${window.isSecureContext} ua=${navigator.userAgent}`,
    );
    this.checkPermissions();
    this.addEventListener('click', async () => {
      try {
        if (this.outputAudioContext) {
          await this.outputAudioContext.resume();
        }
        if (this.inputAudioContext) {
          await this.inputAudioContext.resume();
        }
      } catch {}
    });
  }

  private async checkPermissions() {
    const parts: string[] = [];
    const anyNav: any = navigator as any;
    if (anyNav.permissions?.query) {
      try {
        const mic = await anyNav.permissions.query({name: 'microphone'});
        parts.push(`mic=${mic.state}`);
      } catch {}
      try {
        const cam = await anyNav.permissions.query({name: 'camera'});
        parts.push(`cam=${cam.state}`);
      } catch {}
    }
    if (parts.length) this.updateStatus(`Permissions: ${parts.join(' ')}`);
  }

  private debug(_msg: string, _data?: unknown) {
    // Debug logging disabled for production UI clarity.
  }

  private initAudio() {
    if (this.outputAudioContext && this.inputNode) {
      this.nextStartTime = this.outputAudioContext.currentTime;
      this.inputNode.gain.value = 0.0; // Start with mic muted
    }
  }

  private async initClient() {
    if (!process.env.API_KEY || process.env.API_KEY === 'undefined') {
      this.updateError('Missing GEMINI_API_KEY');
    }

    this.client = new GoogleGenAI({apiKey: process.env.API_KEY});

    // Audio contexts and nodes will be created on user gesture
    // Session will be initialized when user starts session
  }

  private initSession(): Promise<Session> {
    const model = 'gemini-2.5-flash-native-audio-preview-09-2025';

    try {
      return this.client.live.connect({
        model: model,
        callbacks: {
          onopen: () => {
            this.debug('Live session opened');
          },
          onmessage: async (message: LiveServerMessage) => {
            this.debug('Live message received');
            const audio =
              message.serverContent?.modelTurn?.parts[0]?.inlineData;

            if (audio && this.outputAudioContext && this.outputNode) {
              this.nextStartTime = Math.max(
                this.nextStartTime,
                this.outputAudioContext.currentTime,
              );

              const audioBuffer = await decodeAudioData(
                decode(audio.data),
                this.outputAudioContext,
                24000,
                1,
              );
              const source = this.outputAudioContext.createBufferSource();
              source.buffer = audioBuffer;
              source.connect(this.outputNode);
              source.addEventListener('ended', () => {
                this.sources.delete(source);
              });

              source.start(this.nextStartTime);
              this.nextStartTime = this.nextStartTime + audioBuffer.duration;
              this.sources.add(source);
            }

            const interrupted = message.serverContent?.interrupted;
            if (interrupted) {
              for (const source of this.sources.values()) {
                source.stop();
                this.sources.delete(source);
              }
              this.nextStartTime = 0;
            }
          },
          onerror: (e: ErrorEvent) => {
            this.updateError('Live error: ' + e.message);
            console.error('Live error event', e);
          },
          onclose: (e: CloseEvent) => {
            this.updateStatus('Live close: ' + e.reason + ' (' + e.code + ')');
            console.warn('Live close', e);
          },
        },
        config: {
          responseModalities: [Modality.AUDIO],
          systemInstruction: {
            parts: [
              {
                text: `You are an AI assistant integrated into an application where the main interface is a live camera view. The user interacts with you by pressing and holding a large button at the bottom of the screen. When the button is active, you listen to the user's voice input and provide an answer based on what is visible through the camera.

Your task is to analyze the live camera feed and interpret the user’s spoken request in combination. Always respond with precise, spatially grounded instructions that reference items or locations visible on the screen.

When giving instructions:

Be specific and concise.

Refer to locations using clear spatial language (e.g., “third shelf from the top,” “top right corner,” “behind the red box,” etc.).

Only describe what is visible on the camera or can be reasonably inferred from it.

Do not invent objects or locations that are not present in the scene.

Keep answers straightforward and helpful.

Example:
If the user asks, “Where is the gluten-free bread?” and it is visible through the camera, respond with a description such as:
“The gluten-free bread is on the third shelf from the top, in the top right corner.”

Your goal is to help the user locate objects in the environment accurately and efficiently based on real-time visual input.`,
              },
            ],
          },
          speechConfig: {
            voiceConfig: {prebuiltVoiceConfig: {voiceName: 'Zephyr'}},
            // languageCode: 'en-GB'
          },
        },
      });
    } catch (e) {
      console.error(e);
      return Promise.reject(e);
    }
  }

  private updateStatus(msg: string) {
    this.status = msg;
  }

  private updateError(msg: string) {
    this.error = msg;
  }

  private formatErrorDescriptor(error: unknown): { name: string; message: string } {
    if (error instanceof DOMException) {
      return { name: error.name, message: error.message };
    }
    if (error instanceof Error) {
      return { name: error.name || 'Error', message: error.message };
    }
    return { name: 'Error', message: String(error) };
  }

  private blobToBase64(blob: Blob): Promise<string> {
    return new Promise((resolve, reject) => {
      const reader = new FileReader();
      reader.onloadend = () => {
        if (typeof reader.result === 'string') {
          resolve(reader.result.split(',')[1]);
        } else {
          reject(new Error('Failed to read blob as base64 string.'));
        }
      };
      reader.onerror = reject;
      reader.readAsDataURL(blob);
    });
  }

  private async startRecordingWithStream(stream: MediaStream) {
    if (this.isRecording) {
      stream.getTracks().forEach((t) => t.stop());
      return;
    }

    if (!this.inputAudioContext || !this.outputAudioContext || !this.inputNode || !this.outputNode) {
      throw new Error('Audio contexts not initialized');
    }

    if (!this.sessionPromise) {
      this.sessionPromise = this.initSession();
    }

    this.appStopped = false;
    this.framesSent = 0;

    this.mediaStream = stream;

    this.videoElement.srcObject = this.mediaStream;
    try {
      await this.videoElement.play();
    } catch (e) {
      console.warn('video.play() failed', e);
    }

    const vt = this.mediaStream.getVideoTracks()[0];
    const at = this.mediaStream.getAudioTracks()[0];
    this.updateStatus('Media access granted. v=' + (!!vt) + ' a=' + (!!at));

    // Ensure we have an audio track before proceeding
    if (!at) {
      throw new Error('No audio track available in media stream');
    }

    // Ensure audio track is enabled and ready
    if (!at.enabled) {
      at.enabled = true;
    }

    // Ensure audio track is ready - wait a bit if needed
    let trackReadyState = at.readyState;
    if (trackReadyState !== 'live') {
      // Wait for track to become live (max 1 second)
      let attempts = 0;
      while (trackReadyState !== 'live' && attempts < 10) {
        await new Promise(resolve => setTimeout(resolve, 100));
        trackReadyState = at.readyState;
        attempts++;
      }
      if (trackReadyState !== 'live') {
        throw new Error('Audio track is not live. State: ' + trackReadyState);
      }
    }

    // Create media stream source only after audio context is running
    if (this.inputAudioContext.state === 'running') {
      try {
        this.sourceNode = this.inputAudioContext.createMediaStreamSource(
          this.mediaStream,
        );
        this.sourceNode.connect(this.inputNode);
      } catch (e) {
        throw new Error('Failed to create media stream source: ' + (e as Error).message);
      }
    } else {
      throw new Error('Audio context is not running. State: ' + this.inputAudioContext.state);
    }

    const bufferSize = 4096;
    this.scriptProcessorNode = this.inputAudioContext.createScriptProcessor(
      bufferSize,
      1,
      1,
    );

    this.scriptProcessorNode.onaudioprocess = (audioProcessingEvent) => {
      if (!this.isRecording) return;
      if (!this.isHolding) return;

      const inputBuffer = audioProcessingEvent.inputBuffer;
      const pcmData = inputBuffer.getChannelData(0);
      const srcRate = this.inputAudioContext.sampleRate;
      const downsampleTo16k = (float32: Float32Array, inRate: number) => {
        if (inRate === 16000) return float32;
        const ratio = inRate / 16000;
        const newLen = Math.floor(float32.length / ratio);
        const result = new Float32Array(newLen);
        let idx = 0, pos = 0;
        while (idx < newLen) {
          const nextPos = Math.min(float32.length - 1, (idx + 1) * ratio);
          let sum = 0, count = 0;
          for (let i = pos; i < nextPos; i++) { sum += float32[i]; count++; }
          result[idx++] = sum / Math.max(1, count);
          pos = nextPos;
        }
        return result;
      };
      const mono16k = srcRate === 16000 ? pcmData : downsampleTo16k(pcmData, srcRate);

      const sessionPromise = this.sessionPromise;
      if (!sessionPromise) {
        return;
      }

      sessionPromise
        .then((session) => {
          session.sendRealtimeInput({media: createBlob(mono16k)});
        })
        .catch((e) => this.updateError('Session send failed: ' + e.message));
    };

    this.sourceNode.connect(this.scriptProcessorNode);
    this.scriptProcessorNode.connect(this.inputAudioContext.destination);

    this.frameInterval = window.setInterval(() => {
      if (!this.isRecording) return;
      const context = this.canvasElement.getContext('2d');
      this.canvasElement.width = this.videoElement.videoWidth;
      this.canvasElement.height = this.videoElement.videoHeight;
      context.drawImage(
        this.videoElement,
        0,
        0,
        this.canvasElement.width,
        this.canvasElement.height,
      );
      this.canvasElement.toBlob(
        async (blob) => {
          if (blob) {
            const base64Data = await this.blobToBase64(blob);
            const sessionPromise = this.sessionPromise;
            if (!sessionPromise) {
              return;
            }

            sessionPromise
              .then((session) => {
                session.sendRealtimeInput({
                  media: {data: base64Data, mimeType: 'image/jpeg'},
                });
                this.framesSent++;
                if (this.framesSent % 10 === 0) {
                  this.debug(`Frames sent: ${this.framesSent}`);
                }
              })
              .catch((e) =>
                this.updateError('Session send (image) failed: ' + e.message),
              );
          }
        },
        'image/jpeg',
        0.8,
      );
    }, 500); // 2 frames per second

    this.isRecording = true;
    this.updateStatus('');
  }

  private async startRecording() {
    if (this.isRecording) {
      return;
    }

    if (!this.inputAudioContext || !this.outputAudioContext) {
      throw new Error('Audio contexts not initialized');
    }

    const inputResumePromise =
      this.inputAudioContext.state === 'running'
        ? Promise.resolve()
        : this.inputAudioContext.resume();
    const outputResumePromise =
      this.outputAudioContext.state === 'running'
        ? Promise.resolve()
        : this.outputAudioContext.resume();

    this.updateStatus('Requesting camera/mic access...');

    try {
      const preferBackCamera = async (): Promise<MediaStream> => {
        const constraints: MediaStreamConstraints = {
          audio: {
            channelCount: 1,
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true,
          },
          video: {
            facingMode: { ideal: 'environment' },
          },
        };
        this.debug('getUserMedia with constraints', constraints);
        try {
          const stream = await navigator.mediaDevices.getUserMedia(constraints);
          const track = stream.getVideoTracks()[0];
          const settings = track?.getSettings?.() || {} as MediaTrackSettings;
          this.debug('Acquired stream video settings', settings);
          if (settings.facingMode !== 'environment') {
            // Try enumerate fallback to enforce back camera
            const devices = await navigator.mediaDevices.enumerateDevices();
            const back = devices.find(
              (d) => d.kind === 'videoinput' && /back|rear|environment/i.test(d.label),
            );
            if (back) {
              try {
                const enforced = await navigator.mediaDevices.getUserMedia({
                  audio: constraints.audio,
                  video: { deviceId: { exact: back.deviceId } },
                });
                this.debug('Enforced back camera via deviceId', back);
                stream.getTracks().forEach((t) => t.stop());
                return enforced;
              } catch {
                // fall through to original stream
              }
            }
          }
          return stream;
        } catch (e) {
          const { name, message } = this.formatErrorDescriptor(e);
          this.updateError(`getUserMedia failed: ${name} - ${message}`);
          const devices = await navigator.mediaDevices.enumerateDevices();
          const back = devices.find(
            (d) => d.kind === 'videoinput' && /back|rear|environment/i.test(d.label),
          );
          if (back) {
            return await navigator.mediaDevices.getUserMedia({
              audio: constraints.audio,
              video: { deviceId: { exact: back.deviceId } },
            });
          }
          throw e;
        }
      };

      let stream: MediaStream | null = null;
      try {
        stream = await preferBackCamera();
        await Promise.all([inputResumePromise, outputResumePromise]);
        await this.startRecordingWithStream(stream);
      } catch (error) {
        if (stream) {
          stream.getTracks().forEach((t) => t.stop());
        }
        throw error;
      }
    } catch (err) {
      console.error('Error starting recording:', err);
      const { name, message } = this.formatErrorDescriptor(err);
      this.updateError(`Camera/mic activation failed: ${name} - ${message}`);
      this.stopRecording();
    }
  }

  private stopRecording() {
    if (!this.isRecording && !this.mediaStream && !this.inputAudioContext)
      return;

    this.updateStatus('');

    this.isRecording = false;

    if (this.frameInterval) {
      clearInterval(this.frameInterval);
      this.frameInterval = null;
    }

    if (this.scriptProcessorNode && this.sourceNode && this.inputAudioContext) {
      this.scriptProcessorNode.disconnect();
      this.sourceNode.disconnect();
    }

    this.scriptProcessorNode = null;
    this.sourceNode = null;

    if (this.mediaStream) {
      this.mediaStream.getTracks().forEach((track) => track.stop());
      this.mediaStream = null;
    }

    if (this.videoElement) {
      this.videoElement.srcObject = null;
    }

    this.framesSent = 0;
    this.updateStatus('');
  }

  private stopApplication() {
    if (this.appStopped) {
      return;
    }

    this.stopRecording();

    for (const source of this.sources.values()) {
      try {
        source.stop();
      } catch {}
      this.sources.delete(source);
    }

    const sessionPromise = this.sessionPromise;
    this.sessionPromise = undefined;

    sessionPromise
      ?.then((session) => session.close())
      .catch((e) => this.updateError('Session close failed: ' + e.message));

    this.videoElement?.pause();
    if (this.videoElement) {
      this.videoElement.srcObject = null;
    }

    this.appStopped = true;
    this.updateStatus('Session ended');
  }

  private async restartApplication() {
    if (this.isRecording) {
      return;
    }

    this.updateStatus('Restarting session...');
    this.sessionPromise = this.initSession();
    this.appStopped = false;

    try {
      await this.startRecording();
    } catch (error) {
      this.updateError('Restart failed: ' + (error as Error).message);
    }
  }

  private reset() {
    this.sessionPromise?.then((session) => session.close());
    this.sessionPromise = this.initSession();
    this.updateStatus('');
  }

  private async handleStartSession() {
    if (this.sessionStarted) {
      return;
    }
    
    this.sessionStarted = true;
    this.updateStatus('Requesting camera/mic access...');
    
    let stream: MediaStream | null = null;
    
    try {
      // Create audio contexts within user gesture for iOS PWA compatibility
      // This MUST be done synchronously from the event handler
      if (!this.inputAudioContext) {
        this.inputAudioContext = new (window.AudioContext ||
          (window as any).webkitAudioContext)({sampleRate: 16000});
      }
      if (!this.outputAudioContext) {
        this.outputAudioContext = new (window.AudioContext ||
          (window as any).webkitAudioContext)({sampleRate: 24000});
      }
      
      // Create gain nodes if they don't exist
      if (!this.inputNode) {
        this.inputNode = this.inputAudioContext.createGain();
        this.inputNode.gain.value = 0.0; // Start with mic muted
      }
      if (!this.outputNode) {
        this.outputNode = this.outputAudioContext.createGain();
        this.outputNode.connect(this.outputAudioContext.destination);
      }
      
      // Initialize session if not already done
      if (!this.sessionPromise) {
        this.sessionPromise = this.initSession();
      }
      
      // Request permissions directly from user gesture handler
      // This MUST be called synchronously from the event handler on iOS
      const constraints: MediaStreamConstraints = {
        audio: {
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
        },
        video: {
          facingMode: { ideal: 'environment' },
        },
      };
      const inputResumePromise =
        this.inputAudioContext.state === 'running'
          ? Promise.resolve()
          : this.inputAudioContext.resume();
      const outputResumePromise =
        this.outputAudioContext.state === 'running'
          ? Promise.resolve()
          : this.outputAudioContext.resume();
      
      // Call getUserMedia immediately to preserve user gesture context
      stream = await navigator.mediaDevices.getUserMedia(constraints);
      
      await Promise.all([inputResumePromise, outputResumePromise]);
      
      // Small delay to ensure audio contexts are fully ready
      await new Promise(resolve => setTimeout(resolve, 100));
      
      // Initialize audio timing
      this.initAudio();
      
      // Now start recording with the stream
      await this.startRecordingWithStream(stream);
    } catch (err) {
      console.error('Error starting session:', err);
      const { name, message } = this.formatErrorDescriptor(err);
      this.updateError(`Failed to access camera/microphone: ${name} - ${message}`);
      this.updateStatus('');
      this.sessionStarted = false; // Allow retry on error
      
      // Clean up stream if it was created
      if (stream) {
        stream.getTracks().forEach((t) => t.stop());
      }
    }
  }

  private async handleHoldStart() {
    if (!this.isRecording) {
      if (this.appStopped) {
        await this.restartApplication();
      } else {
        try {
          await this.startRecording();
        } catch (err) {
          console.error('Error starting recording:', err);
          return;
        }
      }
    }
    
    this.isHolding = true;
    if (this.inputNode) {
      this.inputNode.gain.value = 1.0;
    }
  }

  private handleHoldEnd() {
    this.isHolding = false;
    if (this.inputNode) {
      this.inputNode.gain.value = 0.0;
    }
  }

  render() {
    return html`
      <div class="layout">
        <video
          id="video-background"
          autoplay
          muted
          playsinline
          ?hidden=${this.appStopped}></video>
        <canvas id="frame-canvas" style="display:none"></canvas>
        ${this.error
          ? html`<div class="error-banner" role="alert">${this.error}</div>`
          : null}
        ${!this.sessionStarted
          ? html`
              <button
                class="start-button"
                type="button"
                aria-label="Start Session"
                @click=${this.handleStartSession}
                @touchstart=${(e: TouchEvent) => {
                  e.preventDefault();
                  this.handleStartSession();
                }}>
                Start Session
              </button>
            `
          : null}
        <div class="speak-area">
          ${this.inputNode && this.outputNode
            ? html`<gdm-live-audio-visuals-3d
                .inputNode=${this.inputNode}
                .outputNode=${this.outputNode}></gdm-live-audio-visuals-3d>`
            : null}
          ${this.appStopped
            ? html`<div class="session-overlay">Session Ended</div>`
            : null}
          <button
            class="speak-button"
            type="button"
            aria-pressed=${this.isHolding}
            aria-label=${this.isHolding ? 'Release to mute' : 'Hold to speak'}
            @touchstart=${(e: TouchEvent) => {
              e.preventDefault();
              this.handleHoldStart();
            }}
            @touchend=${(e: TouchEvent) => {
              e.preventDefault();
              this.handleHoldEnd();
            }}
            @touchcancel=${(e: TouchEvent) => {
              e.preventDefault();
              this.handleHoldEnd();
            }}
            @mousedown=${(e: MouseEvent) => {
              e.preventDefault();
              this.handleHoldStart();
            }}
            @mouseup=${(e: MouseEvent) => {
              e.preventDefault();
              this.handleHoldEnd();
            }}
            @mouseleave=${(e: MouseEvent) => {
              e.preventDefault();
              this.handleHoldEnd();
            }}>
            <span class="speak-label">
              ${this.isHolding ? 'Release to Mute' : 'Hold to Speak'}
            </span>
          </button>
        </div>
      </div>
    `;
  }
}

```

## Summary
- Reworked `startRecording` so `getUserMedia` is the first awaited call in the tap handler while deferring audio-context resumes until after the stream resolves, preserving the iOS PWA user-activation grant and cleaning up on failure with detailed errors (`Camera/mic activation failed: <name> - <message>`).  
```603:664:index.tsx
    const inputResumePromise =
      this.inputAudioContext.state === 'running'
        ? Promise.resolve()
        : this.inputAudioContext.resume();
    const outputResumePromise =
      this.outputAudioContext.state === 'running'
        ? Promise.resolve()
        : this.outputAudioContext.resume();

    this.updateStatus('Requesting camera/mic access...');

    try {
      const preferBackCamera = async (): Promise<MediaStream> => {
        // ... existing code ...
      let stream: MediaStream | null = null;
      try {
        stream = await preferBackCamera();
        await Promise.all([inputResumePromise, outputResumePromise]);
        await this.startRecordingWithStream(stream);
```
- Brought `handleStartSession` into the same pattern—queueing the resume promises before the permission prompt, awaiting them afterward, and surfacing richer error details while resetting UI state for another attempt.  
```804:840:index.tsx
      const inputResumePromise =
        this.inputAudioContext.state === 'running'
          ? Promise.resolve()
          : this.inputAudioContext.resume();
      const outputResumePromise =
        this.outputAudioContext.state === 'running'
          ? Promise.resolve()
          : this.outputAudioContext.resume();
      // ... existing code ...
      await Promise.all([inputResumePromise, outputResumePromise]);
      // ... existing code ...
    } catch (err) {
      console.error('Error starting session:', err);
      const { name, message } = this.formatErrorDescriptor(err);
      this.updateError(`Failed to access camera/microphone: ${name} - ${message}`);
```
- Added a shared formatter for DOMException and Error instances so every user-facing permission failure message now includes the originating error name and message.  
```394:405:index.tsx
  private updateError(msg: string) {
    this.error = msg;
  }

  private formatErrorDescriptor(error: unknown): { name: string; message: string } {
    if (error instanceof DOMException) {
      return { name: error.name, message: error.message };
    }
    if (error instanceof Error) {
      return { name: error.name || 'Error', message: error.message };
    }
    return { name: 'Error', message: String(error) };
  }
```

## Testing
- Not run; rely on on-device validation.

## Next Steps
- Reinstall the PWA on an iPhone, start a fresh session, and confirm mic/camera prompts now succeed consistently.  
- Use Safari’s remote Web Inspector while testing to verify no new permission errors appear.

```typescriptreact
    const inputResumePromise =
      this.inputAudioContext.state === 'running'
        ? Promise.resolve()
        : this.inputAudioContext.resume();
    const outputResumePromise =
      this.outputAudioContext.state === 'running'
        ? Promise.resolve()
        : this.outputAudioContext.resume();

    this.updateStatus('Requesting camera/mic access...');

    try {
      const preferBackCamera = async (): Promise<MediaStream> => {
        // ... existing code ...
      let stream: MediaStream | null = null;
      try {
        stream = await preferBackCamera();
        await Promise.all([inputResumePromise, outputResumePromise]);
        await this.startRecordingWithStream(stream);
```

```typescriptreact
      const inputResumePromise =
        this.inputAudioContext.state === 'running'
          ? Promise.resolve()
          : this.inputAudioContext.resume();
      const outputResumePromise =
        this.outputAudioContext.state === 'running'
          ? Promise.resolve()
          : this.outputAudioContext.resume();
      // ... existing code ...
      await Promise.all([inputResumePromise, outputResumePromise]);
      // ... existing code ...
    } catch (err) {
      console.error('Error starting session:', err);
      const { name, message } = this.formatErrorDescriptor(err);
      this.updateError(`Failed to access camera/microphone: ${name} - ${message}`);
```

```typescriptreact
  private updateError(msg: string) {
    this.error = msg;
  }

  private formatErrorDescriptor(error: unknown): { name: string; message: string } {
    if (error instanceof DOMException) {
      return { name: error.name, message: error.message };
    }
    if (error instanceof Error) {
      return { name: error.name || 'Error', message: error.message };
    }
    return { name: 'Error', message: String(error) };
  }
```

---

**User**

It now works 50% of the time on both safari and the app added to home page. I get this message after clicking allow:


"Camera/mic activation failed: InvalidStateError - Failed to start the audio device"

---

**Cursor**

```typescriptreact
/* tslint:disable */
/**
 * @license
 * SPDX-License-Identifier: Apache-2.0
 */

import {GoogleGenAI, LiveServerMessage, Modality, Session} from '@google/genai';
import {LitElement, css, html} from 'lit';
import {customElement, state} from 'lit/decorators.js';
import {createBlob, decode, decodeAudioData} from './utils';
import './visual-3d';
// PWA: register service worker if available
try {
  // dynamic import optional; vite-plugin-pwa injects virtual module
  // eslint-disable-next-line @typescript-eslint/ban-ts-GB
  // @ts-ignore
  import('virtual:pwa-register').then(({ registerSW }) => {
    registerSW({ immediate: true });
  }).catch(() => {});
} catch {}

@customElement('gdm-live-audio')
export class GdmLiveAudio extends LitElement {
  @state() isRecording = false;
  @state() isHolding = false;
  @state() status = '';
  @state() error = '';
  @state() appStopped = false;
  @state() sessionStarted = false;

  private client: GoogleGenAI;
  private sessionPromise?: Promise<Session>;
  // Audio contexts created on user gesture for iOS PWA compatibility
  private inputAudioContext?: AudioContext;
  private outputAudioContext?: AudioContext;
  @state() inputNode?: GainNode;
  @state() outputNode?: GainNode;
  private nextStartTime = 0;
  private mediaStream: MediaStream;
  private sourceNode: AudioNode;
  private scriptProcessorNode: ScriptProcessorNode;
  private sources = new Set<AudioBufferSourceNode>();
  private frameInterval: number;
  private videoElement: HTMLVideoElement;
  private canvasElement: HTMLCanvasElement;
  private framesSent = 0;

  static styles = css`
    :host {
      display: block;
      position: relative;
      width: 100vw;
      height: 100vh;
      min-height: 100dvh;
      background: #000000;
      color: #ffffff;
      overflow: hidden;
    }

    @supports (height: 100dvh) {
      :host {
        height: 100dvh;
      }
    }

    .layout {
      position: relative;
      width: 100%;
      height: 100%;
    }

    #video-background {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 66.667vh;
      object-fit: cover;
    }

    .speak-area {
      position: absolute;
      bottom: 0;
      left: 0;
      width: 100%;
      height: calc(33.333vh + env(safe-area-inset-bottom));
      border-top-left-radius: 32px;
      border-top-right-radius: 32px;
      border: 1px solid rgba(255, 255, 255, 0.08);
      border-bottom: none;
      overflow: hidden;
      background: rgba(0, 0, 0, 0.3);
      backdrop-filter: blur(6px);
    }

    .session-overlay {
      position: absolute;
      inset: 0;
      display: flex;
      align-items: center;
      justify-content: center;
      padding: 24px;
      font-size: 18px;
      letter-spacing: 0.1em;
      text-transform: uppercase;
      color: rgba(255, 255, 255, 0.92);
      background: linear-gradient(
        180deg,
        rgba(17, 24, 39, 0.6) 0%,
        rgba(15, 23, 42, 0.85) 100%
      );
      pointer-events: none;
      text-align: center;
    }

    gdm-live-audio-visuals-3d {
      display: block;
      width: 100%;
      height: 100%;
      pointer-events: none;
      border-top-left-radius: inherit;
      border-top-right-radius: inherit;
    }

    .speak-button {
      position: absolute;
      inset: 0;
      width: 100%;
      height: 100%;
      border: none;
      background: transparent;
      cursor: pointer;
      display: flex;
      align-items: flex-end;
      justify-content: center;
      padding: 24px 24px calc(24px + env(safe-area-inset-bottom));
      color: inherit;
      transition: background 0.2s ease;
      -webkit-touch-callout: none;
      -webkit-user-select: none;
      -moz-user-select: none;
      -ms-user-select: none;
      user-select: none;
      -webkit-tap-highlight-color: transparent;
      touch-action: manipulation;
    }

    .speak-button:hover,
    .speak-button:focus-visible {
      background: rgba(255, 255, 255, 0.04);
    }

    .speak-button:focus-visible {
      outline: 2px solid rgba(255, 255, 255, 0.25);
      outline-offset: 4px;
    }

    .speak-label {
      font-size: 18px;
      letter-spacing: 0.08em;
      text-transform: uppercase;
      color: rgba(255, 255, 255, 0.92);
      text-shadow: 0 0 6px rgba(0, 0, 0, 0.6);
      -webkit-touch-callout: none;
      -webkit-user-select: none;
      -moz-user-select: none;
      -ms-user-select: none;
      user-select: none;
      pointer-events: none;
    }

    .error-banner {
      position: absolute;
      top: calc(env(safe-area-inset-top) + 12px);
      left: 16px;
      right: 16px;
      z-index: 10;
      padding: 12px 16px;
      background: rgba(220, 38, 38, 0.85);
      color: #ffffff;
      border-radius: 12px;
      text-align: center;
      font-size: 14px;
      line-height: 1.4;
      box-shadow: 0 12px 24px rgba(0, 0, 0, 0.35);
      backdrop-filter: blur(12px);
    }

    .start-button {
      position: absolute;
      top: 50%;
      left: 50%;
      transform: translate(-50%, -50%);
      z-index: 1000;
      padding: 20px 40px;
      background: rgba(255, 255, 255, 0.25);
      color: #ffffff;
      border: 2px solid rgba(255, 255, 255, 0.5);
      border-radius: 16px;
      font-size: 18px;
      font-weight: 500;
      letter-spacing: 0.05em;
      text-transform: uppercase;
      cursor: pointer;
      transition: all 0.2s ease;
      backdrop-filter: blur(12px);
      box-shadow: 0 8px 24px rgba(0, 0, 0, 0.6), 0 0 0 1px rgba(255, 255, 255, 0.1);
      -webkit-tap-highlight-color: transparent;
      touch-action: manipulation;
      white-space: nowrap;
    }

    .start-button:hover,
    .start-button:focus-visible {
      background: rgba(255, 255, 255, 0.25);
      border-color: rgba(255, 255, 255, 0.5);
      transform: translate(-50%, -50%) scale(1.05);
    }

    .start-button:active {
      transform: translate(-50%, -50%) scale(0.98);
    }

    .start-button:focus-visible {
      outline: 2px solid rgba(255, 255, 255, 0.5);
      outline-offset: 4px;
    }
  `;

  constructor() {
    super();
    this.initClient();
  }

  protected firstUpdated() {
    this.videoElement = this.shadowRoot!.querySelector('#video-background');
    this.canvasElement = this.shadowRoot!.querySelector('#frame-canvas');
    this.debug(
      `Boot: secure=${window.isSecureContext} ua=${navigator.userAgent}`,
    );
    this.checkPermissions();
    this.addEventListener('click', async () => {
      try {
        if (this.outputAudioContext) {
          await this.outputAudioContext.resume();
        }
        if (this.inputAudioContext) {
          await this.inputAudioContext.resume();
        }
      } catch {}
    });
  }

  private async checkPermissions() {
    const parts: string[] = [];
    const anyNav: any = navigator as any;
    if (anyNav.permissions?.query) {
      try {
        const mic = await anyNav.permissions.query({name: 'microphone'});
        parts.push(`mic=${mic.state}`);
      } catch {}
      try {
        const cam = await anyNav.permissions.query({name: 'camera'});
        parts.push(`cam=${cam.state}`);
      } catch {}
    }
    if (parts.length) this.updateStatus(`Permissions: ${parts.join(' ')}`);
  }

  private debug(_msg: string, _data?: unknown) {
    // Debug logging disabled for production UI clarity.
  }

  private initAudio() {
    if (this.outputAudioContext && this.inputNode) {
      this.nextStartTime = this.outputAudioContext.currentTime;
      this.inputNode.gain.value = 0.0; // Start with mic muted
    }
  }

  private async initClient() {
    if (!process.env.API_KEY || process.env.API_KEY === 'undefined') {
      this.updateError('Missing GEMINI_API_KEY');
    }

    this.client = new GoogleGenAI({apiKey: process.env.API_KEY});

    // Audio contexts and nodes will be created on user gesture
    // Session will be initialized when user starts session
  }

  private initSession(): Promise<Session> {
    const model = 'gemini-2.5-flash-native-audio-preview-09-2025';

    try {
      return this.client.live.connect({
        model: model,
        callbacks: {
          onopen: () => {
            this.debug('Live session opened');
          },
          onmessage: async (message: LiveServerMessage) => {
            this.debug('Live message received');
            const audio =
              message.serverContent?.modelTurn?.parts[0]?.inlineData;

            if (audio && this.outputAudioContext && this.outputNode) {
              this.nextStartTime = Math.max(
                this.nextStartTime,
                this.outputAudioContext.currentTime,
              );

              const audioBuffer = await decodeAudioData(
                decode(audio.data),
                this.outputAudioContext,
                24000,
                1,
              );
              const source = this.outputAudioContext.createBufferSource();
              source.buffer = audioBuffer;
              source.connect(this.outputNode);
              source.addEventListener('ended', () => {
                this.sources.delete(source);
              });

              source.start(this.nextStartTime);
              this.nextStartTime = this.nextStartTime + audioBuffer.duration;
              this.sources.add(source);
            }

            const interrupted = message.serverContent?.interrupted;
            if (interrupted) {
              for (const source of this.sources.values()) {
                source.stop();
                this.sources.delete(source);
              }
              this.nextStartTime = 0;
            }
          },
          onerror: (e: ErrorEvent) => {
            this.updateError('Live error: ' + e.message);
            console.error('Live error event', e);
          },
          onclose: (e: CloseEvent) => {
            this.updateStatus('Live close: ' + e.reason + ' (' + e.code + ')');
            console.warn('Live close', e);
          },
        },
        config: {
          responseModalities: [Modality.AUDIO],
          systemInstruction: {
            parts: [
              {
                text: `You are an AI assistant integrated into an application where the main interface is a live camera view. The user interacts with you by pressing and holding a large button at the bottom of the screen. When the button is active, you listen to the user's voice input and provide an answer based on what is visible through the camera.

Your task is to analyze the live camera feed and interpret the user’s spoken request in combination. Always respond with precise, spatially grounded instructions that reference items or locations visible on the screen.

When giving instructions:

Be specific and concise.

Refer to locations using clear spatial language (e.g., “third shelf from the top,” “top right corner,” “behind the red box,” etc.).

Only describe what is visible on the camera or can be reasonably inferred from it.

Do not invent objects or locations that are not present in the scene.

Keep answers straightforward and helpful.

Example:
If the user asks, “Where is the gluten-free bread?” and it is visible through the camera, respond with a description such as:
“The gluten-free bread is on the third shelf from the top, in the top right corner.”

Your goal is to help the user locate objects in the environment accurately and efficiently based on real-time visual input.`,
              },
            ],
          },
          speechConfig: {
            voiceConfig: {prebuiltVoiceConfig: {voiceName: 'Zephyr'}},
            // languageCode: 'en-GB'
          },
        },
      });
    } catch (e) {
      console.error(e);
      return Promise.reject(e);
    }
  }

  private updateStatus(msg: string) {
    this.status = msg;
  }

  private updateError(msg: string) {
    this.error = msg;
  }

  private formatErrorDescriptor(error: unknown): { name: string; message: string } {
    if (error instanceof DOMException) {
      return { name: error.name, message: error.message };
    }
    if (error instanceof Error) {
      return { name: error.name || 'Error', message: error.message };
    }
    return { name: 'Error', message: String(error) };
  }

  private async acquireMediaStream(): Promise<MediaStream> {
    const constraints: MediaStreamConstraints = {
      audio: {
        channelCount: 1,
        echoCancellation: true,
        noiseSuppression: true,
        autoGainControl: true,
      },
      video: {
        facingMode: { ideal: 'environment' },
      },
    };

    const stream = await navigator.mediaDevices.getUserMedia(constraints);
    const primaryVideoTrack = stream.getVideoTracks()[0];
    if (!primaryVideoTrack) {
      return stream;
    }

    const settings = (primaryVideoTrack.getSettings?.() ??
      {}) as MediaTrackSettings;
    const facingMode = settings.facingMode;

    if (
      facingMode === 'environment' ||
      typeof navigator.mediaDevices.enumerateDevices !== 'function'
    ) {
      return stream;
    }

    try {
      const devices = await navigator.mediaDevices.enumerateDevices();
      const backCamera = devices.find(
        (device) =>
          device.kind === 'videoinput' &&
          /back|rear|environment/i.test(device.label),
      );

      if (!backCamera) {
        return stream;
      }

      const videoOnlyStream = await navigator.mediaDevices.getUserMedia({
        video: {deviceId: {exact: backCamera.deviceId}},
        audio: false,
      });
      const replacementTrack = videoOnlyStream.getVideoTracks()[0];
      if (!replacementTrack) {
        videoOnlyStream.getTracks().forEach((track) => track.stop());
        return stream;
      }

      stream.addTrack(replacementTrack);
      stream.removeTrack(primaryVideoTrack);
      primaryVideoTrack.stop();

      return stream;
    } catch (fallbackError) {
      this.debug('Back camera fallback failed', fallbackError);
      return stream;
    }
  }

  private blobToBase64(blob: Blob): Promise<string> {
    return new Promise((resolve, reject) => {
      const reader = new FileReader();
      reader.onloadend = () => {
        if (typeof reader.result === 'string') {
          resolve(reader.result.split(',')[1]);
        } else {
          reject(new Error('Failed to read blob as base64 string.'));
        }
      };
      reader.onerror = reject;
      reader.readAsDataURL(blob);
    });
  }

  private async startRecordingWithStream(stream: MediaStream) {
    if (this.isRecording) {
      stream.getTracks().forEach((t) => t.stop());
      return;
    }

    if (!this.inputAudioContext || !this.outputAudioContext || !this.inputNode || !this.outputNode) {
      throw new Error('Audio contexts not initialized');
    }

    if (!this.sessionPromise) {
      this.sessionPromise = this.initSession();
    }

    this.appStopped = false;
    this.framesSent = 0;

    this.mediaStream = stream;

    this.videoElement.srcObject = this.mediaStream;
    try {
      await this.videoElement.play();
    } catch (e) {
      console.warn('video.play() failed', e);
    }

    const vt = this.mediaStream.getVideoTracks()[0];
    const at = this.mediaStream.getAudioTracks()[0];
    this.updateStatus('Media access granted. v=' + (!!vt) + ' a=' + (!!at));

    // Ensure we have an audio track before proceeding
    if (!at) {
      throw new Error('No audio track available in media stream');
    }

    // Ensure audio track is enabled and ready
    if (!at.enabled) {
      at.enabled = true;
    }

    // Ensure audio track is ready - wait a bit if needed
    let trackReadyState = at.readyState;
    if (trackReadyState !== 'live') {
      // Wait for track to become live (max 1 second)
      let attempts = 0;
      while (trackReadyState !== 'live' && attempts < 10) {
        await new Promise(resolve => setTimeout(resolve, 100));
        trackReadyState = at.readyState;
        attempts++;
      }
      if (trackReadyState !== 'live') {
        throw new Error('Audio track is not live. State: ' + trackReadyState);
      }
    }

    // Create media stream source only after audio context is running
    if (this.inputAudioContext.state === 'running') {
      try {
        this.sourceNode = this.inputAudioContext.createMediaStreamSource(
          this.mediaStream,
        );
        this.sourceNode.connect(this.inputNode);
      } catch (e) {
        throw new Error('Failed to create media stream source: ' + (e as Error).message);
      }
    } else {
      throw new Error('Audio context is not running. State: ' + this.inputAudioContext.state);
    }

    const bufferSize = 4096;
    this.scriptProcessorNode = this.inputAudioContext.createScriptProcessor(
      bufferSize,
      1,
      1,
    );

    this.scriptProcessorNode.onaudioprocess = (audioProcessingEvent) => {
      if (!this.isRecording) return;
      if (!this.isHolding) return;

      const inputBuffer = audioProcessingEvent.inputBuffer;
      const pcmData = inputBuffer.getChannelData(0);
      const srcRate = this.inputAudioContext.sampleRate;
      const downsampleTo16k = (float32: Float32Array, inRate: number) => {
        if (inRate === 16000) return float32;
        const ratio = inRate / 16000;
        const newLen = Math.floor(float32.length / ratio);
        const result = new Float32Array(newLen);
        let idx = 0, pos = 0;
        while (idx < newLen) {
          const nextPos = Math.min(float32.length - 1, (idx + 1) * ratio);
          let sum = 0, count = 0;
          for (let i = pos; i < nextPos; i++) { sum += float32[i]; count++; }
          result[idx++] = sum / Math.max(1, count);
          pos = nextPos;
        }
        return result;
      };
      const mono16k = srcRate === 16000 ? pcmData : downsampleTo16k(pcmData, srcRate);

      const sessionPromise = this.sessionPromise;
      if (!sessionPromise) {
        return;
      }

      sessionPromise
        .then((session) => {
          session.sendRealtimeInput({media: createBlob(mono16k)});
        })
        .catch((e) => this.updateError('Session send failed: ' + e.message));
    };

    this.sourceNode.connect(this.scriptProcessorNode);
    this.scriptProcessorNode.connect(this.inputAudioContext.destination);

    this.frameInterval = window.setInterval(() => {
      if (!this.isRecording) return;
      const context = this.canvasElement.getContext('2d');
      this.canvasElement.width = this.videoElement.videoWidth;
      this.canvasElement.height = this.videoElement.videoHeight;
      context.drawImage(
        this.videoElement,
        0,
        0,
        this.canvasElement.width,
        this.canvasElement.height,
      );
      this.canvasElement.toBlob(
        async (blob) => {
          if (blob) {
            const base64Data = await this.blobToBase64(blob);
            const sessionPromise = this.sessionPromise;
            if (!sessionPromise) {
              return;
            }

            sessionPromise
              .then((session) => {
                session.sendRealtimeInput({
                  media: {data: base64Data, mimeType: 'image/jpeg'},
                });
                this.framesSent++;
                if (this.framesSent % 10 === 0) {
                  this.debug(`Frames sent: ${this.framesSent}`);
                }
              })
              .catch((e) =>
                this.updateError('Session send (image) failed: ' + e.message),
              );
          }
        },
        'image/jpeg',
        0.8,
      );
    }, 500); // 2 frames per second

    this.isRecording = true;
    this.updateStatus('');
  }

  private async startRecording() {
    if (this.isRecording) {
      return;
    }

    if (!this.inputAudioContext || !this.outputAudioContext) {
      throw new Error('Audio contexts not initialized');
    }

    const inputResumePromise =
      this.inputAudioContext.state === 'running'
        ? Promise.resolve()
        : this.inputAudioContext.resume();
    const outputResumePromise =
      this.outputAudioContext.state === 'running'
        ? Promise.resolve()
        : this.outputAudioContext.resume();

    this.updateStatus('Requesting camera/mic access...');

    try {
      const preferBackCamera = async (): Promise<MediaStream> => {
        const constraints: MediaStreamConstraints = {
          audio: {
            channelCount: 1,
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true,
          },
          video: {
            facingMode: { ideal: 'environment' },
          },
        };
        this.debug('getUserMedia with constraints', constraints);
        try {
          const stream = await navigator.mediaDevices.getUserMedia(constraints);
          const track = stream.getVideoTracks()[0];
          const settings = track?.getSettings?.() || {} as MediaTrackSettings;
          this.debug('Acquired stream video settings', settings);
          if (settings.facingMode !== 'environment') {
            // Try enumerate fallback to enforce back camera
            const devices = await navigator.mediaDevices.enumerateDevices();
            const back = devices.find(
              (d) => d.kind === 'videoinput' && /back|rear|environment/i.test(d.label),
            );
            if (back) {
              try {
                const enforced = await navigator.mediaDevices.getUserMedia({
                  audio: constraints.audio,
                  video: { deviceId: { exact: back.deviceId } },
                });
                this.debug('Enforced back camera via deviceId', back);
                stream.getTracks().forEach((t) => t.stop());
                return enforced;
              } catch {
                // fall through to original stream
              }
            }
          }
          return stream;
        } catch (e) {
          const { name, message } = this.formatErrorDescriptor(e);
          this.updateError(`getUserMedia failed: ${name} - ${message}`);
          const devices = await navigator.mediaDevices.enumerateDevices();
          const back = devices.find(
            (d) => d.kind === 'videoinput' && /back|rear|environment/i.test(d.label),
          );
          if (back) {
            return await navigator.mediaDevices.getUserMedia({
              audio: constraints.audio,
              video: { deviceId: { exact: back.deviceId } },
            });
          }
          throw e;
        }
      };

      let stream: MediaStream | null = null;
      try {
        stream = await preferBackCamera();
        await Promise.all([inputResumePromise, outputResumePromise]);
        await this.startRecordingWithStream(stream);
      } catch (error) {
        if (stream) {
          stream.getTracks().forEach((t) => t.stop());
        }
        throw error;
      }
    } catch (err) {
      console.error('Error starting recording:', err);
      const { name, message } = this.formatErrorDescriptor(err);
      this.updateError(`Camera/mic activation failed: ${name} - ${message}`);
      this.stopRecording();
    }
  }

  private stopRecording() {
    if (!this.isRecording && !this.mediaStream && !this.inputAudioContext)
      return;

    this.updateStatus('');

    this.isRecording = false;

    if (this.frameInterval) {
      clearInterval(this.frameInterval);
      this.frameInterval = null;
    }

    if (this.scriptProcessorNode && this.sourceNode && this.inputAudioContext) {
      this.scriptProcessorNode.disconnect();
      this.sourceNode.disconnect();
    }

    this.scriptProcessorNode = null;
    this.sourceNode = null;

    if (this.mediaStream) {
      this.mediaStream.getTracks().forEach((track) => track.stop());
      this.mediaStream = null;
    }

    if (this.videoElement) {
      this.videoElement.srcObject = null;
    }

    this.framesSent = 0;
    this.updateStatus('');
  }

  private stopApplication() {
    if (this.appStopped) {
      return;
    }

    this.stopRecording();

    for (const source of this.sources.values()) {
      try {
        source.stop();
      } catch {}
      this.sources.delete(source);
    }

    const sessionPromise = this.sessionPromise;
    this.sessionPromise = undefined;

    sessionPromise
      ?.then((session) => session.close())
      .catch((e) => this.updateError('Session close failed: ' + e.message));

    this.videoElement?.pause();
    if (this.videoElement) {
      this.videoElement.srcObject = null;
    }

    this.appStopped = true;
    this.updateStatus('Session ended');
  }

  private async restartApplication() {
    if (this.isRecording) {
      return;
    }

    this.updateStatus('Restarting session...');
    this.sessionPromise = this.initSession();
    this.appStopped = false;

    try {
      await this.startRecording();
    } catch (error) {
      this.updateError('Restart failed: ' + (error as Error).message);
    }
  }

  private reset() {
    this.sessionPromise?.then((session) => session.close());
    this.sessionPromise = this.initSession();
    this.updateStatus('');
  }

  private async handleStartSession() {
    if (this.sessionStarted) {
      return;
    }
    
    this.sessionStarted = true;
    this.updateStatus('Requesting camera/mic access...');
    
    let stream: MediaStream | null = null;
    
    try {
      // Create audio contexts within user gesture for iOS PWA compatibility
      // This MUST be done synchronously from the event handler
      if (!this.inputAudioContext) {
        this.inputAudioContext = new (window.AudioContext ||
          (window as any).webkitAudioContext)({sampleRate: 16000});
      }
      if (!this.outputAudioContext) {
        this.outputAudioContext = new (window.AudioContext ||
          (window as any).webkitAudioContext)({sampleRate: 24000});
      }
      
      // Create gain nodes if they don't exist
      if (!this.inputNode) {
        this.inputNode = this.inputAudioContext.createGain();
        this.inputNode.gain.value = 0.0; // Start with mic muted
      }
      if (!this.outputNode) {
        this.outputNode = this.outputAudioContext.createGain();
        this.outputNode.connect(this.outputAudioContext.destination);
      }
      
      // Initialize session if not already done
      if (!this.sessionPromise) {
        this.sessionPromise = this.initSession();
      }
      
      // Request permissions directly from user gesture handler
      // This MUST be called synchronously from the event handler on iOS
      const constraints: MediaStreamConstraints = {
        audio: {
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
        },
        video: {
          facingMode: { ideal: 'environment' },
        },
      };
      const inputResumePromise =
        this.inputAudioContext.state === 'running'
          ? Promise.resolve()
          : this.inputAudioContext.resume();
      const outputResumePromise =
        this.outputAudioContext.state === 'running'
          ? Promise.resolve()
          : this.outputAudioContext.resume();
      
      // Call getUserMedia immediately to preserve user gesture context
      stream = await navigator.mediaDevices.getUserMedia(constraints);
      
      await Promise.all([inputResumePromise, outputResumePromise]);
      
      // Small delay to ensure audio contexts are fully ready
      await new Promise(resolve => setTimeout(resolve, 100));
      
      // Initialize audio timing
      this.initAudio();
      
      // Now start recording with the stream
      await this.startRecordingWithStream(stream);
    } catch (err) {
      console.error('Error starting session:', err);
      const { name, message } = this.formatErrorDescriptor(err);
      this.updateError(`Failed to access camera/microphone: ${name} - ${message}`);
      this.updateStatus('');
      this.sessionStarted = false; // Allow retry on error
      
      // Clean up stream if it was created
      if (stream) {
        stream.getTracks().forEach((t) => t.stop());
      }
    }
  }

  private async handleHoldStart() {
    if (!this.isRecording) {
      if (this.appStopped) {
        await this.restartApplication();
      } else {
        try {
          await this.startRecording();
        } catch (err) {
          console.error('Error starting recording:', err);
          return;
        }
      }
    }
    
    this.isHolding = true;
    if (this.inputNode) {
      this.inputNode.gain.value = 1.0;
    }
  }

  private handleHoldEnd() {
    this.isHolding = false;
    if (this.inputNode) {
      this.inputNode.gain.value = 0.0;
    }
  }

  render() {
    return html`
      <div class="layout">
        <video
          id="video-background"
          autoplay
          muted
          playsinline
          ?hidden=${this.appStopped}></video>
        <canvas id="frame-canvas" style="display:none"></canvas>
        ${this.error
          ? html`<div class="error-banner" role="alert">${this.error}</div>`
          : null}
        ${!this.sessionStarted
          ? html`
              <button
                class="start-button"
                type="button"
                aria-label="Start Session"
                @click=${this.handleStartSession}
                @touchstart=${(e: TouchEvent) => {
                  e.preventDefault();
                  this.handleStartSession();
                }}>
                Start Session
              </button>
            `
          : null}
        <div class="speak-area">
          ${this.inputNode && this.outputNode
            ? html`<gdm-live-audio-visuals-3d
                .inputNode=${this.inputNode}
                .outputNode=${this.outputNode}></gdm-live-audio-visuals-3d>`
            : null}
          ${this.appStopped
            ? html`<div class="session-overlay">Session Ended</div>`
            : null}
          <button
            class="speak-button"
            type="button"
            aria-pressed=${this.isHolding}
            aria-label=${this.isHolding ? 'Release to mute' : 'Hold to speak'}
            @touchstart=${(e: TouchEvent) => {
              e.preventDefault();
              this.handleHoldStart();
            }}
            @touchend=${(e: TouchEvent) => {
              e.preventDefault();
              this.handleHoldEnd();
            }}
            @touchcancel=${(e: TouchEvent) => {
              e.preventDefault();
              this.handleHoldEnd();
            }}
            @mousedown=${(e: MouseEvent) => {
              e.preventDefault();
              this.handleHoldStart();
            }}
            @mouseup=${(e: MouseEvent) => {
              e.preventDefault();
              this.handleHoldEnd();
            }}
            @mouseleave=${(e: MouseEvent) => {
              e.preventDefault();
              this.handleHoldEnd();
            }}>
            <span class="speak-label">
              ${this.isHolding ? 'Release to Mute' : 'Hold to Speak'}
            </span>
          </button>
        </div>
      </div>
    `;
  }
}

```

```typescriptreact
/* tslint:disable */
/**
 * @license
 * SPDX-License-Identifier: Apache-2.0
 */

import {GoogleGenAI, LiveServerMessage, Modality, Session} from '@google/genai';
import {LitElement, css, html} from 'lit';
import {customElement, state} from 'lit/decorators.js';
import {createBlob, decode, decodeAudioData} from './utils';
import './visual-3d';
// PWA: register service worker if available
try {
  // dynamic import optional; vite-plugin-pwa injects virtual module
  // eslint-disable-next-line @typescript-eslint/ban-ts-GB
  // @ts-ignore
  import('virtual:pwa-register').then(({ registerSW }) => {
    registerSW({ immediate: true });
  }).catch(() => {});
} catch {}

@customElement('gdm-live-audio')
export class GdmLiveAudio extends LitElement {
  @state() isRecording = false;
  @state() isHolding = false;
  @state() status = '';
  @state() error = '';
  @state() appStopped = false;
  @state() sessionStarted = false;

  private client: GoogleGenAI;
  private sessionPromise?: Promise<Session>;
  // Audio contexts created on user gesture for iOS PWA compatibility
  private inputAudioContext?: AudioContext;
  private outputAudioContext?: AudioContext;
  @state() inputNode?: GainNode;
  @state() outputNode?: GainNode;
  private nextStartTime = 0;
  private mediaStream: MediaStream;
  private sourceNode: AudioNode;
  private scriptProcessorNode: ScriptProcessorNode;
  private sources = new Set<AudioBufferSourceNode>();
  private frameInterval: number;
  private videoElement: HTMLVideoElement;
  private canvasElement: HTMLCanvasElement;
  private framesSent = 0;

  static styles = css`
    :host {
      display: block;
      position: relative;
      width: 100vw;
      height: 100vh;
      min-height: 100dvh;
      background: #000000;
      color: #ffffff;
      overflow: hidden;
    }

    @supports (height: 100dvh) {
      :host {
        height: 100dvh;
      }
    }

    .layout {
      position: relative;
      width: 100%;
      height: 100%;
    }

    #video-background {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 66.667vh;
      object-fit: cover;
    }

    .speak-area {
      position: absolute;
      bottom: 0;
      left: 0;
      width: 100%;
      height: calc(33.333vh + env(safe-area-inset-bottom));
      border-top-left-radius: 32px;
      border-top-right-radius: 32px;
      border: 1px solid rgba(255, 255, 255, 0.08);
      border-bottom: none;
      overflow: hidden;
      background: rgba(0, 0, 0, 0.3);
      backdrop-filter: blur(6px);
    }

    .session-overlay {
      position: absolute;
      inset: 0;
      display: flex;
      align-items: center;
      justify-content: center;
      padding: 24px;
      font-size: 18px;
      letter-spacing: 0.1em;
      text-transform: uppercase;
      color: rgba(255, 255, 255, 0.92);
      background: linear-gradient(
        180deg,
        rgba(17, 24, 39, 0.6) 0%,
        rgba(15, 23, 42, 0.85) 100%
      );
      pointer-events: none;
      text-align: center;
    }

    gdm-live-audio-visuals-3d {
      display: block;
      width: 100%;
      height: 100%;
      pointer-events: none;
      border-top-left-radius: inherit;
      border-top-right-radius: inherit;
    }

    .speak-button {
      position: absolute;
      inset: 0;
      width: 100%;
      height: 100%;
      border: none;
      background: transparent;
      cursor: pointer;
      display: flex;
      align-items: flex-end;
      justify-content: center;
      padding: 24px 24px calc(24px + env(safe-area-inset-bottom));
      color: inherit;
      transition: background 0.2s ease;
      -webkit-touch-callout: none;
      -webkit-user-select: none;
      -moz-user-select: none;
      -ms-user-select: none;
      user-select: none;
      -webkit-tap-highlight-color: transparent;
      touch-action: manipulation;
    }

    .speak-button:hover,
    .speak-button:focus-visible {
      background: rgba(255, 255, 255, 0.04);
    }

    .speak-button:focus-visible {
      outline: 2px solid rgba(255, 255, 255, 0.25);
      outline-offset: 4px;
    }

    .speak-label {
      font-size: 18px;
      letter-spacing: 0.08em;
      text-transform: uppercase;
      color: rgba(255, 255, 255, 0.92);
      text-shadow: 0 0 6px rgba(0, 0, 0, 0.6);
      -webkit-touch-callout: none;
      -webkit-user-select: none;
      -moz-user-select: none;
      -ms-user-select: none;
      user-select: none;
      pointer-events: none;
    }

    .error-banner {
      position: absolute;
      top: calc(env(safe-area-inset-top) + 12px);
      left: 16px;
      right: 16px;
      z-index: 10;
      padding: 12px 16px;
      background: rgba(220, 38, 38, 0.85);
      color: #ffffff;
      border-radius: 12px;
      text-align: center;
      font-size: 14px;
      line-height: 1.4;
      box-shadow: 0 12px 24px rgba(0, 0, 0, 0.35);
      backdrop-filter: blur(12px);
    }

    .start-button {
      position: absolute;
      top: 50%;
      left: 50%;
      transform: translate(-50%, -50%);
      z-index: 1000;
      padding: 20px 40px;
      background: rgba(255, 255, 255, 0.25);
      color: #ffffff;
      border: 2px solid rgba(255, 255, 255, 0.5);
      border-radius: 16px;
      font-size: 18px;
      font-weight: 500;
      letter-spacing: 0.05em;
      text-transform: uppercase;
      cursor: pointer;
      transition: all 0.2s ease;
      backdrop-filter: blur(12px);
      box-shadow: 0 8px 24px rgba(0, 0, 0, 0.6), 0 0 0 1px rgba(255, 255, 255, 0.1);
      -webkit-tap-highlight-color: transparent;
      touch-action: manipulation;
      white-space: nowrap;
    }

    .start-button:hover,
    .start-button:focus-visible {
      background: rgba(255, 255, 255, 0.25);
      border-color: rgba(255, 255, 255, 0.5);
      transform: translate(-50%, -50%) scale(1.05);
    }

    .start-button:active {
      transform: translate(-50%, -50%) scale(0.98);
    }

    .start-button:focus-visible {
      outline: 2px solid rgba(255, 255, 255, 0.5);
      outline-offset: 4px;
    }
  `;

  constructor() {
    super();
    this.initClient();
  }

  protected firstUpdated() {
    this.videoElement = this.shadowRoot!.querySelector('#video-background');
    this.canvasElement = this.shadowRoot!.querySelector('#frame-canvas');
    this.debug(
      `Boot: secure=${window.isSecureContext} ua=${navigator.userAgent}`,
    );
    this.checkPermissions();
    this.addEventListener('click', async () => {
      try {
        if (this.outputAudioContext) {
          await this.outputAudioContext.resume();
        }
        if (this.inputAudioContext) {
          await this.inputAudioContext.resume();
        }
      } catch {}
    });
  }

  private async checkPermissions() {
    const parts: string[] = [];
    const anyNav: any = navigator as any;
    if (anyNav.permissions?.query) {
      try {
        const mic = await anyNav.permissions.query({name: 'microphone'});
        parts.push(`mic=${mic.state}`);
      } catch {}
      try {
        const cam = await anyNav.permissions.query({name: 'camera'});
        parts.push(`cam=${cam.state}`);
      } catch {}
    }
    if (parts.length) this.updateStatus(`Permissions: ${parts.join(' ')}`);
  }

  private debug(_msg: string, _data?: unknown) {
    // Debug logging disabled for production UI clarity.
  }

  private initAudio() {
    if (this.outputAudioContext && this.inputNode) {
      this.nextStartTime = this.outputAudioContext.currentTime;
      this.inputNode.gain.value = 0.0; // Start with mic muted
    }
  }

  private async initClient() {
    if (!process.env.API_KEY || process.env.API_KEY === 'undefined') {
      this.updateError('Missing GEMINI_API_KEY');
    }

    this.client = new GoogleGenAI({apiKey: process.env.API_KEY});

    // Audio contexts and nodes will be created on user gesture
    // Session will be initialized when user starts session
  }

  private initSession(): Promise<Session> {
    const model = 'gemini-2.5-flash-native-audio-preview-09-2025';

    try {
      return this.client.live.connect({
        model: model,
        callbacks: {
          onopen: () => {
            this.debug('Live session opened');
          },
          onmessage: async (message: LiveServerMessage) => {
            this.debug('Live message received');
            const audio =
              message.serverContent?.modelTurn?.parts[0]?.inlineData;

            if (audio && this.outputAudioContext && this.outputNode) {
              this.nextStartTime = Math.max(
                this.nextStartTime,
                this.outputAudioContext.currentTime,
              );

              const audioBuffer = await decodeAudioData(
                decode(audio.data),
                this.outputAudioContext,
                24000,
                1,
              );
              const source = this.outputAudioContext.createBufferSource();
              source.buffer = audioBuffer;
              source.connect(this.outputNode);
              source.addEventListener('ended', () => {
                this.sources.delete(source);
              });

              source.start(this.nextStartTime);
              this.nextStartTime = this.nextStartTime + audioBuffer.duration;
              this.sources.add(source);
            }

            const interrupted = message.serverContent?.interrupted;
            if (interrupted) {
              for (const source of this.sources.values()) {
                source.stop();
                this.sources.delete(source);
              }
              this.nextStartTime = 0;
            }
          },
          onerror: (e: ErrorEvent) => {
            this.updateError('Live error: ' + e.message);
            console.error('Live error event', e);
          },
          onclose: (e: CloseEvent) => {
            this.updateStatus('Live close: ' + e.reason + ' (' + e.code + ')');
            console.warn('Live close', e);
          },
        },
        config: {
          responseModalities: [Modality.AUDIO],
          systemInstruction: {
            parts: [
              {
                text: `You are an AI assistant integrated into an application where the main interface is a live camera view. The user interacts with you by pressing and holding a large button at the bottom of the screen. When the button is active, you listen to the user's voice input and provide an answer based on what is visible through the camera.

Your task is to analyze the live camera feed and interpret the user’s spoken request in combination. Always respond with precise, spatially grounded instructions that reference items or locations visible on the screen.

When giving instructions:

Be specific and concise.

Refer to locations using clear spatial language (e.g., “third shelf from the top,” “top right corner,” “behind the red box,” etc.).

Only describe what is visible on the camera or can be reasonably inferred from it.

Do not invent objects or locations that are not present in the scene.

Keep answers straightforward and helpful.

Example:
If the user asks, “Where is the gluten-free bread?” and it is visible through the camera, respond with a description such as:
“The gluten-free bread is on the third shelf from the top, in the top right corner.”

Your goal is to help the user locate objects in the environment accurately and efficiently based on real-time visual input.`,
              },
            ],
          },
          speechConfig: {
            voiceConfig: {prebuiltVoiceConfig: {voiceName: 'Zephyr'}},
            // languageCode: 'en-GB'
          },
        },
      });
    } catch (e) {
      console.error(e);
      return Promise.reject(e);
    }
  }

  private updateStatus(msg: string) {
    this.status = msg;
  }

  private updateError(msg: string) {
    this.error = msg;
  }

  private formatErrorDescriptor(error: unknown): { name: string; message: string } {
    if (error instanceof DOMException) {
      return { name: error.name, message: error.message };
    }
    if (error instanceof Error) {
      return { name: error.name || 'Error', message: error.message };
    }
    return { name: 'Error', message: String(error) };
  }

  private async acquireMediaStream(): Promise<MediaStream> {
    const constraints: MediaStreamConstraints = {
      audio: {
        channelCount: 1,
        echoCancellation: true,
        noiseSuppression: true,
        autoGainControl: true,
      },
      video: {
        facingMode: { ideal: 'environment' },
      },
    };

    const stream = await navigator.mediaDevices.getUserMedia(constraints);
    const primaryVideoTrack = stream.getVideoTracks()[0];
    if (!primaryVideoTrack) {
      return stream;
    }

    const settings = (primaryVideoTrack.getSettings?.() ??
      {}) as MediaTrackSettings;
    const facingMode = settings.facingMode;

    if (
      facingMode === 'environment' ||
      typeof navigator.mediaDevices.enumerateDevices !== 'function'
    ) {
      return stream;
    }

    try {
      const devices = await navigator.mediaDevices.enumerateDevices();
      const backCamera = devices.find(
        (device) =>
          device.kind === 'videoinput' &&
          /back|rear|environment/i.test(device.label),
      );

      if (!backCamera) {
        return stream;
      }

      const videoOnlyStream = await navigator.mediaDevices.getUserMedia({
        video: {deviceId: {exact: backCamera.deviceId}},
        audio: false,
      });
      const replacementTrack = videoOnlyStream.getVideoTracks()[0];
      if (!replacementTrack) {
        videoOnlyStream.getTracks().forEach((track) => track.stop());
        return stream;
      }

      stream.addTrack(replacementTrack);
      stream.removeTrack(primaryVideoTrack);
      primaryVideoTrack.stop();

      return stream;
    } catch (fallbackError) {
      this.debug('Back camera fallback failed', fallbackError);
      return stream;
    }
  }

  private blobToBase64(blob: Blob): Promise<string> {
    return new Promise((resolve, reject) => {
      const reader = new FileReader();
      reader.onloadend = () => {
        if (typeof reader.result === 'string') {
          resolve(reader.result.split(',')[1]);
        } else {
          reject(new Error('Failed to read blob as base64 string.'));
        }
      };
      reader.onerror = reject;
      reader.readAsDataURL(blob);
    });
  }

  private async startRecordingWithStream(stream: MediaStream) {
    if (this.isRecording) {
      stream.getTracks().forEach((t) => t.stop());
      return;
    }

    if (!this.inputAudioContext || !this.outputAudioContext || !this.inputNode || !this.outputNode) {
      throw new Error('Audio contexts not initialized');
    }

    if (!this.sessionPromise) {
      this.sessionPromise = this.initSession();
    }

    this.appStopped = false;
    this.framesSent = 0;

    this.mediaStream = stream;

    this.videoElement.srcObject = this.mediaStream;
    try {
      await this.videoElement.play();
    } catch (e) {
      console.warn('video.play() failed', e);
    }

    const vt = this.mediaStream.getVideoTracks()[0];
    const at = this.mediaStream.getAudioTracks()[0];
    this.updateStatus('Media access granted. v=' + (!!vt) + ' a=' + (!!at));

    // Ensure we have an audio track before proceeding
    if (!at) {
      throw new Error('No audio track available in media stream');
    }

    // Ensure audio track is enabled and ready
    if (!at.enabled) {
      at.enabled = true;
    }

    // Ensure audio track is ready - wait a bit if needed
    let trackReadyState = at.readyState;
    if (trackReadyState !== 'live') {
      // Wait for track to become live (max 1 second)
      let attempts = 0;
      while (trackReadyState !== 'live' && attempts < 10) {
        await new Promise(resolve => setTimeout(resolve, 100));
        trackReadyState = at.readyState;
        attempts++;
      }
      if (trackReadyState !== 'live') {
        throw new Error('Audio track is not live. State: ' + trackReadyState);
      }
    }

    // Create media stream source only after audio context is running
    if (this.inputAudioContext.state === 'running') {
      try {
        this.sourceNode = this.inputAudioContext.createMediaStreamSource(
          this.mediaStream,
        );
        this.sourceNode.connect(this.inputNode);
      } catch (e) {
        throw new Error('Failed to create media stream source: ' + (e as Error).message);
      }
    } else {
      throw new Error('Audio context is not running. State: ' + this.inputAudioContext.state);
    }

    const bufferSize = 4096;
    this.scriptProcessorNode = this.inputAudioContext.createScriptProcessor(
      bufferSize,
      1,
      1,
    );

    this.scriptProcessorNode.onaudioprocess = (audioProcessingEvent) => {
      if (!this.isRecording) return;
      if (!this.isHolding) return;

      const inputBuffer = audioProcessingEvent.inputBuffer;
      const pcmData = inputBuffer.getChannelData(0);
      const srcRate = this.inputAudioContext.sampleRate;
      const downsampleTo16k = (float32: Float32Array, inRate: number) => {
        if (inRate === 16000) return float32;
        const ratio = inRate / 16000;
        const newLen = Math.floor(float32.length / ratio);
        const result = new Float32Array(newLen);
        let idx = 0, pos = 0;
        while (idx < newLen) {
          const nextPos = Math.min(float32.length - 1, (idx + 1) * ratio);
          let sum = 0, count = 0;
          for (let i = pos; i < nextPos; i++) { sum += float32[i]; count++; }
          result[idx++] = sum / Math.max(1, count);
          pos = nextPos;
        }
        return result;
      };
      const mono16k = srcRate === 16000 ? pcmData : downsampleTo16k(pcmData, srcRate);

      const sessionPromise = this.sessionPromise;
      if (!sessionPromise) {
        return;
      }

      sessionPromise
        .then((session) => {
          session.sendRealtimeInput({media: createBlob(mono16k)});
        })
        .catch((e) => this.updateError('Session send failed: ' + e.message));
    };

    this.sourceNode.connect(this.scriptProcessorNode);
    this.scriptProcessorNode.connect(this.inputAudioContext.destination);

    this.frameInterval = window.setInterval(() => {
      if (!this.isRecording) return;
      const context = this.canvasElement.getContext('2d');
      this.canvasElement.width = this.videoElement.videoWidth;
      this.canvasElement.height = this.videoElement.videoHeight;
      context.drawImage(
        this.videoElement,
        0,
        0,
        this.canvasElement.width,
        this.canvasElement.height,
      );
      this.canvasElement.toBlob(
        async (blob) => {
          if (blob) {
            const base64Data = await this.blobToBase64(blob);
            const sessionPromise = this.sessionPromise;
            if (!sessionPromise) {
              return;
            }

            sessionPromise
              .then((session) => {
                session.sendRealtimeInput({
                  media: {data: base64Data, mimeType: 'image/jpeg'},
                });
                this.framesSent++;
                if (this.framesSent % 10 === 0) {
                  this.debug(`Frames sent: ${this.framesSent}`);
                }
              })
              .catch((e) =>
                this.updateError('Session send (image) failed: ' + e.message),
              );
          }
        },
        'image/jpeg',
        0.8,
      );
    }, 500); // 2 frames per second

    this.isRecording = true;
    this.updateStatus('');
  }

  private async startRecording() {
    if (this.isRecording) {
      return;
    }

    if (!this.inputAudioContext || !this.outputAudioContext) {
      throw new Error('Audio contexts not initialized');
    }

    const inputResumePromise =
      this.inputAudioContext.state === 'running'
        ? Promise.resolve()
        : this.inputAudioContext.resume();
    const outputResumePromise =
      this.outputAudioContext.state === 'running'
        ? Promise.resolve()
        : this.outputAudioContext.resume();

    this.updateStatus('Requesting camera/mic access...');

    try {
      let stream: MediaStream | null = null;
      try {
        stream = await this.acquireMediaStream();
        await Promise.all([inputResumePromise, outputResumePromise]);
        await this.startRecordingWithStream(stream);
      } catch (error) {
        if (stream) {
          stream.getTracks().forEach((t) => t.stop());
        }
        throw error;
      }
    } catch (err) {
      console.error('Error starting recording:', err);
      const { name, message } = this.formatErrorDescriptor(err);
      this.updateError(`Camera/mic activation failed: ${name} - ${message}`);
      this.stopRecording();
    }
  }

  private stopRecording() {
    if (!this.isRecording && !this.mediaStream && !this.inputAudioContext)
      return;

    this.updateStatus('');

    this.isRecording = false;

    if (this.frameInterval) {
      clearInterval(this.frameInterval);
      this.frameInterval = null;
    }

    if (this.scriptProcessorNode && this.sourceNode && this.inputAudioContext) {
      this.scriptProcessorNode.disconnect();
      this.sourceNode.disconnect();
    }

    this.scriptProcessorNode = null;
    this.sourceNode = null;

    if (this.mediaStream) {
      this.mediaStream.getTracks().forEach((track) => track.stop());
      this.mediaStream = null;
    }

    if (this.videoElement) {
      this.videoElement.srcObject = null;
    }

    this.framesSent = 0;
    this.updateStatus('');
  }

  private stopApplication() {
    if (this.appStopped) {
      return;
    }

    this.stopRecording();

    for (const source of this.sources.values()) {
      try {
        source.stop();
      } catch {}
      this.sources.delete(source);
    }

    const sessionPromise = this.sessionPromise;
    this.sessionPromise = undefined;

    sessionPromise
      ?.then((session) => session.close())
      .catch((e) => this.updateError('Session close failed: ' + e.message));

    this.videoElement?.pause();
    if (this.videoElement) {
      this.videoElement.srcObject = null;
    }

    this.appStopped = true;
    this.updateStatus('Session ended');
  }

  private async restartApplication() {
    if (this.isRecording) {
      return;
    }

    this.updateStatus('Restarting session...');
    this.sessionPromise = this.initSession();
    this.appStopped = false;

    try {
      await this.startRecording();
    } catch (error) {
      this.updateError('Restart failed: ' + (error as Error).message);
    }
  }

  private reset() {
    this.sessionPromise?.then((session) => session.close());
    this.sessionPromise = this.initSession();
    this.updateStatus('');
  }

  private async handleStartSession() {
    if (this.sessionStarted) {
      return;
    }
    
    this.sessionStarted = true;
    this.updateStatus('Requesting camera/mic access...');
    
    let stream: MediaStream | null = null;
    
    try {
      // Create audio contexts within user gesture for iOS PWA compatibility
      // This MUST be done synchronously from the event handler
      if (!this.inputAudioContext) {
        this.inputAudioContext = new (window.AudioContext ||
          (window as any).webkitAudioContext)({sampleRate: 16000});
      }
      if (!this.outputAudioContext) {
        this.outputAudioContext = new (window.AudioContext ||
          (window as any).webkitAudioContext)({sampleRate: 24000});
      }
      
      // Create gain nodes if they don't exist
      if (!this.inputNode) {
        this.inputNode = this.inputAudioContext.createGain();
        this.inputNode.gain.value = 0.0; // Start with mic muted
      }
      if (!this.outputNode) {
        this.outputNode = this.outputAudioContext.createGain();
        this.outputNode.connect(this.outputAudioContext.destination);
      }
      
      // Initialize session if not already done
      if (!this.sessionPromise) {
        this.sessionPromise = this.initSession();
      }
      
      // Request permissions directly from user gesture handler
      // This MUST be called synchronously from the event handler on iOS
      const constraints: MediaStreamConstraints = {
        audio: {
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
        },
        video: {
          facingMode: { ideal: 'environment' },
        },
      };
      const inputResumePromise =
        this.inputAudioContext.state === 'running'
          ? Promise.resolve()
          : this.inputAudioContext.resume();
      const outputResumePromise =
        this.outputAudioContext.state === 'running'
          ? Promise.resolve()
          : this.outputAudioContext.resume();
      
      // Call getUserMedia immediately to preserve user gesture context
      stream = await navigator.mediaDevices.getUserMedia(constraints);
      
      await Promise.all([inputResumePromise, outputResumePromise]);
      
      // Small delay to ensure audio contexts are fully ready
      await new Promise(resolve => setTimeout(resolve, 100));
      
      // Initialize audio timing
      this.initAudio();
      
      // Now start recording with the stream
      await this.startRecordingWithStream(stream);
    } catch (err) {
      console.error('Error starting session:', err);
      const { name, message } = this.formatErrorDescriptor(err);
      this.updateError(`Failed to access camera/microphone: ${name} - ${message}`);
      this.updateStatus('');
      this.sessionStarted = false; // Allow retry on error
      
      // Clean up stream if it was created
      if (stream) {
        stream.getTracks().forEach((t) => t.stop());
      }
    }
  }

  private async handleHoldStart() {
    if (!this.isRecording) {
      if (this.appStopped) {
        await this.restartApplication();
      } else {
        try {
          await this.startRecording();
        } catch (err) {
          console.error('Error starting recording:', err);
          return;
        }
      }
    }
    
    this.isHolding = true;
    if (this.inputNode) {
      this.inputNode.gain.value = 1.0;
    }
  }

  private handleHoldEnd() {
    this.isHolding = false;
    if (this.inputNode) {
      this.inputNode.gain.value = 0.0;
    }
  }

  render() {
    return html`
      <div class="layout">
        <video
          id="video-background"
          autoplay
          muted
          playsinline
          ?hidden=${this.appStopped}></video>
        <canvas id="frame-canvas" style="display:none"></canvas>
        ${this.error
          ? html`<div class="error-banner" role="alert">${this.error}</div>`
          : null}
        ${!this.sessionStarted
          ? html`
              <button
                class="start-button"
                type="button"
                aria-label="Start Session"
                @click=${this.handleStartSession}
                @touchstart=${(e: TouchEvent) => {
                  e.preventDefault();
                  this.handleStartSession();
                }}>
                Start Session
              </button>
            `
          : null}
        <div class="speak-area">
          ${this.inputNode && this.outputNode
            ? html`<gdm-live-audio-visuals-3d
                .inputNode=${this.inputNode}
                .outputNode=${this.outputNode}></gdm-live-audio-visuals-3d>`
            : null}
          ${this.appStopped
            ? html`<div class="session-overlay">Session Ended</div>`
            : null}
          <button
            class="speak-button"
            type="button"
            aria-pressed=${this.isHolding}
            aria-label=${this.isHolding ? 'Release to mute' : 'Hold to speak'}
            @touchstart=${(e: TouchEvent) => {
              e.preventDefault();
              this.handleHoldStart();
            }}
            @touchend=${(e: TouchEvent) => {
              e.preventDefault();
              this.handleHoldEnd();
            }}
            @touchcancel=${(e: TouchEvent) => {
              e.preventDefault();
              this.handleHoldEnd();
            }}
            @mousedown=${(e: MouseEvent) => {
              e.preventDefault();
              this.handleHoldStart();
            }}
            @mouseup=${(e: MouseEvent) => {
              e.preventDefault();
              this.handleHoldEnd();
            }}
            @mouseleave=${(e: MouseEvent) => {
              e.preventDefault();
              this.handleHoldEnd();
            }}>
            <span class="speak-label">
              ${this.isHolding ? 'Release to Mute' : 'Hold to Speak'}
            </span>
          </button>
        </div>
      </div>
    `;
  }
}

```

```typescriptreact
/* tslint:disable */
/**
 * @license
 * SPDX-License-Identifier: Apache-2.0
 */

import {GoogleGenAI, LiveServerMessage, Modality, Session} from '@google/genai';
import {LitElement, css, html} from 'lit';
import {customElement, state} from 'lit/decorators.js';
import {createBlob, decode, decodeAudioData} from './utils';
import './visual-3d';
// PWA: register service worker if available
try {
  // dynamic import optional; vite-plugin-pwa injects virtual module
  // eslint-disable-next-line @typescript-eslint/ban-ts-GB
  // @ts-ignore
  import('virtual:pwa-register').then(({ registerSW }) => {
    registerSW({ immediate: true });
  }).catch(() => {});
} catch {}

@customElement('gdm-live-audio')
export class GdmLiveAudio extends LitElement {
  @state() isRecording = false;
  @state() isHolding = false;
  @state() status = '';
  @state() error = '';
  @state() appStopped = false;
  @state() sessionStarted = false;

  private client: GoogleGenAI;
  private sessionPromise?: Promise<Session>;
  // Audio contexts created on user gesture for iOS PWA compatibility
  private inputAudioContext?: AudioContext;
  private outputAudioContext?: AudioContext;
  @state() inputNode?: GainNode;
  @state() outputNode?: GainNode;
  private nextStartTime = 0;
  private mediaStream: MediaStream;
  private sourceNode: AudioNode;
  private scriptProcessorNode: ScriptProcessorNode;
  private sources = new Set<AudioBufferSourceNode>();
  private frameInterval: number;
  private videoElement: HTMLVideoElement;
  private canvasElement: HTMLCanvasElement;
  private framesSent = 0;

  static styles = css`
    :host {
      display: block;
      position: relative;
      width: 100vw;
      height: 100vh;
      min-height: 100dvh;
      background: #000000;
      color: #ffffff;
      overflow: hidden;
    }

    @supports (height: 100dvh) {
      :host {
        height: 100dvh;
      }
    }

    .layout {
      position: relative;
      width: 100%;
      height: 100%;
    }

    #video-background {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 66.667vh;
      object-fit: cover;
    }

    .speak-area {
      position: absolute;
      bottom: 0;
      left: 0;
      width: 100%;
      height: calc(33.333vh + env(safe-area-inset-bottom));
      border-top-left-radius: 32px;
      border-top-right-radius: 32px;
      border: 1px solid rgba(255, 255, 255, 0.08);
      border-bottom: none;
      overflow: hidden;
      background: rgba(0, 0, 0, 0.3);
      backdrop-filter: blur(6px);
    }

    .session-overlay {
      position: absolute;
      inset: 0;
      display: flex;
      align-items: center;
      justify-content: center;
      padding: 24px;
      font-size: 18px;
      letter-spacing: 0.1em;
      text-transform: uppercase;
      color: rgba(255, 255, 255, 0.92);
      background: linear-gradient(
        180deg,
        rgba(17, 24, 39, 0.6) 0%,
        rgba(15, 23, 42, 0.85) 100%
      );
      pointer-events: none;
      text-align: center;
    }

    gdm-live-audio-visuals-3d {
      display: block;
      width: 100%;
      height: 100%;
      pointer-events: none;
      border-top-left-radius: inherit;
      border-top-right-radius: inherit;
    }

    .speak-button {
      position: absolute;
      inset: 0;
      width: 100%;
      height: 100%;
      border: none;
      background: transparent;
      cursor: pointer;
      display: flex;
      align-items: flex-end;
      justify-content: center;
      padding: 24px 24px calc(24px + env(safe-area-inset-bottom));
      color: inherit;
      transition: background 0.2s ease;
      -webkit-touch-callout: none;
      -webkit-user-select: none;
      -moz-user-select: none;
      -ms-user-select: none;
      user-select: none;
      -webkit-tap-highlight-color: transparent;
      touch-action: manipulation;
    }

    .speak-button:hover,
    .speak-button:focus-visible {
      background: rgba(255, 255, 255, 0.04);
    }

    .speak-button:focus-visible {
      outline: 2px solid rgba(255, 255, 255, 0.25);
      outline-offset: 4px;
    }

    .speak-label {
      font-size: 18px;
      letter-spacing: 0.08em;
      text-transform: uppercase;
      color: rgba(255, 255, 255, 0.92);
      text-shadow: 0 0 6px rgba(0, 0, 0, 0.6);
      -webkit-touch-callout: none;
      -webkit-user-select: none;
      -moz-user-select: none;
      -ms-user-select: none;
      user-select: none;
      pointer-events: none;
    }

    .error-banner {
      position: absolute;
      top: calc(env(safe-area-inset-top) + 12px);
      left: 16px;
      right: 16px;
      z-index: 10;
      padding: 12px 16px;
      background: rgba(220, 38, 38, 0.85);
      color: #ffffff;
      border-radius: 12px;
      text-align: center;
      font-size: 14px;
      line-height: 1.4;
      box-shadow: 0 12px 24px rgba(0, 0, 0, 0.35);
      backdrop-filter: blur(12px);
    }

    .start-button {
      position: absolute;
      top: 50%;
      left: 50%;
      transform: translate(-50%, -50%);
      z-index: 1000;
      padding: 20px 40px;
      background: rgba(255, 255, 255, 0.25);
      color: #ffffff;
      border: 2px solid rgba(255, 255, 255, 0.5);
      border-radius: 16px;
      font-size: 18px;
      font-weight: 500;
      letter-spacing: 0.05em;
      text-transform: uppercase;
      cursor: pointer;
      transition: all 0.2s ease;
      backdrop-filter: blur(12px);
      box-shadow: 0 8px 24px rgba(0, 0, 0, 0.6), 0 0 0 1px rgba(255, 255, 255, 0.1);
      -webkit-tap-highlight-color: transparent;
      touch-action: manipulation;
      white-space: nowrap;
    }

    .start-button:hover,
    .start-button:focus-visible {
      background: rgba(255, 255, 255, 0.25);
      border-color: rgba(255, 255, 255, 0.5);
      transform: translate(-50%, -50%) scale(1.05);
    }

    .start-button:active {
      transform: translate(-50%, -50%) scale(0.98);
    }

    .start-button:focus-visible {
      outline: 2px solid rgba(255, 255, 255, 0.5);
      outline-offset: 4px;
    }
  `;

  constructor() {
    super();
    this.initClient();
  }

  protected firstUpdated() {
    this.videoElement = this.shadowRoot!.querySelector('#video-background');
    this.canvasElement = this.shadowRoot!.querySelector('#frame-canvas');
    this.debug(
      `Boot: secure=${window.isSecureContext} ua=${navigator.userAgent}`,
    );
    this.checkPermissions();
    this.addEventListener('click', async () => {
      try {
        if (this.outputAudioContext) {
          await this.outputAudioContext.resume();
        }
        if (this.inputAudioContext) {
          await this.inputAudioContext.resume();
        }
      } catch {}
    });
  }

  private async checkPermissions() {
    const parts: string[] = [];
    const anyNav: any = navigator as any;
    if (anyNav.permissions?.query) {
      try {
        const mic = await anyNav.permissions.query({name: 'microphone'});
        parts.push(`mic=${mic.state}`);
      } catch {}
      try {
        const cam = await anyNav.permissions.query({name: 'camera'});
        parts.push(`cam=${cam.state}`);
      } catch {}
    }
    if (parts.length) this.updateStatus(`Permissions: ${parts.join(' ')}`);
  }

  private debug(_msg: string, _data?: unknown) {
    // Debug logging disabled for production UI clarity.
  }

  private initAudio() {
    if (this.outputAudioContext && this.inputNode) {
      this.nextStartTime = this.outputAudioContext.currentTime;
      this.inputNode.gain.value = 0.0; // Start with mic muted
    }
  }

  private async initClient() {
    if (!process.env.API_KEY || process.env.API_KEY === 'undefined') {
      this.updateError('Missing GEMINI_API_KEY');
    }

    this.client = new GoogleGenAI({apiKey: process.env.API_KEY});

    // Audio contexts and nodes will be created on user gesture
    // Session will be initialized when user starts session
  }

  private initSession(): Promise<Session> {
    const model = 'gemini-2.5-flash-native-audio-preview-09-2025';

    try {
      return this.client.live.connect({
        model: model,
        callbacks: {
          onopen: () => {
            this.debug('Live session opened');
          },
          onmessage: async (message: LiveServerMessage) => {
            this.debug('Live message received');
            const audio =
              message.serverContent?.modelTurn?.parts[0]?.inlineData;

            if (audio && this.outputAudioContext && this.outputNode) {
              this.nextStartTime = Math.max(
                this.nextStartTime,
                this.outputAudioContext.currentTime,
              );

              const audioBuffer = await decodeAudioData(
                decode(audio.data),
                this.outputAudioContext,
                24000,
                1,
              );
              const source = this.outputAudioContext.createBufferSource();
              source.buffer = audioBuffer;
              source.connect(this.outputNode);
              source.addEventListener('ended', () => {
                this.sources.delete(source);
              });

              source.start(this.nextStartTime);
              this.nextStartTime = this.nextStartTime + audioBuffer.duration;
              this.sources.add(source);
            }

            const interrupted = message.serverContent?.interrupted;
            if (interrupted) {
              for (const source of this.sources.values()) {
                source.stop();
                this.sources.delete(source);
              }
              this.nextStartTime = 0;
            }
          },
          onerror: (e: ErrorEvent) => {
            this.updateError('Live error: ' + e.message);
            console.error('Live error event', e);
          },
          onclose: (e: CloseEvent) => {
            this.updateStatus('Live close: ' + e.reason + ' (' + e.code + ')');
            console.warn('Live close', e);
          },
        },
        config: {
          responseModalities: [Modality.AUDIO],
          systemInstruction: {
            parts: [
              {
                text: `You are an AI assistant integrated into an application where the main interface is a live camera view. The user interacts with you by pressing and holding a large button at the bottom of the screen. When the button is active, you listen to the user's voice input and provide an answer based on what is visible through the camera.

Your task is to analyze the live camera feed and interpret the user’s spoken request in combination. Always respond with precise, spatially grounded instructions that reference items or locations visible on the screen.

When giving instructions:

Be specific and concise.

Refer to locations using clear spatial language (e.g., “third shelf from the top,” “top right corner,” “behind the red box,” etc.).

Only describe what is visible on the camera or can be reasonably inferred from it.

Do not invent objects or locations that are not present in the scene.

Keep answers straightforward and helpful.

Example:
If the user asks, “Where is the gluten-free bread?” and it is visible through the camera, respond with a description such as:
“The gluten-free bread is on the third shelf from the top, in the top right corner.”

Your goal is to help the user locate objects in the environment accurately and efficiently based on real-time visual input.`,
              },
            ],
          },
          speechConfig: {
            voiceConfig: {prebuiltVoiceConfig: {voiceName: 'Zephyr'}},
            // languageCode: 'en-GB'
          },
        },
      });
    } catch (e) {
      console.error(e);
      return Promise.reject(e);
    }
  }

  private updateStatus(msg: string) {
    this.status = msg;
  }

  private updateError(msg: string) {
    this.error = msg;
  }

  private formatErrorDescriptor(error: unknown): { name: string; message: string } {
    if (error instanceof DOMException) {
      return { name: error.name, message: error.message };
    }
    if (error instanceof Error) {
      return { name: error.name || 'Error', message: error.message };
    }
    return { name: 'Error', message: String(error) };
  }

  private async acquireMediaStream(): Promise<MediaStream> {
    const constraints: MediaStreamConstraints = {
      audio: {
        channelCount: 1,
        echoCancellation: true,
        noiseSuppression: true,
        autoGainControl: true,
      },
      video: {
        facingMode: { ideal: 'environment' },
      },
    };

    const stream = await navigator.mediaDevices.getUserMedia(constraints);
    const primaryVideoTrack = stream.getVideoTracks()[0];
    if (!primaryVideoTrack) {
      return stream;
    }

    const settings = (primaryVideoTrack.getSettings?.() ??
      {}) as MediaTrackSettings;
    const facingMode = settings.facingMode;

    if (
      facingMode === 'environment' ||
      typeof navigator.mediaDevices.enumerateDevices !== 'function'
    ) {
      return stream;
    }

    try {
      const devices = await navigator.mediaDevices.enumerateDevices();
      const backCamera = devices.find(
        (device) =>
          device.kind === 'videoinput' &&
          /back|rear|environment/i.test(device.label),
      );

      if (!backCamera) {
        return stream;
      }

      const videoOnlyStream = await navigator.mediaDevices.getUserMedia({
        video: {deviceId: {exact: backCamera.deviceId}},
        audio: false,
      });
      const replacementTrack = videoOnlyStream.getVideoTracks()[0];
      if (!replacementTrack) {
        videoOnlyStream.getTracks().forEach((track) => track.stop());
        return stream;
      }

      stream.addTrack(replacementTrack);
      stream.removeTrack(primaryVideoTrack);
      primaryVideoTrack.stop();

      return stream;
    } catch (fallbackError) {
      this.debug('Back camera fallback failed', fallbackError);
      return stream;
    }
  }

  private blobToBase64(blob: Blob): Promise<string> {
    return new Promise((resolve, reject) => {
      const reader = new FileReader();
      reader.onloadend = () => {
        if (typeof reader.result === 'string') {
          resolve(reader.result.split(',')[1]);
        } else {
          reject(new Error('Failed to read blob as base64 string.'));
        }
      };
      reader.onerror = reject;
      reader.readAsDataURL(blob);
    });
  }

  private async startRecordingWithStream(stream: MediaStream) {
    if (this.isRecording) {
      stream.getTracks().forEach((t) => t.stop());
      return;
    }

    if (!this.inputAudioContext || !this.outputAudioContext || !this.inputNode || !this.outputNode) {
      throw new Error('Audio contexts not initialized');
    }

    if (!this.sessionPromise) {
      this.sessionPromise = this.initSession();
    }

    this.appStopped = false;
    this.framesSent = 0;

    this.mediaStream = stream;

    this.videoElement.srcObject = this.mediaStream;
    try {
      await this.videoElement.play();
    } catch (e) {
      console.warn('video.play() failed', e);
    }

    const vt = this.mediaStream.getVideoTracks()[0];
    const at = this.mediaStream.getAudioTracks()[0];
    this.updateStatus('Media access granted. v=' + (!!vt) + ' a=' + (!!at));

    // Ensure we have an audio track before proceeding
    if (!at) {
      throw new Error('No audio track available in media stream');
    }

    // Ensure audio track is enabled and ready
    if (!at.enabled) {
      at.enabled = true;
    }

    // Ensure audio track is ready - wait a bit if needed
    let trackReadyState = at.readyState;
    if (trackReadyState !== 'live') {
      // Wait for track to become live (max 1 second)
      let attempts = 0;
      while (trackReadyState !== 'live' && attempts < 10) {
        await new Promise(resolve => setTimeout(resolve, 100));
        trackReadyState = at.readyState;
        attempts++;
      }
      if (trackReadyState !== 'live') {
        throw new Error('Audio track is not live. State: ' + trackReadyState);
      }
    }

    // Create media stream source only after audio context is running
    if (this.inputAudioContext.state === 'running') {
      try {
        this.sourceNode = this.inputAudioContext.createMediaStreamSource(
          this.mediaStream,
        );
        this.sourceNode.connect(this.inputNode);
      } catch (e) {
        throw new Error('Failed to create media stream source: ' + (e as Error).message);
      }
    } else {
      throw new Error('Audio context is not running. State: ' + this.inputAudioContext.state);
    }

    const bufferSize = 4096;
    this.scriptProcessorNode = this.inputAudioContext.createScriptProcessor(
      bufferSize,
      1,
      1,
    );

    this.scriptProcessorNode.onaudioprocess = (audioProcessingEvent) => {
      if (!this.isRecording) return;
      if (!this.isHolding) return;

      const inputBuffer = audioProcessingEvent.inputBuffer;
      const pcmData = inputBuffer.getChannelData(0);
      const srcRate = this.inputAudioContext.sampleRate;
      const downsampleTo16k = (float32: Float32Array, inRate: number) => {
        if (inRate === 16000) return float32;
        const ratio = inRate / 16000;
        const newLen = Math.floor(float32.length / ratio);
        const result = new Float32Array(newLen);
        let idx = 0, pos = 0;
        while (idx < newLen) {
          const nextPos = Math.min(float32.length - 1, (idx + 1) * ratio);
          let sum = 0, count = 0;
          for (let i = pos; i < nextPos; i++) { sum += float32[i]; count++; }
          result[idx++] = sum / Math.max(1, count);
          pos = nextPos;
        }
        return result;
      };
      const mono16k = srcRate === 16000 ? pcmData : downsampleTo16k(pcmData, srcRate);

      const sessionPromise = this.sessionPromise;
      if (!sessionPromise) {
        return;
      }

      sessionPromise
        .then((session) => {
          session.sendRealtimeInput({media: createBlob(mono16k)});
        })
        .catch((e) => this.updateError('Session send failed: ' + e.message));
    };

    this.sourceNode.connect(this.scriptProcessorNode);
    this.scriptProcessorNode.connect(this.inputAudioContext.destination);

    this.frameInterval = window.setInterval(() => {
      if (!this.isRecording) return;
      const context = this.canvasElement.getContext('2d');
      this.canvasElement.width = this.videoElement.videoWidth;
      this.canvasElement.height = this.videoElement.videoHeight;
      context.drawImage(
        this.videoElement,
        0,
        0,
        this.canvasElement.width,
        this.canvasElement.height,
      );
      this.canvasElement.toBlob(
        async (blob) => {
          if (blob) {
            const base64Data = await this.blobToBase64(blob);
            const sessionPromise = this.sessionPromise;
            if (!sessionPromise) {
              return;
            }

            sessionPromise
              .then((session) => {
                session.sendRealtimeInput({
                  media: {data: base64Data, mimeType: 'image/jpeg'},
                });
                this.framesSent++;
                if (this.framesSent % 10 === 0) {
                  this.debug(`Frames sent: ${this.framesSent}`);
                }
              })
              .catch((e) =>
                this.updateError('Session send (image) failed: ' + e.message),
              );
          }
        },
        'image/jpeg',
        0.8,
      );
    }, 500); // 2 frames per second

    this.isRecording = true;
    this.updateStatus('');
  }

  private async startRecording() {
    if (this.isRecording) {
      return;
    }

    if (!this.inputAudioContext || !this.outputAudioContext) {
      throw new Error('Audio contexts not initialized');
    }

    const inputResumePromise =
      this.inputAudioContext.state === 'running'
        ? Promise.resolve()
        : this.inputAudioContext.resume();
    const outputResumePromise =
      this.outputAudioContext.state === 'running'
        ? Promise.resolve()
        : this.outputAudioContext.resume();

    this.updateStatus('Requesting camera/mic access...');

    try {
      let stream: MediaStream | null = null;
      try {
        stream = await this.acquireMediaStream();
        await Promise.all([inputResumePromise, outputResumePromise]);
        await this.startRecordingWithStream(stream);
      } catch (error) {
        if (stream) {
          stream.getTracks().forEach((t) => t.stop());
        }
        throw error;
      }
    } catch (err) {
      console.error('Error starting recording:', err);
      const { name, message } = this.formatErrorDescriptor(err);
      this.updateError(`Camera/mic activation failed: ${name} - ${message}`);
      this.stopRecording();
    }
  }

  private stopRecording() {
    if (!this.isRecording && !this.mediaStream && !this.inputAudioContext)
      return;

    this.updateStatus('');

    this.isRecording = false;

    if (this.frameInterval) {
      clearInterval(this.frameInterval);
      this.frameInterval = null;
    }

    if (this.scriptProcessorNode && this.sourceNode && this.inputAudioContext) {
      this.scriptProcessorNode.disconnect();
      this.sourceNode.disconnect();
    }

    this.scriptProcessorNode = null;
    this.sourceNode = null;

    if (this.mediaStream) {
      this.mediaStream.getTracks().forEach((track) => track.stop());
      this.mediaStream = null;
    }

    if (this.videoElement) {
      this.videoElement.srcObject = null;
    }

    this.framesSent = 0;
    this.updateStatus('');
  }

  private stopApplication() {
    if (this.appStopped) {
      return;
    }

    this.stopRecording();

    for (const source of this.sources.values()) {
      try {
        source.stop();
      } catch {}
      this.sources.delete(source);
    }

    const sessionPromise = this.sessionPromise;
    this.sessionPromise = undefined;

    sessionPromise
      ?.then((session) => session.close())
      .catch((e) => this.updateError('Session close failed: ' + e.message));

    this.videoElement?.pause();
    if (this.videoElement) {
      this.videoElement.srcObject = null;
    }

    this.appStopped = true;
    this.updateStatus('Session ended');
  }

  private async restartApplication() {
    if (this.isRecording) {
      return;
    }

    this.updateStatus('Restarting session...');
    this.sessionPromise = this.initSession();
    this.appStopped = false;

    try {
      await this.startRecording();
    } catch (error) {
      this.updateError('Restart failed: ' + (error as Error).message);
    }
  }

  private reset() {
    this.sessionPromise?.then((session) => session.close());
    this.sessionPromise = this.initSession();
    this.updateStatus('');
  }

  private async handleStartSession() {
    if (this.sessionStarted) {
      return;
    }
    
    this.sessionStarted = true;
    this.updateStatus('Requesting camera/mic access...');
    
    let stream: MediaStream | null = null;
    
    try {
      // Create audio contexts within user gesture for iOS PWA compatibility
      // This MUST be done synchronously from the event handler
      if (!this.inputAudioContext) {
        this.inputAudioContext = new (window.AudioContext ||
          (window as any).webkitAudioContext)({sampleRate: 16000});
      }
      if (!this.outputAudioContext) {
        this.outputAudioContext = new (window.AudioContext ||
          (window as any).webkitAudioContext)({sampleRate: 24000});
      }
      
      // Create gain nodes if they don't exist
      if (!this.inputNode) {
        this.inputNode = this.inputAudioContext.createGain();
        this.inputNode.gain.value = 0.0; // Start with mic muted
      }
      if (!this.outputNode) {
        this.outputNode = this.outputAudioContext.createGain();
        this.outputNode.connect(this.outputAudioContext.destination);
      }
      
      // Initialize session if not already done
      if (!this.sessionPromise) {
        this.sessionPromise = this.initSession();
      }
      
      // Request permissions directly from user gesture handler
      // This MUST be called synchronously from the event handler on iOS
      const constraints: MediaStreamConstraints = {
        audio: {
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
        },
        video: {
          facingMode: { ideal: 'environment' },
        },
      };
      const inputResumePromise =
        this.inputAudioContext.state === 'running'
          ? Promise.resolve()
          : this.inputAudioContext.resume();
      const outputResumePromise =
        this.outputAudioContext.state === 'running'
          ? Promise.resolve()
          : this.outputAudioContext.resume();
      
      // Call getUserMedia immediately to preserve user gesture context
      stream = await this.acquireMediaStream();
      
      await Promise.all([inputResumePromise, outputResumePromise]);
      
      // Small delay to ensure audio contexts are fully ready
      await new Promise(resolve => setTimeout(resolve, 100));
      
      // Initialize audio timing
      this.initAudio();
      
      // Now start recording with the stream
      await this.startRecordingWithStream(stream);
    } catch (err) {
      console.error('Error starting session:', err);
      const { name, message } = this.formatErrorDescriptor(err);
      this.updateError(`Failed to access camera/microphone: ${name} - ${message}`);
      this.updateStatus('');
      this.sessionStarted = false; // Allow retry on error
      
      // Clean up stream if it was created
      if (stream) {
        stream.getTracks().forEach((t) => t.stop());
      }
    }
  }

  private async handleHoldStart() {
    if (!this.isRecording) {
      if (this.appStopped) {
        await this.restartApplication();
      } else {
        try {
          await this.startRecording();
        } catch (err) {
          console.error('Error starting recording:', err);
          return;
        }
      }
    }
    
    this.isHolding = true;
    if (this.inputNode) {
      this.inputNode.gain.value = 1.0;
    }
  }

  private handleHoldEnd() {
    this.isHolding = false;
    if (this.inputNode) {
      this.inputNode.gain.value = 0.0;
    }
  }

  render() {
    return html`
      <div class="layout">
        <video
          id="video-background"
          autoplay
          muted
          playsinline
          ?hidden=${this.appStopped}></video>
        <canvas id="frame-canvas" style="display:none"></canvas>
        ${this.error
          ? html`<div class="error-banner" role="alert">${this.error}</div>`
          : null}
        ${!this.sessionStarted
          ? html`
              <button
                class="start-button"
                type="button"
                aria-label="Start Session"
                @click=${this.handleStartSession}
                @touchstart=${(e: TouchEvent) => {
                  e.preventDefault();
                  this.handleStartSession();
                }}>
                Start Session
              </button>
            `
          : null}
        <div class="speak-area">
          ${this.inputNode && this.outputNode
            ? html`<gdm-live-audio-visuals-3d
                .inputNode=${this.inputNode}
                .outputNode=${this.outputNode}></gdm-live-audio-visuals-3d>`
            : null}
          ${this.appStopped
            ? html`<div class="session-overlay">Session Ended</div>`
            : null}
          <button
            class="speak-button"
            type="button"
            aria-pressed=${this.isHolding}
            aria-label=${this.isHolding ? 'Release to mute' : 'Hold to speak'}
            @touchstart=${(e: TouchEvent) => {
              e.preventDefault();
              this.handleHoldStart();
            }}
            @touchend=${(e: TouchEvent) => {
              e.preventDefault();
              this.handleHoldEnd();
            }}
            @touchcancel=${(e: TouchEvent) => {
              e.preventDefault();
              this.handleHoldEnd();
            }}
            @mousedown=${(e: MouseEvent) => {
              e.preventDefault();
              this.handleHoldStart();
            }}
            @mouseup=${(e: MouseEvent) => {
              e.preventDefault();
              this.handleHoldEnd();
            }}
            @mouseleave=${(e: MouseEvent) => {
              e.preventDefault();
              this.handleHoldEnd();
            }}>
            <span class="speak-label">
              ${this.isHolding ? 'Release to Mute' : 'Hold to Speak'}
            </span>
          </button>
        </div>
      </div>
    `;
  }
}

```

```typescriptreact
/* tslint:disable */
/**
 * @license
 * SPDX-License-Identifier: Apache-2.0
 */

import {GoogleGenAI, LiveServerMessage, Modality, Session} from '@google/genai';
import {LitElement, css, html} from 'lit';
import {customElement, state} from 'lit/decorators.js';
import {createBlob, decode, decodeAudioData} from './utils';
import './visual-3d';
// PWA: register service worker if available
try {
  // dynamic import optional; vite-plugin-pwa injects virtual module
  // eslint-disable-next-line @typescript-eslint/ban-ts-GB
  // @ts-ignore
  import('virtual:pwa-register').then(({ registerSW }) => {
    registerSW({ immediate: true });
  }).catch(() => {});
} catch {}

@customElement('gdm-live-audio')
export class GdmLiveAudio extends LitElement {
  @state() isRecording = false;
  @state() isHolding = false;
  @state() status = '';
  @state() error = '';
  @state() appStopped = false;
  @state() sessionStarted = false;

  private client: GoogleGenAI;
  private sessionPromise?: Promise<Session>;
  // Audio contexts created on user gesture for iOS PWA compatibility
  private inputAudioContext?: AudioContext;
  private outputAudioContext?: AudioContext;
  @state() inputNode?: GainNode;
  @state() outputNode?: GainNode;
  private nextStartTime = 0;
  private mediaStream: MediaStream;
  private sourceNode: AudioNode;
  private scriptProcessorNode: ScriptProcessorNode;
  private sources = new Set<AudioBufferSourceNode>();
  private frameInterval: number;
  private videoElement: HTMLVideoElement;
  private canvasElement: HTMLCanvasElement;
  private framesSent = 0;

  static styles = css`
    :host {
      display: block;
      position: relative;
      width: 100vw;
      height: 100vh;
      min-height: 100dvh;
      background: #000000;
      color: #ffffff;
      overflow: hidden;
    }

    @supports (height: 100dvh) {
      :host {
        height: 100dvh;
      }
    }

    .layout {
      position: relative;
      width: 100%;
      height: 100%;
    }

    #video-background {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 66.667vh;
      object-fit: cover;
    }

    .speak-area {
      position: absolute;
      bottom: 0;
      left: 0;
      width: 100%;
      height: calc(33.333vh + env(safe-area-inset-bottom));
      border-top-left-radius: 32px;
      border-top-right-radius: 32px;
      border: 1px solid rgba(255, 255, 255, 0.08);
      border-bottom: none;
      overflow: hidden;
      background: rgba(0, 0, 0, 0.3);
      backdrop-filter: blur(6px);
    }

    .session-overlay {
      position: absolute;
      inset: 0;
      display: flex;
      align-items: center;
      justify-content: center;
      padding: 24px;
      font-size: 18px;
      letter-spacing: 0.1em;
      text-transform: uppercase;
      color: rgba(255, 255, 255, 0.92);
      background: linear-gradient(
        180deg,
        rgba(17, 24, 39, 0.6) 0%,
        rgba(15, 23, 42, 0.85) 100%
      );
      pointer-events: none;
      text-align: center;
    }

    gdm-live-audio-visuals-3d {
      display: block;
      width: 100%;
      height: 100%;
      pointer-events: none;
      border-top-left-radius: inherit;
      border-top-right-radius: inherit;
    }

    .speak-button {
      position: absolute;
      inset: 0;
      width: 100%;
      height: 100%;
      border: none;
      background: transparent;
      cursor: pointer;
      display: flex;
      align-items: flex-end;
      justify-content: center;
      padding: 24px 24px calc(24px + env(safe-area-inset-bottom));
      color: inherit;
      transition: background 0.2s ease;
      -webkit-touch-callout: none;
      -webkit-user-select: none;
      -moz-user-select: none;
      -ms-user-select: none;
      user-select: none;
      -webkit-tap-highlight-color: transparent;
      touch-action: manipulation;
    }

    .speak-button:hover,
    .speak-button:focus-visible {
      background: rgba(255, 255, 255, 0.04);
    }

    .speak-button:focus-visible {
      outline: 2px solid rgba(255, 255, 255, 0.25);
      outline-offset: 4px;
    }

    .speak-label {
      font-size: 18px;
      letter-spacing: 0.08em;
      text-transform: uppercase;
      color: rgba(255, 255, 255, 0.92);
      text-shadow: 0 0 6px rgba(0, 0, 0, 0.6);
      -webkit-touch-callout: none;
      -webkit-user-select: none;
      -moz-user-select: none;
      -ms-user-select: none;
      user-select: none;
      pointer-events: none;
    }

    .error-banner {
      position: absolute;
      top: calc(env(safe-area-inset-top) + 12px);
      left: 16px;
      right: 16px;
      z-index: 10;
      padding: 12px 16px;
      background: rgba(220, 38, 38, 0.85);
      color: #ffffff;
      border-radius: 12px;
      text-align: center;
      font-size: 14px;
      line-height: 1.4;
      box-shadow: 0 12px 24px rgba(0, 0, 0, 0.35);
      backdrop-filter: blur(12px);
    }

    .start-button {
      position: absolute;
      top: 50%;
      left: 50%;
      transform: translate(-50%, -50%);
      z-index: 1000;
      padding: 20px 40px;
      background: rgba(255, 255, 255, 0.25);
      color: #ffffff;
      border: 2px solid rgba(255, 255, 255, 0.5);
      border-radius: 16px;
      font-size: 18px;
      font-weight: 500;
      letter-spacing: 0.05em;
      text-transform: uppercase;
      cursor: pointer;
      transition: all 0.2s ease;
      backdrop-filter: blur(12px);
      box-shadow: 0 8px 24px rgba(0, 0, 0, 0.6), 0 0 0 1px rgba(255, 255, 255, 0.1);
      -webkit-tap-highlight-color: transparent;
      touch-action: manipulation;
      white-space: nowrap;
    }

    .start-button:hover,
    .start-button:focus-visible {
      background: rgba(255, 255, 255, 0.25);
      border-color: rgba(255, 255, 255, 0.5);
      transform: translate(-50%, -50%) scale(1.05);
    }

    .start-button:active {
      transform: translate(-50%, -50%) scale(0.98);
    }

    .start-button:focus-visible {
      outline: 2px solid rgba(255, 255, 255, 0.5);
      outline-offset: 4px;
    }
  `;

  constructor() {
    super();
    this.initClient();
  }

  protected firstUpdated() {
    this.videoElement = this.shadowRoot!.querySelector('#video-background');
    this.canvasElement = this.shadowRoot!.querySelector('#frame-canvas');
    this.debug(
      `Boot: secure=${window.isSecureContext} ua=${navigator.userAgent}`,
    );
    this.checkPermissions();
    this.addEventListener('click', async () => {
      try {
        if (this.outputAudioContext) {
          await this.outputAudioContext.resume();
        }
        if (this.inputAudioContext) {
          await this.inputAudioContext.resume();
        }
      } catch {}
    });
  }

  private async checkPermissions() {
    const parts: string[] = [];
    const anyNav: any = navigator as any;
    if (anyNav.permissions?.query) {
      try {
        const mic = await anyNav.permissions.query({name: 'microphone'});
        parts.push(`mic=${mic.state}`);
      } catch {}
      try {
        const cam = await anyNav.permissions.query({name: 'camera'});
        parts.push(`cam=${cam.state}`);
      } catch {}
    }
    if (parts.length) this.updateStatus(`Permissions: ${parts.join(' ')}`);
  }

  private debug(_msg: string, _data?: unknown) {
    // Debug logging disabled for production UI clarity.
  }

  private initAudio() {
    if (this.outputAudioContext && this.inputNode) {
      this.nextStartTime = this.outputAudioContext.currentTime;
      this.inputNode.gain.value = 0.0; // Start with mic muted
    }
  }

  private async initClient() {
    if (!process.env.API_KEY || process.env.API_KEY === 'undefined') {
      this.updateError('Missing GEMINI_API_KEY');
    }

    this.client = new GoogleGenAI({apiKey: process.env.API_KEY});

    // Audio contexts and nodes will be created on user gesture
    // Session will be initialized when user starts session
  }

  private initSession(): Promise<Session> {
    const model = 'gemini-2.5-flash-native-audio-preview-09-2025';

    try {
      return this.client.live.connect({
        model: model,
        callbacks: {
          onopen: () => {
            this.debug('Live session opened');
          },
          onmessage: async (message: LiveServerMessage) => {
            this.debug('Live message received');
            const audio =
              message.serverContent?.modelTurn?.parts[0]?.inlineData;

            if (audio && this.outputAudioContext && this.outputNode) {
              this.nextStartTime = Math.max(
                this.nextStartTime,
                this.outputAudioContext.currentTime,
              );

              const audioBuffer = await decodeAudioData(
                decode(audio.data),
                this.outputAudioContext,
                24000,
                1,
              );
              const source = this.outputAudioContext.createBufferSource();
              source.buffer = audioBuffer;
              source.connect(this.outputNode);
              source.addEventListener('ended', () => {
                this.sources.delete(source);
              });

              source.start(this.nextStartTime);
              this.nextStartTime = this.nextStartTime + audioBuffer.duration;
              this.sources.add(source);
            }

            const interrupted = message.serverContent?.interrupted;
            if (interrupted) {
              for (const source of this.sources.values()) {
                source.stop();
                this.sources.delete(source);
              }
              this.nextStartTime = 0;
            }
          },
          onerror: (e: ErrorEvent) => {
            this.updateError('Live error: ' + e.message);
            console.error('Live error event', e);
          },
          onclose: (e: CloseEvent) => {
            this.updateStatus('Live close: ' + e.reason + ' (' + e.code + ')');
            console.warn('Live close', e);
          },
        },
        config: {
          responseModalities: [Modality.AUDIO],
          systemInstruction: {
            parts: [
              {
                text: `You are an AI assistant integrated into an application where the main interface is a live camera view. The user interacts with you by pressing and holding a large button at the bottom of the screen. When the button is active, you listen to the user's voice input and provide an answer based on what is visible through the camera.

Your task is to analyze the live camera feed and interpret the user’s spoken request in combination. Always respond with precise, spatially grounded instructions that reference items or locations visible on the screen.

When giving instructions:

Be specific and concise.

Refer to locations using clear spatial language (e.g., “third shelf from the top,” “top right corner,” “behind the red box,” etc.).

Only describe what is visible on the camera or can be reasonably inferred from it.

Do not invent objects or locations that are not present in the scene.

Keep answers straightforward and helpful.

Example:
If the user asks, “Where is the gluten-free bread?” and it is visible through the camera, respond with a description such as:
“The gluten-free bread is on the third shelf from the top, in the top right corner.”

Your goal is to help the user locate objects in the environment accurately and efficiently based on real-time visual input.`,
              },
            ],
          },
          speechConfig: {
            voiceConfig: {prebuiltVoiceConfig: {voiceName: 'Zephyr'}},
            // languageCode: 'en-GB'
          },
        },
      });
    } catch (e) {
      console.error(e);
      return Promise.reject(e);
    }
  }

  private updateStatus(msg: string) {
    this.status = msg;
  }

  private updateError(msg: string) {
    this.error = msg;
  }

  private formatErrorDescriptor(error: unknown): { name: string; message: string } {
    if (error instanceof DOMException) {
      return { name: error.name, message: error.message };
    }
    if (error instanceof Error) {
      return { name: error.name || 'Error', message: error.message };
    }
    return { name: 'Error', message: String(error) };
  }

  private async acquireMediaStream(): Promise<MediaStream> {
    const constraints: MediaStreamConstraints = {
      audio: {
        channelCount: 1,
        echoCancellation: true,
        noiseSuppression: true,
        autoGainControl: true,
      },
      video: {
        facingMode: { ideal: 'environment' },
      },
    };

    const stream = await navigator.mediaDevices.getUserMedia(constraints);
    const primaryVideoTrack = stream.getVideoTracks()[0];
    if (!primaryVideoTrack) {
      return stream;
    }

    const settings = (primaryVideoTrack.getSettings?.() ??
      {}) as MediaTrackSettings;
    const facingMode = settings.facingMode;

    if (
      facingMode === 'environment' ||
      typeof navigator.mediaDevices.enumerateDevices !== 'function'
    ) {
      return stream;
    }

    try {
      const devices = await navigator.mediaDevices.enumerateDevices();
      const backCamera = devices.find(
        (device) =>
          device.kind === 'videoinput' &&
          /back|rear|environment/i.test(device.label),
      );

      if (!backCamera) {
        return stream;
      }

      const videoOnlyStream = await navigator.mediaDevices.getUserMedia({
        video: {deviceId: {exact: backCamera.deviceId}},
        audio: false,
      });
      const replacementTrack = videoOnlyStream.getVideoTracks()[0];
      if (!replacementTrack) {
        videoOnlyStream.getTracks().forEach((track) => track.stop());
        return stream;
      }

      stream.addTrack(replacementTrack);
      stream.removeTrack(primaryVideoTrack);
      primaryVideoTrack.stop();

      return stream;
    } catch (fallbackError) {
      this.debug('Back camera fallback failed', fallbackError);
      return stream;
    }
  }

  private blobToBase64(blob: Blob): Promise<string> {
    return new Promise((resolve, reject) => {
      const reader = new FileReader();
      reader.onloadend = () => {
        if (typeof reader.result === 'string') {
          resolve(reader.result.split(',')[1]);
        } else {
          reject(new Error('Failed to read blob as base64 string.'));
        }
      };
      reader.onerror = reject;
      reader.readAsDataURL(blob);
    });
  }

  private async startRecordingWithStream(stream: MediaStream) {
    if (this.isRecording) {
      stream.getTracks().forEach((t) => t.stop());
      return;
    }

    if (!this.inputAudioContext || !this.outputAudioContext || !this.inputNode || !this.outputNode) {
      throw new Error('Audio contexts not initialized');
    }

    if (!this.sessionPromise) {
      this.sessionPromise = this.initSession();
    }

    this.appStopped = false;
    this.framesSent = 0;

    this.mediaStream = stream;

    this.videoElement.srcObject = this.mediaStream;
    try {
      await this.videoElement.play();
    } catch (e) {
      console.warn('video.play() failed', e);
    }

    const vt = this.mediaStream.getVideoTracks()[0];
    const at = this.mediaStream.getAudioTracks()[0];
    this.updateStatus('Media access granted. v=' + (!!vt) + ' a=' + (!!at));

    // Ensure we have an audio track before proceeding
    if (!at) {
      throw new Error('No audio track available in media stream');
    }

    // Ensure audio track is enabled and ready
    if (!at.enabled) {
      at.enabled = true;
    }

    // Ensure audio track is ready - wait a bit if needed
    let trackReadyState = at.readyState;
    if (trackReadyState !== 'live') {
      // Wait for track to become live (max 1 second)
      let attempts = 0;
      while (trackReadyState !== 'live' && attempts < 10) {
        await new Promise(resolve => setTimeout(resolve, 100));
        trackReadyState = at.readyState;
        attempts++;
      }
      if (trackReadyState !== 'live') {
        throw new Error('Audio track is not live. State: ' + trackReadyState);
      }
    }

    // Create media stream source only after audio context is running
    if (this.inputAudioContext.state === 'running') {
      try {
        this.sourceNode = this.inputAudioContext.createMediaStreamSource(
          this.mediaStream,
        );
        this.sourceNode.connect(this.inputNode);
      } catch (e) {
        throw new Error('Failed to create media stream source: ' + (e as Error).message);
      }
    } else {
      throw new Error('Audio context is not running. State: ' + this.inputAudioContext.state);
    }

    const bufferSize = 4096;
    this.scriptProcessorNode = this.inputAudioContext.createScriptProcessor(
      bufferSize,
      1,
      1,
    );

    this.scriptProcessorNode.onaudioprocess = (audioProcessingEvent) => {
      if (!this.isRecording) return;
      if (!this.isHolding) return;

      const inputBuffer = audioProcessingEvent.inputBuffer;
      const pcmData = inputBuffer.getChannelData(0);
      const srcRate = this.inputAudioContext.sampleRate;
      const downsampleTo16k = (float32: Float32Array, inRate: number) => {
        if (inRate === 16000) return float32;
        const ratio = inRate / 16000;
        const newLen = Math.floor(float32.length / ratio);
        const result = new Float32Array(newLen);
        let idx = 0, pos = 0;
        while (idx < newLen) {
          const nextPos = Math.min(float32.length - 1, (idx + 1) * ratio);
          let sum = 0, count = 0;
          for (let i = pos; i < nextPos; i++) { sum += float32[i]; count++; }
          result[idx++] = sum / Math.max(1, count);
          pos = nextPos;
        }
        return result;
      };
      const mono16k = srcRate === 16000 ? pcmData : downsampleTo16k(pcmData, srcRate);

      const sessionPromise = this.sessionPromise;
      if (!sessionPromise) {
        return;
      }

      sessionPromise
        .then((session) => {
          session.sendRealtimeInput({media: createBlob(mono16k)});
        })
        .catch((e) => this.updateError('Session send failed: ' + e.message));
    };

    this.sourceNode.connect(this.scriptProcessorNode);
    this.scriptProcessorNode.connect(this.inputAudioContext.destination);

    this.frameInterval = window.setInterval(() => {
      if (!this.isRecording) return;
      const context = this.canvasElement.getContext('2d');
      this.canvasElement.width = this.videoElement.videoWidth;
      this.canvasElement.height = this.videoElement.videoHeight;
      context.drawImage(
        this.videoElement,
        0,
        0,
        this.canvasElement.width,
        this.canvasElement.height,
      );
      this.canvasElement.toBlob(
        async (blob) => {
          if (blob) {
            const base64Data = await this.blobToBase64(blob);
            const sessionPromise = this.sessionPromise;
            if (!sessionPromise) {
              return;
            }

            sessionPromise
              .then((session) => {
                session.sendRealtimeInput({
                  media: {data: base64Data, mimeType: 'image/jpeg'},
                });
                this.framesSent++;
                if (this.framesSent % 10 === 0) {
                  this.debug(`Frames sent: ${this.framesSent}`);
                }
              })
              .catch((e) =>
                this.updateError('Session send (image) failed: ' + e.message),
              );
          }
        },
        'image/jpeg',
        0.8,
      );
    }, 500); // 2 frames per second

    this.isRecording = true;
    this.updateStatus('');
  }

  private async startRecording() {
    if (this.isRecording) {
      return;
    }

    if (!this.inputAudioContext || !this.outputAudioContext) {
      throw new Error('Audio contexts not initialized');
    }

    const inputResumePromise =
      this.inputAudioContext.state === 'running'
        ? Promise.resolve()
        : this.inputAudioContext.resume();
    const outputResumePromise =
      this.outputAudioContext.state === 'running'
        ? Promise.resolve()
        : this.outputAudioContext.resume();

    this.updateStatus('Requesting camera/mic access...');

    try {
      let stream: MediaStream | null = null;
      try {
        stream = await this.acquireMediaStream();
        await Promise.all([inputResumePromise, outputResumePromise]);
        await this.startRecordingWithStream(stream);
      } catch (error) {
        if (stream) {
          stream.getTracks().forEach((t) => t.stop());
        }
        throw error;
      }
    } catch (err) {
      console.error('Error starting recording:', err);
      const { name, message } = this.formatErrorDescriptor(err);
      this.updateError(`Camera/mic activation failed: ${name} - ${message}`);
      this.stopRecording();
    }
  }

  private stopRecording() {
    if (!this.isRecording && !this.mediaStream && !this.inputAudioContext)
      return;

    this.updateStatus('');

    this.isRecording = false;

    if (this.frameInterval) {
      clearInterval(this.frameInterval);
      this.frameInterval = null;
    }

    if (this.scriptProcessorNode && this.sourceNode && this.inputAudioContext) {
      this.scriptProcessorNode.disconnect();
      this.sourceNode.disconnect();
    }

    this.scriptProcessorNode = null;
    this.sourceNode = null;

    if (this.mediaStream) {
      this.mediaStream.getTracks().forEach((track) => track.stop());
      this.mediaStream = null;
    }

    if (this.videoElement) {
      this.videoElement.srcObject = null;
    }

    this.framesSent = 0;
    this.updateStatus('');
  }

  private stopApplication() {
    if (this.appStopped) {
      return;
    }

    this.stopRecording();

    for (const source of this.sources.values()) {
      try {
        source.stop();
      } catch {}
      this.sources.delete(source);
    }

    const sessionPromise = this.sessionPromise;
    this.sessionPromise = undefined;

    sessionPromise
      ?.then((session) => session.close())
      .catch((e) => this.updateError('Session close failed: ' + e.message));

    this.videoElement?.pause();
    if (this.videoElement) {
      this.videoElement.srcObject = null;
    }

    this.appStopped = true;
    this.updateStatus('Session ended');
  }

  private async restartApplication() {
    if (this.isRecording) {
      return;
    }

    this.updateStatus('Restarting session...');
    this.sessionPromise = this.initSession();
    this.appStopped = false;

    try {
      await this.startRecording();
    } catch (error) {
      this.updateError('Restart failed: ' + (error as Error).message);
    }
  }

  private reset() {
    this.sessionPromise?.then((session) => session.close());
    this.sessionPromise = this.initSession();
    this.updateStatus('');
  }

  private async handleStartSession() {
    if (this.sessionStarted) {
      return;
    }
    
    this.sessionStarted = true;
    this.updateStatus('Requesting camera/mic access...');
    
    let stream: MediaStream | null = null;
    
    try {
      // Create audio contexts within user gesture for iOS PWA compatibility
      // This MUST be done synchronously from the event handler
      if (!this.inputAudioContext) {
        this.inputAudioContext = new (window.AudioContext ||
          (window as any).webkitAudioContext)({sampleRate: 16000});
      }
      if (!this.outputAudioContext) {
        this.outputAudioContext = new (window.AudioContext ||
          (window as any).webkitAudioContext)({sampleRate: 24000});
      }
      
      // Create gain nodes if they don't exist
      if (!this.inputNode) {
        this.inputNode = this.inputAudioContext.createGain();
        this.inputNode.gain.value = 0.0; // Start with mic muted
      }
      if (!this.outputNode) {
        this.outputNode = this.outputAudioContext.createGain();
        this.outputNode.connect(this.outputAudioContext.destination);
      }
      
      // Initialize session if not already done
      if (!this.sessionPromise) {
        this.sessionPromise = this.initSession();
      }
      
      // Request permissions directly from user gesture handler
      // This MUST be called synchronously from the event handler on iOS
      const inputResumePromise =
        this.inputAudioContext.state === 'running'
          ? Promise.resolve()
          : this.inputAudioContext.resume();
      const outputResumePromise =
        this.outputAudioContext.state === 'running'
          ? Promise.resolve()
          : this.outputAudioContext.resume();
      
      // Call getUserMedia immediately to preserve user gesture context
      stream = await this.acquireMediaStream();
      
      await Promise.all([inputResumePromise, outputResumePromise]);
      
      // Small delay to ensure audio contexts are fully ready
      await new Promise(resolve => setTimeout(resolve, 100));
      
      // Initialize audio timing
      this.initAudio();
      
      // Now start recording with the stream
      await this.startRecordingWithStream(stream);
    } catch (err) {
      console.error('Error starting session:', err);
      const { name, message } = this.formatErrorDescriptor(err);
      this.updateError(`Failed to access camera/microphone: ${name} - ${message}`);
      this.updateStatus('');
      this.sessionStarted = false; // Allow retry on error
      
      // Clean up stream if it was created
      if (stream) {
        stream.getTracks().forEach((t) => t.stop());
      }
    }
  }

  private async handleHoldStart() {
    if (!this.isRecording) {
      if (this.appStopped) {
        await this.restartApplication();
      } else {
        try {
          await this.startRecording();
        } catch (err) {
          console.error('Error starting recording:', err);
          return;
        }
      }
    }
    
    this.isHolding = true;
    if (this.inputNode) {
      this.inputNode.gain.value = 1.0;
    }
  }

  private handleHoldEnd() {
    this.isHolding = false;
    if (this.inputNode) {
      this.inputNode.gain.value = 0.0;
    }
  }

  render() {
    return html`
      <div class="layout">
        <video
          id="video-background"
          autoplay
          muted
          playsinline
          ?hidden=${this.appStopped}></video>
        <canvas id="frame-canvas" style="display:none"></canvas>
        ${this.error
          ? html`<div class="error-banner" role="alert">${this.error}</div>`
          : null}
        ${!this.sessionStarted
          ? html`
              <button
                class="start-button"
                type="button"
                aria-label="Start Session"
                @click=${this.handleStartSession}
                @touchstart=${(e: TouchEvent) => {
                  e.preventDefault();
                  this.handleStartSession();
                }}>
                Start Session
              </button>
            `
          : null}
        <div class="speak-area">
          ${this.inputNode && this.outputNode
            ? html`<gdm-live-audio-visuals-3d
                .inputNode=${this.inputNode}
                .outputNode=${this.outputNode}></gdm-live-audio-visuals-3d>`
            : null}
          ${this.appStopped
            ? html`<div class="session-overlay">Session Ended</div>`
            : null}
          <button
            class="speak-button"
            type="button"
            aria-pressed=${this.isHolding}
            aria-label=${this.isHolding ? 'Release to mute' : 'Hold to speak'}
            @touchstart=${(e: TouchEvent) => {
              e.preventDefault();
              this.handleHoldStart();
            }}
            @touchend=${(e: TouchEvent) => {
              e.preventDefault();
              this.handleHoldEnd();
            }}
            @touchcancel=${(e: TouchEvent) => {
              e.preventDefault();
              this.handleHoldEnd();
            }}
            @mousedown=${(e: MouseEvent) => {
              e.preventDefault();
              this.handleHoldStart();
            }}
            @mouseup=${(e: MouseEvent) => {
              e.preventDefault();
              this.handleHoldEnd();
            }}
            @mouseleave=${(e: MouseEvent) => {
              e.preventDefault();
              this.handleHoldEnd();
            }}>
            <span class="speak-label">
              ${this.isHolding ? 'Release to Mute' : 'Hold to Speak'}
            </span>
          </button>
        </div>
      </div>
    `;
  }
}

```

## Summary
- Added `acquireMediaStream()` so the mic spins up only once per gesture, then swaps in a rear-camera track without touching the already-live audio; on fallbacks we now reuse the original stream instead of reopening mic hardware, eliminating the race that produced `InvalidStateError`.  
```398:446:index.tsx
  private async acquireMediaStream(): Promise<MediaStream> {
    const constraints: MediaStreamConstraints = {
      audio: {
        channelCount: 1,
        echoCancellation: true,
        noiseSuppression: true,
        autoGainControl: true,
      },
      video: {
        facingMode: { ideal: 'environment' },
      },
    };

    const stream = await navigator.mediaDevices.getUserMedia(constraints);
    const primaryVideoTrack = stream.getVideoTracks()[0];
    // ... existing code ...
    try {
      const devices = await navigator.mediaDevices.enumerateDevices();
      const backCamera = devices.find(
        (device) =>
          device.kind === 'videoinput' &&
          /back|rear|environment/i.test(device.label),
      );
      // ... existing code ...
      const videoOnlyStream = await navigator.mediaDevices.getUserMedia({
        video: {deviceId: {exact: backCamera.deviceId}},
        audio: false,
      });
      // ... existing code ...
      stream.addTrack(replacementTrack);
      stream.removeTrack(primaryVideoTrack);
      primaryVideoTrack.stop();
```
- Reused that helper in both `startRecording()` and `handleStartSession()` so every entry point shares the single-stream logic, with cleanup on failures and detailed errors via `formatErrorDescriptor`.  
```602:676:index.tsx
    let stream: MediaStream | null = null;
    try {
      stream = await this.acquireMediaStream();
      await Promise.all([inputResumePromise, outputResumePromise]);
      await this.startRecordingWithStream(stream);
    } catch (error) {
      if (stream) {
        stream.getTracks().forEach((t) => t.stop());
      }
      throw error;
    }
```
```815:843:index.tsx
      const inputResumePromise =
        this.inputAudioContext.state === 'running'
          ? Promise.resolve()
          : this.inputAudioContext.resume();
      const outputResumePromise =
        this.outputAudioContext.state === 'running'
          ? Promise.resolve()
          : this.outputAudioContext.resume();

      stream = await this.acquireMediaStream();
      await Promise.all([inputResumePromise, outputResumePromise]);
      // ... existing code ...
```

## Testing
- Not run; please validate on-device.

## Next Steps
- Reinstall PWA and Safari tab, trigger permission prompts repeatedly (initial launch, after stop/start, after backgrounding) to confirm no further `InvalidStateError`.

```typescriptreact
  private async acquireMediaStream(): Promise<MediaStream> {
    const constraints: MediaStreamConstraints = {
      audio: {
        channelCount: 1,
        echoCancellation: true,
        noiseSuppression: true,
        autoGainControl: true,
      },
      video: {
        facingMode: { ideal: 'environment' },
      },
    };

    const stream = await navigator.mediaDevices.getUserMedia(constraints);
    const primaryVideoTrack = stream.getVideoTracks()[0];
    // ... existing code ...
    try {
      const devices = await navigator.mediaDevices.enumerateDevices();
      const backCamera = devices.find(
        (device) =>
          device.kind === 'videoinput' &&
          /back|rear|environment/i.test(device.label),
      );
      // ... existing code ...
      const videoOnlyStream = await navigator.mediaDevices.getUserMedia({
        video: {deviceId: {exact: backCamera.deviceId}},
        audio: false,
      });
      // ... existing code ...
      stream.addTrack(replacementTrack);
      stream.removeTrack(primaryVideoTrack);
      primaryVideoTrack.stop();
```

```typescriptreact
    let stream: MediaStream | null = null;
    try {
      stream = await this.acquireMediaStream();
      await Promise.all([inputResumePromise, outputResumePromise]);
      await this.startRecordingWithStream(stream);
    } catch (error) {
      if (stream) {
        stream.getTracks().forEach((t) => t.stop());
      }
      throw error;
    }
```

```typescriptreact
      const inputResumePromise =
        this.inputAudioContext.state === 'running'
          ? Promise.resolve()
          : this.inputAudioContext.resume();
      const outputResumePromise =
        this.outputAudioContext.state === 'running'
          ? Promise.resolve()
          : this.outputAudioContext.resume();

      stream = await this.acquireMediaStream();
      await Promise.all([inputResumePromise, outputResumePromise]);
      // ... existing code ...
```

---

**User**

I'm still not able to do it on the PWA, getting this error: Camera/mic activation failed: InvalidStateError - Failed to start the audio device

---

**Cursor**

```typescriptreact
/* tslint:disable */
/**
 * @license
 * SPDX-License-Identifier: Apache-2.0
 */

import {GoogleGenAI, LiveServerMessage, Modality, Session} from '@google/genai';
import {LitElement, css, html} from 'lit';
import {customElement, state} from 'lit/decorators.js';
import {createBlob, decode, decodeAudioData} from './utils';
import './visual-3d';
// PWA: register service worker if available
try {
  // dynamic import optional; vite-plugin-pwa injects virtual module
  // eslint-disable-next-line @typescript-eslint/ban-ts-GB
  // @ts-ignore
  import('virtual:pwa-register').then(({ registerSW }) => {
    registerSW({ immediate: true });
  }).catch(() => {});
} catch {}

@customElement('gdm-live-audio')
export class GdmLiveAudio extends LitElement {
  @state() isRecording = false;
  @state() isHolding = false;
  @state() status = '';
  @state() error = '';
  @state() appStopped = false;
  @state() sessionStarted = false;

  private client: GoogleGenAI;
  private sessionPromise?: Promise<Session>;
  // Audio contexts created on user gesture for iOS PWA compatibility
  private inputAudioContext?: AudioContext;
  private outputAudioContext?: AudioContext;
  @state() inputNode?: GainNode;
  @state() outputNode?: GainNode;
  private nextStartTime = 0;
  private mediaStream: MediaStream;
  private sourceNode: AudioNode;
  private scriptProcessorNode: ScriptProcessorNode;
  private sources = new Set<AudioBufferSourceNode>();
  private frameInterval: number;
  private videoElement: HTMLVideoElement;
  private canvasElement: HTMLCanvasElement;
  private framesSent = 0;

  static styles = css`
    :host {
      display: block;
      position: relative;
      width: 100vw;
      height: 100vh;
      min-height: 100dvh;
      background: #000000;
      color: #ffffff;
      overflow: hidden;
    }

    @supports (height: 100dvh) {
      :host {
        height: 100dvh;
      }
    }

    .layout {
      position: relative;
      width: 100%;
      height: 100%;
    }

    #video-background {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 66.667vh;
      object-fit: cover;
    }

    .speak-area {
      position: absolute;
      bottom: 0;
      left: 0;
      width: 100%;
      height: calc(33.333vh + env(safe-area-inset-bottom));
      border-top-left-radius: 32px;
      border-top-right-radius: 32px;
      border: 1px solid rgba(255, 255, 255, 0.08);
      border-bottom: none;
      overflow: hidden;
      background: rgba(0, 0, 0, 0.3);
      backdrop-filter: blur(6px);
    }

    .session-overlay {
      position: absolute;
      inset: 0;
      display: flex;
      align-items: center;
      justify-content: center;
      padding: 24px;
      font-size: 18px;
      letter-spacing: 0.1em;
      text-transform: uppercase;
      color: rgba(255, 255, 255, 0.92);
      background: linear-gradient(
        180deg,
        rgba(17, 24, 39, 0.6) 0%,
        rgba(15, 23, 42, 0.85) 100%
      );
      pointer-events: none;
      text-align: center;
    }

    gdm-live-audio-visuals-3d {
      display: block;
      width: 100%;
      height: 100%;
      pointer-events: none;
      border-top-left-radius: inherit;
      border-top-right-radius: inherit;
    }

    .speak-button {
      position: absolute;
      inset: 0;
      width: 100%;
      height: 100%;
      border: none;
      background: transparent;
      cursor: pointer;
      display: flex;
      align-items: flex-end;
      justify-content: center;
      padding: 24px 24px calc(24px + env(safe-area-inset-bottom));
      color: inherit;
      transition: background 0.2s ease;
      -webkit-touch-callout: none;
      -webkit-user-select: none;
      -moz-user-select: none;
      -ms-user-select: none;
      user-select: none;
      -webkit-tap-highlight-color: transparent;
      touch-action: manipulation;
    }

    .speak-button:hover,
    .speak-button:focus-visible {
      background: rgba(255, 255, 255, 0.04);
    }

    .speak-button:focus-visible {
      outline: 2px solid rgba(255, 255, 255, 0.25);
      outline-offset: 4px;
    }

    .speak-label {
      font-size: 18px;
      letter-spacing: 0.08em;
      text-transform: uppercase;
      color: rgba(255, 255, 255, 0.92);
      text-shadow: 0 0 6px rgba(0, 0, 0, 0.6);
      -webkit-touch-callout: none;
      -webkit-user-select: none;
      -moz-user-select: none;
      -ms-user-select: none;
      user-select: none;
      pointer-events: none;
    }

    .error-banner {
      position: absolute;
      top: calc(env(safe-area-inset-top) + 12px);
      left: 16px;
      right: 16px;
      z-index: 10;
      padding: 12px 16px;
      background: rgba(220, 38, 38, 0.85);
      color: #ffffff;
      border-radius: 12px;
      text-align: center;
      font-size: 14px;
      line-height: 1.4;
      box-shadow: 0 12px 24px rgba(0, 0, 0, 0.35);
      backdrop-filter: blur(12px);
    }

    .start-button {
      position: absolute;
      top: 50%;
      left: 50%;
      transform: translate(-50%, -50%);
      z-index: 1000;
      padding: 20px 40px;
      background: rgba(255, 255, 255, 0.25);
      color: #ffffff;
      border: 2px solid rgba(255, 255, 255, 0.5);
      border-radius: 16px;
      font-size: 18px;
      font-weight: 500;
      letter-spacing: 0.05em;
      text-transform: uppercase;
      cursor: pointer;
      transition: all 0.2s ease;
      backdrop-filter: blur(12px);
      box-shadow: 0 8px 24px rgba(0, 0, 0, 0.6), 0 0 0 1px rgba(255, 255, 255, 0.1);
      -webkit-tap-highlight-color: transparent;
      touch-action: manipulation;
      white-space: nowrap;
    }

    .start-button:hover,
    .start-button:focus-visible {
      background: rgba(255, 255, 255, 0.25);
      border-color: rgba(255, 255, 255, 0.5);
      transform: translate(-50%, -50%) scale(1.05);
    }

    .start-button:active {
      transform: translate(-50%, -50%) scale(0.98);
    }

    .start-button:focus-visible {
      outline: 2px solid rgba(255, 255, 255, 0.5);
      outline-offset: 4px;
    }
  `;

  constructor() {
    super();
    this.initClient();
  }

  protected firstUpdated() {
    this.videoElement = this.shadowRoot!.querySelector('#video-background');
    this.canvasElement = this.shadowRoot!.querySelector('#frame-canvas');
    this.debug(
      `Boot: secure=${window.isSecureContext} ua=${navigator.userAgent}`,
    );
    this.checkPermissions();
    this.addEventListener('click', async () => {
      try {
        if (this.outputAudioContext) {
          await this.outputAudioContext.resume();
        }
        if (this.inputAudioContext) {
          await this.inputAudioContext.resume();
        }
      } catch {}
    });
  }

  private async checkPermissions() {
    const parts: string[] = [];
    const anyNav: any = navigator as any;
    if (anyNav.permissions?.query) {
      try {
        const mic = await anyNav.permissions.query({name: 'microphone'});
        parts.push(`mic=${mic.state}`);
      } catch {}
      try {
        const cam = await anyNav.permissions.query({name: 'camera'});
        parts.push(`cam=${cam.state}`);
      } catch {}
    }
    if (parts.length) this.updateStatus(`Permissions: ${parts.join(' ')}`);
  }

  private debug(_msg: string, _data?: unknown) {
    // Debug logging disabled for production UI clarity.
  }

  private initAudio() {
    if (this.outputAudioContext && this.inputNode) {
      this.nextStartTime = this.outputAudioContext.currentTime;
      this.inputNode.gain.value = 0.0; // Start with mic muted
    }
  }

  private async initClient() {
    if (!process.env.API_KEY || process.env.API_KEY === 'undefined') {
      this.updateError('Missing GEMINI_API_KEY');
    }

    this.client = new GoogleGenAI({apiKey: process.env.API_KEY});

    // Audio contexts and nodes will be created on user gesture
    // Session will be initialized when user starts session
  }

  private initSession(): Promise<Session> {
    const model = 'gemini-2.5-flash-native-audio-preview-09-2025';

    try {
      return this.client.live.connect({
        model: model,
        callbacks: {
          onopen: () => {
            this.debug('Live session opened');
          },
          onmessage: async (message: LiveServerMessage) => {
            this.debug('Live message received');
            const audio =
              message.serverContent?.modelTurn?.parts[0]?.inlineData;

            if (audio && this.outputAudioContext && this.outputNode) {
              this.nextStartTime = Math.max(
                this.nextStartTime,
                this.outputAudioContext.currentTime,
              );

              const audioBuffer = await decodeAudioData(
                decode(audio.data),
                this.outputAudioContext,
                24000,
                1,
              );
              const source = this.outputAudioContext.createBufferSource();
              source.buffer = audioBuffer;
              source.connect(this.outputNode);
              source.addEventListener('ended', () => {
                this.sources.delete(source);
              });

              source.start(this.nextStartTime);
              this.nextStartTime = this.nextStartTime + audioBuffer.duration;
              this.sources.add(source);
            }

            const interrupted = message.serverContent?.interrupted;
            if (interrupted) {
              for (const source of this.sources.values()) {
                source.stop();
                this.sources.delete(source);
              }
              this.nextStartTime = 0;
            }
          },
          onerror: (e: ErrorEvent) => {
            this.updateError('Live error: ' + e.message);
            console.error('Live error event', e);
          },
          onclose: (e: CloseEvent) => {
            this.updateStatus('Live close: ' + e.reason + ' (' + e.code + ')');
            console.warn('Live close', e);
          },
        },
        config: {
          responseModalities: [Modality.AUDIO],
          systemInstruction: {
            parts: [
              {
                text: `You are an AI assistant integrated into an application where the main interface is a live camera view. The user interacts with you by pressing and holding a large button at the bottom of the screen. When the button is active, you listen to the user's voice input and provide an answer based on what is visible through the camera.

Your task is to analyze the live camera feed and interpret the user’s spoken request in combination. Always respond with precise, spatially grounded instructions that reference items or locations visible on the screen.

When giving instructions:

Be specific and concise.

Refer to locations using clear spatial language (e.g., “third shelf from the top,” “top right corner,” “behind the red box,” etc.).

Only describe what is visible on the camera or can be reasonably inferred from it.

Do not invent objects or locations that are not present in the scene.

Keep answers straightforward and helpful.

Example:
If the user asks, “Where is the gluten-free bread?” and it is visible through the camera, respond with a description such as:
“The gluten-free bread is on the third shelf from the top, in the top right corner.”

Your goal is to help the user locate objects in the environment accurately and efficiently based on real-time visual input.`,
              },
            ],
          },
          speechConfig: {
            voiceConfig: {prebuiltVoiceConfig: {voiceName: 'Zephyr'}},
            // languageCode: 'en-GB'
          },
        },
      });
    } catch (e) {
      console.error(e);
      return Promise.reject(e);
    }
  }

  private updateStatus(msg: string) {
    this.status = msg;
  }

  private updateError(msg: string) {
    this.error = msg;
  }

  private formatErrorDescriptor(error: unknown): { name: string; message: string } {
    if (error instanceof DOMException) {
      return { name: error.name, message: error.message };
    }
    if (error instanceof Error) {
      return { name: error.name || 'Error', message: error.message };
    }
    return { name: 'Error', message: String(error) };
  }

  private async tryResumeAudioContext(
    context: AudioContext,
  ): Promise<boolean> {
    if (!context || context.state === 'running') {
      return true;
    }
    try {
      await context.resume();
      return true;
    } catch (error) {
      const {name} = this.formatErrorDescriptor(error);
      if (name === 'InvalidStateError') {
        this.debug('AudioContext resume blocked (InvalidStateError)', error);
        return false;
      }
      throw error;
    }
  }

  private async acquireMediaStream(): Promise<MediaStream> {
    const constraints: MediaStreamConstraints = {
      audio: {
        channelCount: 1,
        echoCancellation: true,
        noiseSuppression: true,
        autoGainControl: true,
      },
      video: {
        facingMode: { ideal: 'environment' },
      },
    };

    const stream = await navigator.mediaDevices.getUserMedia(constraints);
    const primaryVideoTrack = stream.getVideoTracks()[0];
    if (!primaryVideoTrack) {
      return stream;
    }

    const settings = (primaryVideoTrack.getSettings?.() ??
      {}) as MediaTrackSettings;
    const facingMode = settings.facingMode;

    if (
      facingMode === 'environment' ||
      typeof navigator.mediaDevices.enumerateDevices !== 'function'
    ) {
      return stream;
    }

    try {
      const devices = await navigator.mediaDevices.enumerateDevices();
      const backCamera = devices.find(
        (device) =>
          device.kind === 'videoinput' &&
          /back|rear|environment/i.test(device.label),
      );

      if (!backCamera) {
        return stream;
      }

      const videoOnlyStream = await navigator.mediaDevices.getUserMedia({
        video: {deviceId: {exact: backCamera.deviceId}},
        audio: false,
      });
      const replacementTrack = videoOnlyStream.getVideoTracks()[0];
      if (!replacementTrack) {
        videoOnlyStream.getTracks().forEach((track) => track.stop());
        return stream;
      }

      stream.addTrack(replacementTrack);
      stream.removeTrack(primaryVideoTrack);
      primaryVideoTrack.stop();

      return stream;
    } catch (fallbackError) {
      this.debug('Back camera fallback failed', fallbackError);
      return stream;
    }
  }

  private blobToBase64(blob: Blob): Promise<string> {
    return new Promise((resolve, reject) => {
      const reader = new FileReader();
      reader.onloadend = () => {
        if (typeof reader.result === 'string') {
          resolve(reader.result.split(',')[1]);
        } else {
          reject(new Error('Failed to read blob as base64 string.'));
        }
      };
      reader.onerror = reject;
      reader.readAsDataURL(blob);
    });
  }

  private async startRecordingWithStream(stream: MediaStream) {
    if (this.isRecording) {
      stream.getTracks().forEach((t) => t.stop());
      return;
    }

    if (!this.inputAudioContext || !this.outputAudioContext || !this.inputNode || !this.outputNode) {
      throw new Error('Audio contexts not initialized');
    }

    if (!this.sessionPromise) {
      this.sessionPromise = this.initSession();
    }

    this.appStopped = false;
    this.framesSent = 0;

    this.mediaStream = stream;

    this.videoElement.srcObject = this.mediaStream;
    try {
      await this.videoElement.play();
    } catch (e) {
      console.warn('video.play() failed', e);
    }

    const vt = this.mediaStream.getVideoTracks()[0];
    const at = this.mediaStream.getAudioTracks()[0];
    this.updateStatus('Media access granted. v=' + (!!vt) + ' a=' + (!!at));

    // Ensure we have an audio track before proceeding
    if (!at) {
      throw new Error('No audio track available in media stream');
    }

    // Ensure audio track is enabled and ready
    if (!at.enabled) {
      at.enabled = true;
    }

    // Ensure audio track is ready - wait a bit if needed
    let trackReadyState = at.readyState;
    if (trackReadyState !== 'live') {
      // Wait for track to become live (max 1 second)
      let attempts = 0;
      while (trackReadyState !== 'live' && attempts < 10) {
        await new Promise(resolve => setTimeout(resolve, 100));
        trackReadyState = at.readyState;
        attempts++;
      }
      if (trackReadyState !== 'live') {
        throw new Error('Audio track is not live. State: ' + trackReadyState);
      }
    }

    // Create media stream source only after audio context is running
    if (this.inputAudioContext.state === 'running') {
      try {
        this.sourceNode = this.inputAudioContext.createMediaStreamSource(
          this.mediaStream,
        );
        this.sourceNode.connect(this.inputNode);
      } catch (e) {
        throw new Error('Failed to create media stream source: ' + (e as Error).message);
      }
    } else {
      throw new Error('Audio context is not running. State: ' + this.inputAudioContext.state);
    }

    const bufferSize = 4096;
    this.scriptProcessorNode = this.inputAudioContext.createScriptProcessor(
      bufferSize,
      1,
      1,
    );

    this.scriptProcessorNode.onaudioprocess = (audioProcessingEvent) => {
      if (!this.isRecording) return;
      if (!this.isHolding) return;

      const inputBuffer = audioProcessingEvent.inputBuffer;
      const pcmData = inputBuffer.getChannelData(0);
      const srcRate = this.inputAudioContext.sampleRate;
      const downsampleTo16k = (float32: Float32Array, inRate: number) => {
        if (inRate === 16000) return float32;
        const ratio = inRate / 16000;
        const newLen = Math.floor(float32.length / ratio);
        const result = new Float32Array(newLen);
        let idx = 0, pos = 0;
        while (idx < newLen) {
          const nextPos = Math.min(float32.length - 1, (idx + 1) * ratio);
          let sum = 0, count = 0;
          for (let i = pos; i < nextPos; i++) { sum += float32[i]; count++; }
          result[idx++] = sum / Math.max(1, count);
          pos = nextPos;
        }
        return result;
      };
      const mono16k = srcRate === 16000 ? pcmData : downsampleTo16k(pcmData, srcRate);

      const sessionPromise = this.sessionPromise;
      if (!sessionPromise) {
        return;
      }

      sessionPromise
        .then((session) => {
          session.sendRealtimeInput({media: createBlob(mono16k)});
        })
        .catch((e) => this.updateError('Session send failed: ' + e.message));
    };

    this.sourceNode.connect(this.scriptProcessorNode);
    this.scriptProcessorNode.connect(this.inputAudioContext.destination);

    this.frameInterval = window.setInterval(() => {
      if (!this.isRecording) return;
      const context = this.canvasElement.getContext('2d');
      this.canvasElement.width = this.videoElement.videoWidth;
      this.canvasElement.height = this.videoElement.videoHeight;
      context.drawImage(
        this.videoElement,
        0,
        0,
        this.canvasElement.width,
        this.canvasElement.height,
      );
      this.canvasElement.toBlob(
        async (blob) => {
          if (blob) {
            const base64Data = await this.blobToBase64(blob);
            const sessionPromise = this.sessionPromise;
            if (!sessionPromise) {
              return;
            }

            sessionPromise
              .then((session) => {
                session.sendRealtimeInput({
                  media: {data: base64Data, mimeType: 'image/jpeg'},
                });
                this.framesSent++;
                if (this.framesSent % 10 === 0) {
                  this.debug(`Frames sent: ${this.framesSent}`);
                }
              })
              .catch((e) =>
                this.updateError('Session send (image) failed: ' + e.message),
              );
          }
        },
        'image/jpeg',
        0.8,
      );
    }, 500); // 2 frames per second

    this.isRecording = true;
    this.updateStatus('');
  }

  private async startRecording() {
    if (this.isRecording) {
      return;
    }

    if (!this.inputAudioContext || !this.outputAudioContext) {
      throw new Error('Audio contexts not initialized');
    }

    const inputResumePromise =
      this.inputAudioContext.state === 'running'
        ? Promise.resolve()
        : this.inputAudioContext.resume();
    const outputResumePromise =
      this.outputAudioContext.state === 'running'
        ? Promise.resolve()
        : this.outputAudioContext.resume();

    this.updateStatus('Requesting camera/mic access...');

    try {
      let stream: MediaStream | null = null;
      try {
        stream = await this.acquireMediaStream();
        await Promise.all([inputResumePromise, outputResumePromise]);
        await this.startRecordingWithStream(stream);
      } catch (error) {
        if (stream) {
          stream.getTracks().forEach((t) => t.stop());
        }
        throw error;
      }
    } catch (err) {
      console.error('Error starting recording:', err);
      const { name, message } = this.formatErrorDescriptor(err);
      this.updateError(`Camera/mic activation failed: ${name} - ${message}`);
      this.stopRecording();
    }
  }

  private stopRecording() {
    if (!this.isRecording && !this.mediaStream && !this.inputAudioContext)
      return;

    this.updateStatus('');

    this.isRecording = false;

    if (this.frameInterval) {
      clearInterval(this.frameInterval);
      this.frameInterval = null;
    }

    if (this.scriptProcessorNode && this.sourceNode && this.inputAudioContext) {
      this.scriptProcessorNode.disconnect();
      this.sourceNode.disconnect();
    }

    this.scriptProcessorNode = null;
    this.sourceNode = null;

    if (this.mediaStream) {
      this.mediaStream.getTracks().forEach((track) => track.stop());
      this.mediaStream = null;
    }

    if (this.videoElement) {
      this.videoElement.srcObject = null;
    }

    this.framesSent = 0;
    this.updateStatus('');
  }

  private stopApplication() {
    if (this.appStopped) {
      return;
    }

    this.stopRecording();

    for (const source of this.sources.values()) {
      try {
        source.stop();
      } catch {}
      this.sources.delete(source);
    }

    const sessionPromise = this.sessionPromise;
    this.sessionPromise = undefined;

    sessionPromise
      ?.then((session) => session.close())
      .catch((e) => this.updateError('Session close failed: ' + e.message));

    this.videoElement?.pause();
    if (this.videoElement) {
      this.videoElement.srcObject = null;
    }

    this.appStopped = true;
    this.updateStatus('Session ended');
  }

  private async restartApplication() {
    if (this.isRecording) {
      return;
    }

    this.updateStatus('Restarting session...');
    this.sessionPromise = this.initSession();
    this.appStopped = false;

    try {
      await this.startRecording();
    } catch (error) {
      this.updateError('Restart failed: ' + (error as Error).message);
    }
  }

  private reset() {
    this.sessionPromise?.then((session) => session.close());
    this.sessionPromise = this.initSession();
    this.updateStatus('');
  }

  private async handleStartSession() {
    if (this.sessionStarted) {
      return;
    }
    
    this.sessionStarted = true;
    this.updateStatus('Requesting camera/mic access...');
    
    let stream: MediaStream | null = null;
    
    try {
      // Create audio contexts within user gesture for iOS PWA compatibility
      // This MUST be done synchronously from the event handler
      if (!this.inputAudioContext) {
        this.inputAudioContext = new (window.AudioContext ||
          (window as any).webkitAudioContext)({sampleRate: 16000});
      }
      if (!this.outputAudioContext) {
        this.outputAudioContext = new (window.AudioContext ||
          (window as any).webkitAudioContext)({sampleRate: 24000});
      }
      
      // Create gain nodes if they don't exist
      if (!this.inputNode) {
        this.inputNode = this.inputAudioContext.createGain();
        this.inputNode.gain.value = 0.0; // Start with mic muted
      }
      if (!this.outputNode) {
        this.outputNode = this.outputAudioContext.createGain();
        this.outputNode.connect(this.outputAudioContext.destination);
      }
      
      // Initialize session if not already done
      if (!this.sessionPromise) {
        this.sessionPromise = this.initSession();
      }
      
      // Request permissions directly from user gesture handler
      // This MUST be called synchronously from the event handler on iOS
      const inputResumePromise =
        this.inputAudioContext.state === 'running'
          ? Promise.resolve()
          : this.inputAudioContext.resume();
      const outputResumePromise =
        this.outputAudioContext.state === 'running'
          ? Promise.resolve()
          : this.outputAudioContext.resume();
      
      // Call getUserMedia immediately to preserve user gesture context
      stream = await this.acquireMediaStream();
      
      await Promise.all([inputResumePromise, outputResumePromise]);
      
      // Small delay to ensure audio contexts are fully ready
      await new Promise(resolve => setTimeout(resolve, 100));
      
      // Initialize audio timing
      this.initAudio();
      
      // Now start recording with the stream
      await this.startRecordingWithStream(stream);
    } catch (err) {
      console.error('Error starting session:', err);
      const { name, message } = this.formatErrorDescriptor(err);
      this.updateError(`Failed to access camera/microphone: ${name} - ${message}`);
      this.updateStatus('');
      this.sessionStarted = false; // Allow retry on error
      
      // Clean up stream if it was created
      if (stream) {
        stream.getTracks().forEach((t) => t.stop());
      }
    }
  }

  private async handleHoldStart() {
    if (!this.isRecording) {
      if (this.appStopped) {
        await this.restartApplication();
      } else {
        try {
          await this.startRecording();
        } catch (err) {
          console.error('Error starting recording:', err);
          return;
        }
      }
    }
    
    this.isHolding = true;
    if (this.inputNode) {
      this.inputNode.gain.value = 1.0;
    }
  }

  private handleHoldEnd() {
    this.isHolding = false;
    if (this.inputNode) {
      this.inputNode.gain.value = 0.0;
    }
  }

  render() {
    return html`
      <div class="layout">
        <video
          id="video-background"
          autoplay
          muted
          playsinline
          ?hidden=${this.appStopped}></video>
        <canvas id="frame-canvas" style="display:none"></canvas>
        ${this.error
          ? html`<div class="error-banner" role="alert">${this.error}</div>`
          : null}
        ${!this.sessionStarted
          ? html`
              <button
                class="start-button"
                type="button"
                aria-label="Start Session"
                @click=${this.handleStartSession}
                @touchstart=${(e: TouchEvent) => {
                  e.preventDefault();
                  this.handleStartSession();
                }}>
                Start Session
              </button>
            `
          : null}
        <div class="speak-area">
          ${this.inputNode && this.outputNode
            ? html`<gdm-live-audio-visuals-3d
                .inputNode=${this.inputNode}
                .outputNode=${this.outputNode}></gdm-live-audio-visuals-3d>`
            : null}
          ${this.appStopped
            ? html`<div class="session-overlay">Session Ended</div>`
            : null}
          <button
            class="speak-button"
            type="button"
            aria-pressed=${this.isHolding}
            aria-label=${this.isHolding ? 'Release to mute' : 'Hold to speak'}
            @touchstart=${(e: TouchEvent) => {
              e.preventDefault();
              this.handleHoldStart();
            }}
            @touchend=${(e: TouchEvent) => {
              e.preventDefault();
              this.handleHoldEnd();
            }}
            @touchcancel=${(e: TouchEvent) => {
              e.preventDefault();
              this.handleHoldEnd();
            }}
            @mousedown=${(e: MouseEvent) => {
              e.preventDefault();
              this.handleHoldStart();
            }}
            @mouseup=${(e: MouseEvent) => {
              e.preventDefault();
              this.handleHoldEnd();
            }}
            @mouseleave=${(e: MouseEvent) => {
              e.preventDefault();
              this.handleHoldEnd();
            }}>
            <span class="speak-label">
              ${this.isHolding ? 'Release to Mute' : 'Hold to Speak'}
            </span>
          </button>
        </div>
      </div>
    `;
  }
}

```

```typescriptreact
/* tslint:disable */
/**
 * @license
 * SPDX-License-Identifier: Apache-2.0
 */

import {GoogleGenAI, LiveServerMessage, Modality, Session} from '@google/genai';
import {LitElement, css, html} from 'lit';
import {customElement, state} from 'lit/decorators.js';
import {createBlob, decode, decodeAudioData} from './utils';
import './visual-3d';
// PWA: register service worker if available
try {
  // dynamic import optional; vite-plugin-pwa injects virtual module
  // eslint-disable-next-line @typescript-eslint/ban-ts-GB
  // @ts-ignore
  import('virtual:pwa-register').then(({ registerSW }) => {
    registerSW({ immediate: true });
  }).catch(() => {});
} catch {}

@customElement('gdm-live-audio')
export class GdmLiveAudio extends LitElement {
  @state() isRecording = false;
  @state() isHolding = false;
  @state() status = '';
  @state() error = '';
  @state() appStopped = false;
  @state() sessionStarted = false;

  private client: GoogleGenAI;
  private sessionPromise?: Promise<Session>;
  // Audio contexts created on user gesture for iOS PWA compatibility
  private inputAudioContext?: AudioContext;
  private outputAudioContext?: AudioContext;
  @state() inputNode?: GainNode;
  @state() outputNode?: GainNode;
  private nextStartTime = 0;
  private mediaStream: MediaStream;
  private sourceNode: AudioNode;
  private scriptProcessorNode: ScriptProcessorNode;
  private sources = new Set<AudioBufferSourceNode>();
  private frameInterval: number;
  private videoElement: HTMLVideoElement;
  private canvasElement: HTMLCanvasElement;
  private framesSent = 0;

  static styles = css`
    :host {
      display: block;
      position: relative;
      width: 100vw;
      height: 100vh;
      min-height: 100dvh;
      background: #000000;
      color: #ffffff;
      overflow: hidden;
    }

    @supports (height: 100dvh) {
      :host {
        height: 100dvh;
      }
    }

    .layout {
      position: relative;
      width: 100%;
      height: 100%;
    }

    #video-background {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 66.667vh;
      object-fit: cover;
    }

    .speak-area {
      position: absolute;
      bottom: 0;
      left: 0;
      width: 100%;
      height: calc(33.333vh + env(safe-area-inset-bottom));
      border-top-left-radius: 32px;
      border-top-right-radius: 32px;
      border: 1px solid rgba(255, 255, 255, 0.08);
      border-bottom: none;
      overflow: hidden;
      background: rgba(0, 0, 0, 0.3);
      backdrop-filter: blur(6px);
    }

    .session-overlay {
      position: absolute;
      inset: 0;
      display: flex;
      align-items: center;
      justify-content: center;
      padding: 24px;
      font-size: 18px;
      letter-spacing: 0.1em;
      text-transform: uppercase;
      color: rgba(255, 255, 255, 0.92);
      background: linear-gradient(
        180deg,
        rgba(17, 24, 39, 0.6) 0%,
        rgba(15, 23, 42, 0.85) 100%
      );
      pointer-events: none;
      text-align: center;
    }

    gdm-live-audio-visuals-3d {
      display: block;
      width: 100%;
      height: 100%;
      pointer-events: none;
      border-top-left-radius: inherit;
      border-top-right-radius: inherit;
    }

    .speak-button {
      position: absolute;
      inset: 0;
      width: 100%;
      height: 100%;
      border: none;
      background: transparent;
      cursor: pointer;
      display: flex;
      align-items: flex-end;
      justify-content: center;
      padding: 24px 24px calc(24px + env(safe-area-inset-bottom));
      color: inherit;
      transition: background 0.2s ease;
      -webkit-touch-callout: none;
      -webkit-user-select: none;
      -moz-user-select: none;
      -ms-user-select: none;
      user-select: none;
      -webkit-tap-highlight-color: transparent;
      touch-action: manipulation;
    }

    .speak-button:hover,
    .speak-button:focus-visible {
      background: rgba(255, 255, 255, 0.04);
    }

    .speak-button:focus-visible {
      outline: 2px solid rgba(255, 255, 255, 0.25);
      outline-offset: 4px;
    }

    .speak-label {
      font-size: 18px;
      letter-spacing: 0.08em;
      text-transform: uppercase;
      color: rgba(255, 255, 255, 0.92);
      text-shadow: 0 0 6px rgba(0, 0, 0, 0.6);
      -webkit-touch-callout: none;
      -webkit-user-select: none;
      -moz-user-select: none;
      -ms-user-select: none;
      user-select: none;
      pointer-events: none;
    }

    .error-banner {
      position: absolute;
      top: calc(env(safe-area-inset-top) + 12px);
      left: 16px;
      right: 16px;
      z-index: 10;
      padding: 12px 16px;
      background: rgba(220, 38, 38, 0.85);
      color: #ffffff;
      border-radius: 12px;
      text-align: center;
      font-size: 14px;
      line-height: 1.4;
      box-shadow: 0 12px 24px rgba(0, 0, 0, 0.35);
      backdrop-filter: blur(12px);
    }

    .start-button {
      position: absolute;
      top: 50%;
      left: 50%;
      transform: translate(-50%, -50%);
      z-index: 1000;
      padding: 20px 40px;
      background: rgba(255, 255, 255, 0.25);
      color: #ffffff;
      border: 2px solid rgba(255, 255, 255, 0.5);
      border-radius: 16px;
      font-size: 18px;
      font-weight: 500;
      letter-spacing: 0.05em;
      text-transform: uppercase;
      cursor: pointer;
      transition: all 0.2s ease;
      backdrop-filter: blur(12px);
      box-shadow: 0 8px 24px rgba(0, 0, 0, 0.6), 0 0 0 1px rgba(255, 255, 255, 0.1);
      -webkit-tap-highlight-color: transparent;
      touch-action: manipulation;
      white-space: nowrap;
    }

    .start-button:hover,
    .start-button:focus-visible {
      background: rgba(255, 255, 255, 0.25);
      border-color: rgba(255, 255, 255, 0.5);
      transform: translate(-50%, -50%) scale(1.05);
    }

    .start-button:active {
      transform: translate(-50%, -50%) scale(0.98);
    }

    .start-button:focus-visible {
      outline: 2px solid rgba(255, 255, 255, 0.5);
      outline-offset: 4px;
    }
  `;

  constructor() {
    super();
    this.initClient();
  }

  protected firstUpdated() {
    this.videoElement = this.shadowRoot!.querySelector('#video-background');
    this.canvasElement = this.shadowRoot!.querySelector('#frame-canvas');
    this.debug(
      `Boot: secure=${window.isSecureContext} ua=${navigator.userAgent}`,
    );
    this.checkPermissions();
    this.addEventListener('click', async () => {
      try {
        if (this.outputAudioContext) {
          await this.outputAudioContext.resume();
        }
        if (this.inputAudioContext) {
          await this.inputAudioContext.resume();
        }
      } catch {}
    });
  }

  private async checkPermissions() {
    const parts: string[] = [];
    const anyNav: any = navigator as any;
    if (anyNav.permissions?.query) {
      try {
        const mic = await anyNav.permissions.query({name: 'microphone'});
        parts.push(`mic=${mic.state}`);
      } catch {}
      try {
        const cam = await anyNav.permissions.query({name: 'camera'});
        parts.push(`cam=${cam.state}`);
      } catch {}
    }
    if (parts.length) this.updateStatus(`Permissions: ${parts.join(' ')}`);
  }

  private debug(_msg: string, _data?: unknown) {
    // Debug logging disabled for production UI clarity.
  }

  private initAudio() {
    if (this.outputAudioContext && this.inputNode) {
      this.nextStartTime = this.outputAudioContext.currentTime;
      this.inputNode.gain.value = 0.0; // Start with mic muted
    }
  }

  private async initClient() {
    if (!process.env.API_KEY || process.env.API_KEY === 'undefined') {
      this.updateError('Missing GEMINI_API_KEY');
    }

    this.client = new GoogleGenAI({apiKey: process.env.API_KEY});

    // Audio contexts and nodes will be created on user gesture
    // Session will be initialized when user starts session
  }

  private initSession(): Promise<Session> {
    const model = 'gemini-2.5-flash-native-audio-preview-09-2025';

    try {
      return this.client.live.connect({
        model: model,
        callbacks: {
          onopen: () => {
            this.debug('Live session opened');
          },
          onmessage: async (message: LiveServerMessage) => {
            this.debug('Live message received');
            const audio =
              message.serverContent?.modelTurn?.parts[0]?.inlineData;

            if (audio && this.outputAudioContext && this.outputNode) {
              this.nextStartTime = Math.max(
                this.nextStartTime,
                this.outputAudioContext.currentTime,
              );

              const audioBuffer = await decodeAudioData(
                decode(audio.data),
                this.outputAudioContext,
                24000,
                1,
              );
              const source = this.outputAudioContext.createBufferSource();
              source.buffer = audioBuffer;
              source.connect(this.outputNode);
              source.addEventListener('ended', () => {
                this.sources.delete(source);
              });

              source.start(this.nextStartTime);
              this.nextStartTime = this.nextStartTime + audioBuffer.duration;
              this.sources.add(source);
            }

            const interrupted = message.serverContent?.interrupted;
            if (interrupted) {
              for (const source of this.sources.values()) {
                source.stop();
                this.sources.delete(source);
              }
              this.nextStartTime = 0;
            }
          },
          onerror: (e: ErrorEvent) => {
            this.updateError('Live error: ' + e.message);
            console.error('Live error event', e);
          },
          onclose: (e: CloseEvent) => {
            this.updateStatus('Live close: ' + e.reason + ' (' + e.code + ')');
            console.warn('Live close', e);
          },
        },
        config: {
          responseModalities: [Modality.AUDIO],
          systemInstruction: {
            parts: [
              {
                text: `You are an AI assistant integrated into an application where the main interface is a live camera view. The user interacts with you by pressing and holding a large button at the bottom of the screen. When the button is active, you listen to the user's voice input and provide an answer based on what is visible through the camera.

Your task is to analyze the live camera feed and interpret the user’s spoken request in combination. Always respond with precise, spatially grounded instructions that reference items or locations visible on the screen.

When giving instructions:

Be specific and concise.

Refer to locations using clear spatial language (e.g., “third shelf from the top,” “top right corner,” “behind the red box,” etc.).

Only describe what is visible on the camera or can be reasonably inferred from it.

Do not invent objects or locations that are not present in the scene.

Keep answers straightforward and helpful.

Example:
If the user asks, “Where is the gluten-free bread?” and it is visible through the camera, respond with a description such as:
“The gluten-free bread is on the third shelf from the top, in the top right corner.”

Your goal is to help the user locate objects in the environment accurately and efficiently based on real-time visual input.`,
              },
            ],
          },
          speechConfig: {
            voiceConfig: {prebuiltVoiceConfig: {voiceName: 'Zephyr'}},
            // languageCode: 'en-GB'
          },
        },
      });
    } catch (e) {
      console.error(e);
      return Promise.reject(e);
    }
  }

  private updateStatus(msg: string) {
    this.status = msg;
  }

  private updateError(msg: string) {
    this.error = msg;
  }

  private formatErrorDescriptor(error: unknown): { name: string; message: string } {
    if (error instanceof DOMException) {
      return { name: error.name, message: error.message };
    }
    if (error instanceof Error) {
      return { name: error.name || 'Error', message: error.message };
    }
    return { name: 'Error', message: String(error) };
  }

  private async tryResumeAudioContext(
    context: AudioContext,
  ): Promise<boolean> {
    if (!context || context.state === 'running') {
      return true;
    }
    try {
      await context.resume();
      return true;
    } catch (error) {
      const {name} = this.formatErrorDescriptor(error);
      if (name === 'InvalidStateError') {
        this.debug('AudioContext resume blocked (InvalidStateError)', error);
        return false;
      }
      throw error;
    }
  }

  private async acquireMediaStream(): Promise<MediaStream> {
    const constraints: MediaStreamConstraints = {
      audio: {
        channelCount: 1,
        echoCancellation: true,
        noiseSuppression: true,
        autoGainControl: true,
      },
      video: {
        facingMode: { ideal: 'environment' },
      },
    };

    const stream = await navigator.mediaDevices.getUserMedia(constraints);
    const primaryVideoTrack = stream.getVideoTracks()[0];
    if (!primaryVideoTrack) {
      return stream;
    }

    const settings = (primaryVideoTrack.getSettings?.() ??
      {}) as MediaTrackSettings;
    const facingMode = settings.facingMode;

    if (
      facingMode === 'environment' ||
      typeof navigator.mediaDevices.enumerateDevices !== 'function'
    ) {
      return stream;
    }

    try {
      const devices = await navigator.mediaDevices.enumerateDevices();
      const backCamera = devices.find(
        (device) =>
          device.kind === 'videoinput' &&
          /back|rear|environment/i.test(device.label),
      );

      if (!backCamera) {
        return stream;
      }

      const videoOnlyStream = await navigator.mediaDevices.getUserMedia({
        video: {deviceId: {exact: backCamera.deviceId}},
        audio: false,
      });
      const replacementTrack = videoOnlyStream.getVideoTracks()[0];
      if (!replacementTrack) {
        videoOnlyStream.getTracks().forEach((track) => track.stop());
        return stream;
      }

      stream.addTrack(replacementTrack);
      stream.removeTrack(primaryVideoTrack);
      primaryVideoTrack.stop();

      return stream;
    } catch (fallbackError) {
      this.debug('Back camera fallback failed', fallbackError);
      return stream;
    }
  }

  private blobToBase64(blob: Blob): Promise<string> {
    return new Promise((resolve, reject) => {
      const reader = new FileReader();
      reader.onloadend = () => {
        if (typeof reader.result === 'string') {
          resolve(reader.result.split(',')[1]);
        } else {
          reject(new Error('Failed to read blob as base64 string.'));
        }
      };
      reader.onerror = reject;
      reader.readAsDataURL(blob);
    });
  }

  private async startRecordingWithStream(stream: MediaStream) {
    if (this.isRecording) {
      stream.getTracks().forEach((t) => t.stop());
      return;
    }

    if (!this.inputAudioContext || !this.outputAudioContext || !this.inputNode || !this.outputNode) {
      throw new Error('Audio contexts not initialized');
    }

    if (!this.sessionPromise) {
      this.sessionPromise = this.initSession();
    }

    this.appStopped = false;
    this.framesSent = 0;

    this.mediaStream = stream;

    this.videoElement.srcObject = this.mediaStream;
    try {
      await this.videoElement.play();
    } catch (e) {
      console.warn('video.play() failed', e);
    }

    const vt = this.mediaStream.getVideoTracks()[0];
    const at = this.mediaStream.getAudioTracks()[0];
    this.updateStatus('Media access granted. v=' + (!!vt) + ' a=' + (!!at));

    // Ensure we have an audio track before proceeding
    if (!at) {
      throw new Error('No audio track available in media stream');
    }

    // Ensure audio track is enabled and ready
    if (!at.enabled) {
      at.enabled = true;
    }

    // Ensure audio track is ready - wait a bit if needed
    let trackReadyState = at.readyState;
    if (trackReadyState !== 'live') {
      // Wait for track to become live (max 1 second)
      let attempts = 0;
      while (trackReadyState !== 'live' && attempts < 10) {
        await new Promise(resolve => setTimeout(resolve, 100));
        trackReadyState = at.readyState;
        attempts++;
      }
      if (trackReadyState !== 'live') {
        throw new Error('Audio track is not live. State: ' + trackReadyState);
      }
    }

    // Create media stream source only after audio context is running
    if (this.inputAudioContext.state === 'running') {
      try {
        this.sourceNode = this.inputAudioContext.createMediaStreamSource(
          this.mediaStream,
        );
        this.sourceNode.connect(this.inputNode);
      } catch (e) {
        throw new Error('Failed to create media stream source: ' + (e as Error).message);
      }
    } else {
      throw new Error('Audio context is not running. State: ' + this.inputAudioContext.state);
    }

    const bufferSize = 4096;
    this.scriptProcessorNode = this.inputAudioContext.createScriptProcessor(
      bufferSize,
      1,
      1,
    );

    this.scriptProcessorNode.onaudioprocess = (audioProcessingEvent) => {
      if (!this.isRecording) return;
      if (!this.isHolding) return;

      const inputBuffer = audioProcessingEvent.inputBuffer;
      const pcmData = inputBuffer.getChannelData(0);
      const srcRate = this.inputAudioContext.sampleRate;
      const downsampleTo16k = (float32: Float32Array, inRate: number) => {
        if (inRate === 16000) return float32;
        const ratio = inRate / 16000;
        const newLen = Math.floor(float32.length / ratio);
        const result = new Float32Array(newLen);
        let idx = 0, pos = 0;
        while (idx < newLen) {
          const nextPos = Math.min(float32.length - 1, (idx + 1) * ratio);
          let sum = 0, count = 0;
          for (let i = pos; i < nextPos; i++) { sum += float32[i]; count++; }
          result[idx++] = sum / Math.max(1, count);
          pos = nextPos;
        }
        return result;
      };
      const mono16k = srcRate === 16000 ? pcmData : downsampleTo16k(pcmData, srcRate);

      const sessionPromise = this.sessionPromise;
      if (!sessionPromise) {
        return;
      }

      sessionPromise
        .then((session) => {
          session.sendRealtimeInput({media: createBlob(mono16k)});
        })
        .catch((e) => this.updateError('Session send failed: ' + e.message));
    };

    this.sourceNode.connect(this.scriptProcessorNode);
    this.scriptProcessorNode.connect(this.inputAudioContext.destination);

    this.frameInterval = window.setInterval(() => {
      if (!this.isRecording) return;
      const context = this.canvasElement.getContext('2d');
      this.canvasElement.width = this.videoElement.videoWidth;
      this.canvasElement.height = this.videoElement.videoHeight;
      context.drawImage(
        this.videoElement,
        0,
        0,
        this.canvasElement.width,
        this.canvasElement.height,
      );
      this.canvasElement.toBlob(
        async (blob) => {
          if (blob) {
            const base64Data = await this.blobToBase64(blob);
            const sessionPromise = this.sessionPromise;
            if (!sessionPromise) {
              return;
            }

            sessionPromise
              .then((session) => {
                session.sendRealtimeInput({
                  media: {data: base64Data, mimeType: 'image/jpeg'},
                });
                this.framesSent++;
                if (this.framesSent % 10 === 0) {
                  this.debug(`Frames sent: ${this.framesSent}`);
                }
              })
              .catch((e) =>
                this.updateError('Session send (image) failed: ' + e.message),
              );
          }
        },
        'image/jpeg',
        0.8,
      );
    }, 500); // 2 frames per second

    this.isRecording = true;
    this.updateStatus('');
  }

  private async startRecording() {
    if (this.isRecording) {
      return;
    }

    if (!this.inputAudioContext || !this.outputAudioContext) {
      throw new Error('Audio contexts not initialized');
    }

    const inputResumePromise = this.tryResumeAudioContext(
      this.inputAudioContext,
    );
    const outputResumePromise = this.tryResumeAudioContext(
      this.outputAudioContext,
    );

    this.updateStatus('Requesting camera/mic access...');

    try {
      let stream: MediaStream | null = null;
      try {
        stream = await this.acquireMediaStream();
        const [inputRunning, outputRunning] = await Promise.all([
          inputResumePromise,
          outputResumePromise,
        ]);

        if (!inputRunning || !outputRunning) {
          this.updateStatus(
            'Audio is pending activation — touch the screen to continue.',
          );
        }
        await this.startRecordingWithStream(stream);
      } catch (error) {
        if (stream) {
          stream.getTracks().forEach((t) => t.stop());
        }
        throw error;
      }
    } catch (err) {
      console.error('Error starting recording:', err);
      const { name, message } = this.formatErrorDescriptor(err);
      this.updateError(`Camera/mic activation failed: ${name} - ${message}`);
      this.stopRecording();
    }
  }

  private stopRecording() {
    if (!this.isRecording && !this.mediaStream && !this.inputAudioContext)
      return;

    this.updateStatus('');

    this.isRecording = false;

    if (this.frameInterval) {
      clearInterval(this.frameInterval);
      this.frameInterval = null;
    }

    if (this.scriptProcessorNode && this.sourceNode && this.inputAudioContext) {
      this.scriptProcessorNode.disconnect();
      this.sourceNode.disconnect();
    }

    this.scriptProcessorNode = null;
    this.sourceNode = null;

    if (this.mediaStream) {
      this.mediaStream.getTracks().forEach((track) => track.stop());
      this.mediaStream = null;
    }

    if (this.videoElement) {
      this.videoElement.srcObject = null;
    }

    this.framesSent = 0;
    this.updateStatus('');
  }

  private stopApplication() {
    if (this.appStopped) {
      return;
    }

    this.stopRecording();

    for (const source of this.sources.values()) {
      try {
        source.stop();
      } catch {}
      this.sources.delete(source);
    }

    const sessionPromise = this.sessionPromise;
    this.sessionPromise = undefined;

    sessionPromise
      ?.then((session) => session.close())
      .catch((e) => this.updateError('Session close failed: ' + e.message));

    this.videoElement?.pause();
    if (this.videoElement) {
      this.videoElement.srcObject = null;
    }

    this.appStopped = true;
    this.updateStatus('Session ended');
  }

  private async restartApplication() {
    if (this.isRecording) {
      return;
    }

    this.updateStatus('Restarting session...');
    this.sessionPromise = this.initSession();
    this.appStopped = false;

    try {
      await this.startRecording();
    } catch (error) {
      this.updateError('Restart failed: ' + (error as Error).message);
    }
  }

  private reset() {
    this.sessionPromise?.then((session) => session.close());
    this.sessionPromise = this.initSession();
    this.updateStatus('');
  }

  private async handleStartSession() {
    if (this.sessionStarted) {
      return;
    }
    
    this.sessionStarted = true;
    this.updateStatus('Requesting camera/mic access...');
    
    let stream: MediaStream | null = null;
    
    try {
      // Create audio contexts within user gesture for iOS PWA compatibility
      // This MUST be done synchronously from the event handler
      if (!this.inputAudioContext) {
        this.inputAudioContext = new (window.AudioContext ||
          (window as any).webkitAudioContext)({sampleRate: 16000});
      }
      if (!this.outputAudioContext) {
        this.outputAudioContext = new (window.AudioContext ||
          (window as any).webkitAudioContext)({sampleRate: 24000});
      }
      
      // Create gain nodes if they don't exist
      if (!this.inputNode) {
        this.inputNode = this.inputAudioContext.createGain();
        this.inputNode.gain.value = 0.0; // Start with mic muted
      }
      if (!this.outputNode) {
        this.outputNode = this.outputAudioContext.createGain();
        this.outputNode.connect(this.outputAudioContext.destination);
      }
      
      // Initialize session if not already done
      if (!this.sessionPromise) {
        this.sessionPromise = this.initSession();
      }
      
      // Request permissions directly from user gesture handler
      // This MUST be called synchronously from the event handler on iOS
      const inputResumePromise = this.tryResumeAudioContext(
        this.inputAudioContext,
      );
      const outputResumePromise = this.tryResumeAudioContext(
        this.outputAudioContext,
      );
      
      // Call getUserMedia immediately to preserve user gesture context
      stream = await this.acquireMediaStream();
      
      const [inputRunning, outputRunning] = await Promise.all([
        inputResumePromise,
        outputResumePromise,
      ]);

      if (!inputRunning || !outputRunning) {
        this.updateStatus(
          'Audio is pending activation — touch the screen to continue.',
        );
      }
      
      // Small delay to ensure audio contexts are fully ready
      await new Promise(resolve => setTimeout(resolve, 100));
      
      // Initialize audio timing
      this.initAudio();
      
      // Now start recording with the stream
      await this.startRecordingWithStream(stream);
    } catch (err) {
      console.error('Error starting session:', err);
      const { name, message } = this.formatErrorDescriptor(err);
      this.updateError(`Failed to access camera/microphone: ${name} - ${message}`);
      this.updateStatus('');
      this.sessionStarted = false; // Allow retry on error
      
      // Clean up stream if it was created
      if (stream) {
        stream.getTracks().forEach((t) => t.stop());
      }
    }
  }

  private async handleHoldStart() {
    if (!this.isRecording) {
      if (this.appStopped) {
        await this.restartApplication();
      } else {
        try {
          await this.startRecording();
        } catch (err) {
          console.error('Error starting recording:', err);
          return;
        }
      }
    }
    
    this.isHolding = true;
    if (this.inputNode) {
      this.inputNode.gain.value = 1.0;
    }
  }

  private handleHoldEnd() {
    this.isHolding = false;
    if (this.inputNode) {
      this.inputNode.gain.value = 0.0;
    }
  }

  render() {
    return html`
      <div class="layout">
        <video
          id="video-background"
          autoplay
          muted
          playsinline
          ?hidden=${this.appStopped}></video>
        <canvas id="frame-canvas" style="display:none"></canvas>
        ${this.error
          ? html`<div class="error-banner" role="alert">${this.error}</div>`
          : null}
        ${!this.sessionStarted
          ? html`
              <button
                class="start-button"
                type="button"
                aria-label="Start Session"
                @click=${this.handleStartSession}
                @touchstart=${(e: TouchEvent) => {
                  e.preventDefault();
                  this.handleStartSession();
                }}>
                Start Session
              </button>
            `
          : null}
        <div class="speak-area">
          ${this.inputNode && this.outputNode
            ? html`<gdm-live-audio-visuals-3d
                .inputNode=${this.inputNode}
                .outputNode=${this.outputNode}></gdm-live-audio-visuals-3d>`
            : null}
          ${this.appStopped
            ? html`<div class="session-overlay">Session Ended</div>`
            : null}
          <button
            class="speak-button"
            type="button"
            aria-pressed=${this.isHolding}
            aria-label=${this.isHolding ? 'Release to mute' : 'Hold to speak'}
            @touchstart=${(e: TouchEvent) => {
              e.preventDefault();
              this.handleHoldStart();
            }}
            @touchend=${(e: TouchEvent) => {
              e.preventDefault();
              this.handleHoldEnd();
            }}
            @touchcancel=${(e: TouchEvent) => {
              e.preventDefault();
              this.handleHoldEnd();
            }}
            @mousedown=${(e: MouseEvent) => {
              e.preventDefault();
              this.handleHoldStart();
            }}
            @mouseup=${(e: MouseEvent) => {
              e.preventDefault();
              this.handleHoldEnd();
            }}
            @mouseleave=${(e: MouseEvent) => {
              e.preventDefault();
              this.handleHoldEnd();
            }}>
            <span class="speak-label">
              ${this.isHolding ? 'Release to Mute' : 'Hold to Speak'}
            </span>
          </button>
        </div>
      </div>
    `;
  }
}

```

```typescriptreact
/* tslint:disable */
/**
 * @license
 * SPDX-License-Identifier: Apache-2.0
 */

import {GoogleGenAI, LiveServerMessage, Modality, Session} from '@google/genai';
import {LitElement, css, html} from 'lit';
import {customElement, state} from 'lit/decorators.js';
import {createBlob, decode, decodeAudioData} from './utils';
import './visual-3d';
// PWA: register service worker if available
try {
  // dynamic import optional; vite-plugin-pwa injects virtual module
  // eslint-disable-next-line @typescript-eslint/ban-ts-GB
  // @ts-ignore
  import('virtual:pwa-register').then(({ registerSW }) => {
    registerSW({ immediate: true });
  }).catch(() => {});
} catch {}

@customElement('gdm-live-audio')
export class GdmLiveAudio extends LitElement {
  @state() isRecording = false;
  @state() isHolding = false;
  @state() status = '';
  @state() error = '';
  @state() appStopped = false;
  @state() sessionStarted = false;

  private client: GoogleGenAI;
  private sessionPromise?: Promise<Session>;
  // Audio contexts created on user gesture for iOS PWA compatibility
  private inputAudioContext?: AudioContext;
  private outputAudioContext?: AudioContext;
  @state() inputNode?: GainNode;
  @state() outputNode?: GainNode;
  private nextStartTime = 0;
  private mediaStream: MediaStream;
  private sourceNode: AudioNode;
  private scriptProcessorNode: ScriptProcessorNode;
  private sources = new Set<AudioBufferSourceNode>();
  private frameInterval: number;
  private videoElement: HTMLVideoElement;
  private canvasElement: HTMLCanvasElement;
  private framesSent = 0;

  static styles = css`
    :host {
      display: block;
      position: relative;
      width: 100vw;
      height: 100vh;
      min-height: 100dvh;
      background: #000000;
      color: #ffffff;
      overflow: hidden;
    }

    @supports (height: 100dvh) {
      :host {
        height: 100dvh;
      }
    }

    .layout {
      position: relative;
      width: 100%;
      height: 100%;
    }

    #video-background {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 66.667vh;
      object-fit: cover;
    }

    .speak-area {
      position: absolute;
      bottom: 0;
      left: 0;
      width: 100%;
      height: calc(33.333vh + env(safe-area-inset-bottom));
      border-top-left-radius: 32px;
      border-top-right-radius: 32px;
      border: 1px solid rgba(255, 255, 255, 0.08);
      border-bottom: none;
      overflow: hidden;
      background: rgba(0, 0, 0, 0.3);
      backdrop-filter: blur(6px);
    }

    .session-overlay {
      position: absolute;
      inset: 0;
      display: flex;
      align-items: center;
      justify-content: center;
      padding: 24px;
      font-size: 18px;
      letter-spacing: 0.1em;
      text-transform: uppercase;
      color: rgba(255, 255, 255, 0.92);
      background: linear-gradient(
        180deg,
        rgba(17, 24, 39, 0.6) 0%,
        rgba(15, 23, 42, 0.85) 100%
      );
      pointer-events: none;
      text-align: center;
    }

    gdm-live-audio-visuals-3d {
      display: block;
      width: 100%;
      height: 100%;
      pointer-events: none;
      border-top-left-radius: inherit;
      border-top-right-radius: inherit;
    }

    .speak-button {
      position: absolute;
      inset: 0;
      width: 100%;
      height: 100%;
      border: none;
      background: transparent;
      cursor: pointer;
      display: flex;
      align-items: flex-end;
      justify-content: center;
      padding: 24px 24px calc(24px + env(safe-area-inset-bottom));
      color: inherit;
      transition: background 0.2s ease;
      -webkit-touch-callout: none;
      -webkit-user-select: none;
      -moz-user-select: none;
      -ms-user-select: none;
      user-select: none;
      -webkit-tap-highlight-color: transparent;
      touch-action: manipulation;
    }

    .speak-button:hover,
    .speak-button:focus-visible {
      background: rgba(255, 255, 255, 0.04);
    }

    .speak-button:focus-visible {
      outline: 2px solid rgba(255, 255, 255, 0.25);
      outline-offset: 4px;
    }

    .speak-label {
      font-size: 18px;
      letter-spacing: 0.08em;
      text-transform: uppercase;
      color: rgba(255, 255, 255, 0.92);
      text-shadow: 0 0 6px rgba(0, 0, 0, 0.6);
      -webkit-touch-callout: none;
      -webkit-user-select: none;
      -moz-user-select: none;
      -ms-user-select: none;
      user-select: none;
      pointer-events: none;
    }

    .error-banner {
      position: absolute;
      top: calc(env(safe-area-inset-top) + 12px);
      left: 16px;
      right: 16px;
      z-index: 10;
      padding: 12px 16px;
      background: rgba(220, 38, 38, 0.85);
      color: #ffffff;
      border-radius: 12px;
      text-align: center;
      font-size: 14px;
      line-height: 1.4;
      box-shadow: 0 12px 24px rgba(0, 0, 0, 0.35);
      backdrop-filter: blur(12px);
    }

    .start-button {
      position: absolute;
      top: 50%;
      left: 50%;
      transform: translate(-50%, -50%);
      z-index: 1000;
      padding: 20px 40px;
      background: rgba(255, 255, 255, 0.25);
      color: #ffffff;
      border: 2px solid rgba(255, 255, 255, 0.5);
      border-radius: 16px;
      font-size: 18px;
      font-weight: 500;
      letter-spacing: 0.05em;
      text-transform: uppercase;
      cursor: pointer;
      transition: all 0.2s ease;
      backdrop-filter: blur(12px);
      box-shadow: 0 8px 24px rgba(0, 0, 0, 0.6), 0 0 0 1px rgba(255, 255, 255, 0.1);
      -webkit-tap-highlight-color: transparent;
      touch-action: manipulation;
      white-space: nowrap;
    }

    .start-button:hover,
    .start-button:focus-visible {
      background: rgba(255, 255, 255, 0.25);
      border-color: rgba(255, 255, 255, 0.5);
      transform: translate(-50%, -50%) scale(1.05);
    }

    .start-button:active {
      transform: translate(-50%, -50%) scale(0.98);
    }

    .start-button:focus-visible {
      outline: 2px solid rgba(255, 255, 255, 0.5);
      outline-offset: 4px;
    }
  `;

  constructor() {
    super();
    this.initClient();
  }

  protected firstUpdated() {
    this.videoElement = this.shadowRoot!.querySelector('#video-background');
    this.canvasElement = this.shadowRoot!.querySelector('#frame-canvas');
    this.debug(
      `Boot: secure=${window.isSecureContext} ua=${navigator.userAgent}`,
    );
    this.checkPermissions();
    this.addEventListener('click', async () => {
      try {
        if (this.outputAudioContext) {
          await this.outputAudioContext.resume();
        }
        if (this.inputAudioContext) {
          await this.inputAudioContext.resume();
        }
      } catch {}
    });
  }

  private async checkPermissions() {
    const parts: string[] = [];
    const anyNav: any = navigator as any;
    if (anyNav.permissions?.query) {
      try {
        const mic = await anyNav.permissions.query({name: 'microphone'});
        parts.push(`mic=${mic.state}`);
      } catch {}
      try {
        const cam = await anyNav.permissions.query({name: 'camera'});
        parts.push(`cam=${cam.state}`);
      } catch {}
    }
    if (parts.length) this.updateStatus(`Permissions: ${parts.join(' ')}`);
  }

  private debug(_msg: string, _data?: unknown) {
    // Debug logging disabled for production UI clarity.
  }

  private initAudio() {
    if (this.outputAudioContext && this.inputNode) {
      this.nextStartTime = this.outputAudioContext.currentTime;
      this.inputNode.gain.value = 0.0; // Start with mic muted
    }
  }

  private async initClient() {
    if (!process.env.API_KEY || process.env.API_KEY === 'undefined') {
      this.updateError('Missing GEMINI_API_KEY');
    }

    this.client = new GoogleGenAI({apiKey: process.env.API_KEY});

    // Audio contexts and nodes will be created on user gesture
    // Session will be initialized when user starts session
  }

  private initSession(): Promise<Session> {
    const model = 'gemini-2.5-flash-native-audio-preview-09-2025';

    try {
      return this.client.live.connect({
        model: model,
        callbacks: {
          onopen: () => {
            this.debug('Live session opened');
          },
          onmessage: async (message: LiveServerMessage) => {
            this.debug('Live message received');
            const audio =
              message.serverContent?.modelTurn?.parts[0]?.inlineData;

            if (audio && this.outputAudioContext && this.outputNode) {
              this.nextStartTime = Math.max(
                this.nextStartTime,
                this.outputAudioContext.currentTime,
              );

              const audioBuffer = await decodeAudioData(
                decode(audio.data),
                this.outputAudioContext,
                24000,
                1,
              );
              const source = this.outputAudioContext.createBufferSource();
              source.buffer = audioBuffer;
              source.connect(this.outputNode);
              source.addEventListener('ended', () => {
                this.sources.delete(source);
              });

              source.start(this.nextStartTime);
              this.nextStartTime = this.nextStartTime + audioBuffer.duration;
              this.sources.add(source);
            }

            const interrupted = message.serverContent?.interrupted;
            if (interrupted) {
              for (const source of this.sources.values()) {
                source.stop();
                this.sources.delete(source);
              }
              this.nextStartTime = 0;
            }
          },
          onerror: (e: ErrorEvent) => {
            this.updateError('Live error: ' + e.message);
            console.error('Live error event', e);
          },
          onclose: (e: CloseEvent) => {
            this.updateStatus('Live close: ' + e.reason + ' (' + e.code + ')');
            console.warn('Live close', e);
          },
        },
        config: {
          responseModalities: [Modality.AUDIO],
          systemInstruction: {
            parts: [
              {
                text: `You are an AI assistant integrated into an application where the main interface is a live camera view. The user interacts with you by pressing and holding a large button at the bottom of the screen. When the button is active, you listen to the user's voice input and provide an answer based on what is visible through the camera.

Your task is to analyze the live camera feed and interpret the user’s spoken request in combination. Always respond with precise, spatially grounded instructions that reference items or locations visible on the screen.

When giving instructions:

Be specific and concise.

Refer to locations using clear spatial language (e.g., “third shelf from the top,” “top right corner,” “behind the red box,” etc.).

Only describe what is visible on the camera or can be reasonably inferred from it.

Do not invent objects or locations that are not present in the scene.

Keep answers straightforward and helpful.

Example:
If the user asks, “Where is the gluten-free bread?” and it is visible through the camera, respond with a description such as:
“The gluten-free bread is on the third shelf from the top, in the top right corner.”

Your goal is to help the user locate objects in the environment accurately and efficiently based on real-time visual input.`,
              },
            ],
          },
          speechConfig: {
            voiceConfig: {prebuiltVoiceConfig: {voiceName: 'Zephyr'}},
            // languageCode: 'en-GB'
          },
        },
      });
    } catch (e) {
      console.error(e);
      return Promise.reject(e);
    }
  }

  private updateStatus(msg: string) {
    this.status = msg;
  }

  private updateError(msg: string) {
    this.error = msg;
  }

  private formatErrorDescriptor(error: unknown): { name: string; message: string } {
    if (error instanceof DOMException) {
      return { name: error.name, message: error.message };
    }
    if (error instanceof Error) {
      return { name: error.name || 'Error', message: error.message };
    }
    return { name: 'Error', message: String(error) };
  }

  private async tryResumeAudioContext(
    context: AudioContext,
  ): Promise<boolean> {
    if (!context || context.state === 'running') {
      return true;
    }
    try {
      await context.resume();
      return true;
    } catch (error) {
      const {name} = this.formatErrorDescriptor(error);
      if (name === 'InvalidStateError') {
        this.debug('AudioContext resume blocked (InvalidStateError)', error);
        return false;
      }
      throw error;
    }
  }

  private async acquireMediaStream(): Promise<MediaStream> {
    const constraints: MediaStreamConstraints = {
      audio: {
        channelCount: 1,
        echoCancellation: true,
        noiseSuppression: true,
        autoGainControl: true,
      },
      video: {
        facingMode: { ideal: 'environment' },
      },
    };

    const stream = await navigator.mediaDevices.getUserMedia(constraints);
    const primaryVideoTrack = stream.getVideoTracks()[0];
    if (!primaryVideoTrack) {
      return stream;
    }

    const settings = (primaryVideoTrack.getSettings?.() ??
      {}) as MediaTrackSettings;
    const facingMode = settings.facingMode;

    if (
      facingMode === 'environment' ||
      typeof navigator.mediaDevices.enumerateDevices !== 'function'
    ) {
      return stream;
    }

    try {
      const devices = await navigator.mediaDevices.enumerateDevices();
      const backCamera = devices.find(
        (device) =>
          device.kind === 'videoinput' &&
          /back|rear|environment/i.test(device.label),
      );

      if (!backCamera) {
        return stream;
      }

      const videoOnlyStream = await navigator.mediaDevices.getUserMedia({
        video: {deviceId: {exact: backCamera.deviceId}},
        audio: false,
      });
      const replacementTrack = videoOnlyStream.getVideoTracks()[0];
      if (!replacementTrack) {
        videoOnlyStream.getTracks().forEach((track) => track.stop());
        return stream;
      }

      stream.addTrack(replacementTrack);
      stream.removeTrack(primaryVideoTrack);
      primaryVideoTrack.stop();

      return stream;
    } catch (fallbackError) {
      this.debug('Back camera fallback failed', fallbackError);
      return stream;
    }
  }

  private blobToBase64(blob: Blob): Promise<string> {
    return new Promise((resolve, reject) => {
      const reader = new FileReader();
      reader.onloadend = () => {
        if (typeof reader.result === 'string') {
          resolve(reader.result.split(',')[1]);
        } else {
          reject(new Error('Failed to read blob as base64 string.'));
        }
      };
      reader.onerror = reject;
      reader.readAsDataURL(blob);
    });
  }

  private async startRecordingWithStream(stream: MediaStream) {
    if (this.isRecording) {
      stream.getTracks().forEach((t) => t.stop());
      return;
    }

    if (!this.inputAudioContext || !this.outputAudioContext || !this.inputNode || !this.outputNode) {
      throw new Error('Audio contexts not initialized');
    }

    if (!this.sessionPromise) {
      this.sessionPromise = this.initSession();
    }

    this.appStopped = false;
    this.framesSent = 0;

    this.mediaStream = stream;

    this.videoElement.srcObject = this.mediaStream;
    try {
      await this.videoElement.play();
    } catch (e) {
      console.warn('video.play() failed', e);
    }

    const vt = this.mediaStream.getVideoTracks()[0];
    const at = this.mediaStream.getAudioTracks()[0];
    this.updateStatus('Media access granted. v=' + (!!vt) + ' a=' + (!!at));

    // Ensure we have an audio track before proceeding
    if (!at) {
      throw new Error('No audio track available in media stream');
    }

    // Ensure audio track is enabled and ready
    if (!at.enabled) {
      at.enabled = true;
    }

    // Ensure audio track is ready - wait a bit if needed
    let trackReadyState = at.readyState;
    if (trackReadyState !== 'live') {
      // Wait for track to become live (max 1 second)
      let attempts = 0;
      while (trackReadyState !== 'live' && attempts < 10) {
        await new Promise(resolve => setTimeout(resolve, 100));
        trackReadyState = at.readyState;
        attempts++;
      }
      if (trackReadyState !== 'live') {
        throw new Error('Audio track is not live. State: ' + trackReadyState);
      }
    }

    if (this.inputAudioContext.state !== 'running') {
      await this.tryResumeAudioContext(this.inputAudioContext);
    }

    try {
      this.sourceNode = this.inputAudioContext.createMediaStreamSource(
        this.mediaStream,
      );
      this.sourceNode.connect(this.inputNode);
    } catch (e) {
      throw new Error(
        'Failed to create media stream source: ' + (e as Error).message,
      );
    }

    const bufferSize = 4096;
    this.scriptProcessorNode = this.inputAudioContext.createScriptProcessor(
      bufferSize,
      1,
      1,
    );

    this.scriptProcessorNode.onaudioprocess = (audioProcessingEvent) => {
      if (!this.isRecording) return;
      if (!this.isHolding) return;

      const inputBuffer = audioProcessingEvent.inputBuffer;
      const pcmData = inputBuffer.getChannelData(0);
      const srcRate = this.inputAudioContext.sampleRate;
      const downsampleTo16k = (float32: Float32Array, inRate: number) => {
        if (inRate === 16000) return float32;
        const ratio = inRate / 16000;
        const newLen = Math.floor(float32.length / ratio);
        const result = new Float32Array(newLen);
        let idx = 0, pos = 0;
        while (idx < newLen) {
          const nextPos = Math.min(float32.length - 1, (idx + 1) * ratio);
          let sum = 0, count = 0;
          for (let i = pos; i < nextPos; i++) { sum += float32[i]; count++; }
          result[idx++] = sum / Math.max(1, count);
          pos = nextPos;
        }
        return result;
      };
      const mono16k = srcRate === 16000 ? pcmData : downsampleTo16k(pcmData, srcRate);

      const sessionPromise = this.sessionPromise;
      if (!sessionPromise) {
        return;
      }

      sessionPromise
        .then((session) => {
          session.sendRealtimeInput({media: createBlob(mono16k)});
        })
        .catch((e) => this.updateError('Session send failed: ' + e.message));
    };

    this.sourceNode.connect(this.scriptProcessorNode);
    this.scriptProcessorNode.connect(this.inputAudioContext.destination);

    this.frameInterval = window.setInterval(() => {
      if (!this.isRecording) return;
      const context = this.canvasElement.getContext('2d');
      this.canvasElement.width = this.videoElement.videoWidth;
      this.canvasElement.height = this.videoElement.videoHeight;
      context.drawImage(
        this.videoElement,
        0,
        0,
        this.canvasElement.width,
        this.canvasElement.height,
      );
      this.canvasElement.toBlob(
        async (blob) => {
          if (blob) {
            const base64Data = await this.blobToBase64(blob);
            const sessionPromise = this.sessionPromise;
            if (!sessionPromise) {
              return;
            }

            sessionPromise
              .then((session) => {
                session.sendRealtimeInput({
                  media: {data: base64Data, mimeType: 'image/jpeg'},
                });
                this.framesSent++;
                if (this.framesSent % 10 === 0) {
                  this.debug(`Frames sent: ${this.framesSent}`);
                }
              })
              .catch((e) =>
                this.updateError('Session send (image) failed: ' + e.message),
              );
          }
        },
        'image/jpeg',
        0.8,
      );
    }, 500); // 2 frames per second

    this.isRecording = true;
    this.updateStatus('');
  }

  private async startRecording() {
    if (this.isRecording) {
      return;
    }

    if (!this.inputAudioContext || !this.outputAudioContext) {
      throw new Error('Audio contexts not initialized');
    }

    const inputResumePromise = this.tryResumeAudioContext(
      this.inputAudioContext,
    );
    const outputResumePromise = this.tryResumeAudioContext(
      this.outputAudioContext,
    );

    this.updateStatus('Requesting camera/mic access...');

    try {
      let stream: MediaStream | null = null;
      try {
        stream = await this.acquireMediaStream();
        const [inputRunning, outputRunning] = await Promise.all([
          inputResumePromise,
          outputResumePromise,
        ]);

        if (!inputRunning || !outputRunning) {
          this.updateStatus(
            'Audio is pending activation — touch the screen to continue.',
          );
        }
        await this.startRecordingWithStream(stream);
      } catch (error) {
        if (stream) {
          stream.getTracks().forEach((t) => t.stop());
        }
        throw error;
      }
    } catch (err) {
      console.error('Error starting recording:', err);
      const { name, message } = this.formatErrorDescriptor(err);
      this.updateError(`Camera/mic activation failed: ${name} - ${message}`);
      this.stopRecording();
    }
  }

  private stopRecording() {
    if (!this.isRecording && !this.mediaStream && !this.inputAudioContext)
      return;

    this.updateStatus('');

    this.isRecording = false;

    if (this.frameInterval) {
      clearInterval(this.frameInterval);
      this.frameInterval = null;
    }

    if (this.scriptProcessorNode && this.sourceNode && this.inputAudioContext) {
      this.scriptProcessorNode.disconnect();
      this.sourceNode.disconnect();
    }

    this.scriptProcessorNode = null;
    this.sourceNode = null;

    if (this.mediaStream) {
      this.mediaStream.getTracks().forEach((track) => track.stop());
      this.mediaStream = null;
    }

    if (this.videoElement) {
      this.videoElement.srcObject = null;
    }

    this.framesSent = 0;
    this.updateStatus('');
  }

  private stopApplication() {
    if (this.appStopped) {
      return;
    }

    this.stopRecording();

    for (const source of this.sources.values()) {
      try {
        source.stop();
      } catch {}
      this.sources.delete(source);
    }

    const sessionPromise = this.sessionPromise;
    this.sessionPromise = undefined;

    sessionPromise
      ?.then((session) => session.close())
      .catch((e) => this.updateError('Session close failed: ' + e.message));

    this.videoElement?.pause();
    if (this.videoElement) {
      this.videoElement.srcObject = null;
    }

    this.appStopped = true;
    this.updateStatus('Session ended');
  }

  private async restartApplication() {
    if (this.isRecording) {
      return;
    }

    this.updateStatus('Restarting session...');
    this.sessionPromise = this.initSession();
    this.appStopped = false;

    try {
      await this.startRecording();
    } catch (error) {
      this.updateError('Restart failed: ' + (error as Error).message);
    }
  }

  private reset() {
    this.sessionPromise?.then((session) => session.close());
    this.sessionPromise = this.initSession();
    this.updateStatus('');
  }

  private async handleStartSession() {
    if (this.sessionStarted) {
      return;
    }
    
    this.sessionStarted = true;
    this.updateStatus('Requesting camera/mic access...');
    
    let stream: MediaStream | null = null;
    
    try {
      // Create audio contexts within user gesture for iOS PWA compatibility
      // This MUST be done synchronously from the event handler
      if (!this.inputAudioContext) {
        this.inputAudioContext = new (window.AudioContext ||
          (window as any).webkitAudioContext)({sampleRate: 16000});
      }
      if (!this.outputAudioContext) {
        this.outputAudioContext = new (window.AudioContext ||
          (window as any).webkitAudioContext)({sampleRate: 24000});
      }
      
      // Create gain nodes if they don't exist
      if (!this.inputNode) {
        this.inputNode = this.inputAudioContext.createGain();
        this.inputNode.gain.value = 0.0; // Start with mic muted
      }
      if (!this.outputNode) {
        this.outputNode = this.outputAudioContext.createGain();
        this.outputNode.connect(this.outputAudioContext.destination);
      }
      
      // Initialize session if not already done
      if (!this.sessionPromise) {
        this.sessionPromise = this.initSession();
      }
      
      // Request permissions directly from user gesture handler
      // This MUST be called synchronously from the event handler on iOS
      const inputResumePromise = this.tryResumeAudioContext(
        this.inputAudioContext,
      );
      const outputResumePromise = this.tryResumeAudioContext(
        this.outputAudioContext,
      );
      
      // Call getUserMedia immediately to preserve user gesture context
      stream = await this.acquireMediaStream();
      
      const [inputRunning, outputRunning] = await Promise.all([
        inputResumePromise,
        outputResumePromise,
      ]);

      if (!inputRunning || !outputRunning) {
        this.updateStatus(
          'Audio is pending activation — touch the screen to continue.',
        );
      }
      
      // Small delay to ensure audio contexts are fully ready
      await new Promise(resolve => setTimeout(resolve, 100));
      
      // Initialize audio timing
      this.initAudio();
      
      // Now start recording with the stream
      await this.startRecordingWithStream(stream);
    } catch (err) {
      console.error('Error starting session:', err);
      const { name, message } = this.formatErrorDescriptor(err);
      this.updateError(`Failed to access camera/microphone: ${name} - ${message}`);
      this.updateStatus('');
      this.sessionStarted = false; // Allow retry on error
      
      // Clean up stream if it was created
      if (stream) {
        stream.getTracks().forEach((t) => t.stop());
      }
    }
  }

  private async handleHoldStart() {
    if (!this.isRecording) {
      if (this.appStopped) {
        await this.restartApplication();
      } else {
        try {
          await this.startRecording();
        } catch (err) {
          console.error('Error starting recording:', err);
          return;
        }
      }
    }
    
    this.isHolding = true;
    if (this.inputNode) {
      this.inputNode.gain.value = 1.0;
    }
  }

  private handleHoldEnd() {
    this.isHolding = false;
    if (this.inputNode) {
      this.inputNode.gain.value = 0.0;
    }
  }

  render() {
    return html`
      <div class="layout">
        <video
          id="video-background"
          autoplay
          muted
          playsinline
          ?hidden=${this.appStopped}></video>
        <canvas id="frame-canvas" style="display:none"></canvas>
        ${this.error
          ? html`<div class="error-banner" role="alert">${this.error}</div>`
          : null}
        ${!this.sessionStarted
          ? html`
              <button
                class="start-button"
                type="button"
                aria-label="Start Session"
                @click=${this.handleStartSession}
                @touchstart=${(e: TouchEvent) => {
                  e.preventDefault();
                  this.handleStartSession();
                }}>
                Start Session
              </button>
            `
          : null}
        <div class="speak-area">
          ${this.inputNode && this.outputNode
            ? html`<gdm-live-audio-visuals-3d
                .inputNode=${this.inputNode}
                .outputNode=${this.outputNode}></gdm-live-audio-visuals-3d>`
            : null}
          ${this.appStopped
            ? html`<div class="session-overlay">Session Ended</div>`
            : null}
          <button
            class="speak-button"
            type="button"
            aria-pressed=${this.isHolding}
            aria-label=${this.isHolding ? 'Release to mute' : 'Hold to speak'}
            @touchstart=${(e: TouchEvent) => {
              e.preventDefault();
              this.handleHoldStart();
            }}
            @touchend=${(e: TouchEvent) => {
              e.preventDefault();
              this.handleHoldEnd();
            }}
            @touchcancel=${(e: TouchEvent) => {
              e.preventDefault();
              this.handleHoldEnd();
            }}
            @mousedown=${(e: MouseEvent) => {
              e.preventDefault();
              this.handleHoldStart();
            }}
            @mouseup=${(e: MouseEvent) => {
              e.preventDefault();
              this.handleHoldEnd();
            }}
            @mouseleave=${(e: MouseEvent) => {
              e.preventDefault();
              this.handleHoldEnd();
            }}>
            <span class="speak-label">
              ${this.isHolding ? 'Release to Mute' : 'Hold to Speak'}
            </span>
          </button>
        </div>
      </div>
    `;
  }
}

```

```typescriptreact
/* tslint:disable */
/**
 * @license
 * SPDX-License-Identifier: Apache-2.0
 */

import {GoogleGenAI, LiveServerMessage, Modality, Session} from '@google/genai';
import {LitElement, css, html} from 'lit';
import {customElement, state} from 'lit/decorators.js';
import {createBlob, decode, decodeAudioData} from './utils';
import './visual-3d';
// PWA: register service worker if available
try {
  // dynamic import optional; vite-plugin-pwa injects virtual module
  // eslint-disable-next-line @typescript-eslint/ban-ts-GB
  // @ts-ignore
  import('virtual:pwa-register').then(({ registerSW }) => {
    registerSW({ immediate: true });
  }).catch(() => {});
} catch {}

@customElement('gdm-live-audio')
export class GdmLiveAudio extends LitElement {
  @state() isRecording = false;
  @state() isHolding = false;
  @state() status = '';
  @state() error = '';
  @state() appStopped = false;
  @state() sessionStarted = false;

  private client: GoogleGenAI;
  private sessionPromise?: Promise<Session>;
  // Audio contexts created on user gesture for iOS PWA compatibility
  private inputAudioContext?: AudioContext;
  private outputAudioContext?: AudioContext;
  @state() inputNode?: GainNode;
  @state() outputNode?: GainNode;
  private nextStartTime = 0;
  private mediaStream: MediaStream;
  private sourceNode: AudioNode;
  private scriptProcessorNode: ScriptProcessorNode;
  private sources = new Set<AudioBufferSourceNode>();
  private frameInterval: number;
  private videoElement: HTMLVideoElement;
  private canvasElement: HTMLCanvasElement;
  private framesSent = 0;

  static styles = css`
    :host {
      display: block;
      position: relative;
      width: 100vw;
      height: 100vh;
      min-height: 100dvh;
      background: #000000;
      color: #ffffff;
      overflow: hidden;
    }

    @supports (height: 100dvh) {
      :host {
        height: 100dvh;
      }
    }

    .layout {
      position: relative;
      width: 100%;
      height: 100%;
    }

    #video-background {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 66.667vh;
      object-fit: cover;
    }

    .speak-area {
      position: absolute;
      bottom: 0;
      left: 0;
      width: 100%;
      height: calc(33.333vh + env(safe-area-inset-bottom));
      border-top-left-radius: 32px;
      border-top-right-radius: 32px;
      border: 1px solid rgba(255, 255, 255, 0.08);
      border-bottom: none;
      overflow: hidden;
      background: rgba(0, 0, 0, 0.3);
      backdrop-filter: blur(6px);
    }

    .session-overlay {
      position: absolute;
      inset: 0;
      display: flex;
      align-items: center;
      justify-content: center;
      padding: 24px;
      font-size: 18px;
      letter-spacing: 0.1em;
      text-transform: uppercase;
      color: rgba(255, 255, 255, 0.92);
      background: linear-gradient(
        180deg,
        rgba(17, 24, 39, 0.6) 0%,
        rgba(15, 23, 42, 0.85) 100%
      );
      pointer-events: none;
      text-align: center;
    }

    gdm-live-audio-visuals-3d {
      display: block;
      width: 100%;
      height: 100%;
      pointer-events: none;
      border-top-left-radius: inherit;
      border-top-right-radius: inherit;
    }

    .speak-button {
      position: absolute;
      inset: 0;
      width: 100%;
      height: 100%;
      border: none;
      background: transparent;
      cursor: pointer;
      display: flex;
      align-items: flex-end;
      justify-content: center;
      padding: 24px 24px calc(24px + env(safe-area-inset-bottom));
      color: inherit;
      transition: background 0.2s ease;
      -webkit-touch-callout: none;
      -webkit-user-select: none;
      -moz-user-select: none;
      -ms-user-select: none;
      user-select: none;
      -webkit-tap-highlight-color: transparent;
      touch-action: manipulation;
    }

    .speak-button:hover,
    .speak-button:focus-visible {
      background: rgba(255, 255, 255, 0.04);
    }

    .speak-button:focus-visible {
      outline: 2px solid rgba(255, 255, 255, 0.25);
      outline-offset: 4px;
    }

    .speak-label {
      font-size: 18px;
      letter-spacing: 0.08em;
      text-transform: uppercase;
      color: rgba(255, 255, 255, 0.92);
      text-shadow: 0 0 6px rgba(0, 0, 0, 0.6);
      -webkit-touch-callout: none;
      -webkit-user-select: none;
      -moz-user-select: none;
      -ms-user-select: none;
      user-select: none;
      pointer-events: none;
    }

    .error-banner {
      position: absolute;
      top: calc(env(safe-area-inset-top) + 12px);
      left: 16px;
      right: 16px;
      z-index: 10;
      padding: 12px 16px;
      background: rgba(220, 38, 38, 0.85);
      color: #ffffff;
      border-radius: 12px;
      text-align: center;
      font-size: 14px;
      line-height: 1.4;
      box-shadow: 0 12px 24px rgba(0, 0, 0, 0.35);
      backdrop-filter: blur(12px);
    }

    .start-button {
      position: absolute;
      top: 50%;
      left: 50%;
      transform: translate(-50%, -50%);
      z-index: 1000;
      padding: 20px 40px;
      background: rgba(255, 255, 255, 0.25);
      color: #ffffff;
      border: 2px solid rgba(255, 255, 255, 0.5);
      border-radius: 16px;
      font-size: 18px;
      font-weight: 500;
      letter-spacing: 0.05em;
      text-transform: uppercase;
      cursor: pointer;
      transition: all 0.2s ease;
      backdrop-filter: blur(12px);
      box-shadow: 0 8px 24px rgba(0, 0, 0, 0.6), 0 0 0 1px rgba(255, 255, 255, 0.1);
      -webkit-tap-highlight-color: transparent;
      touch-action: manipulation;
      white-space: nowrap;
    }

    .start-button:hover,
    .start-button:focus-visible {
      background: rgba(255, 255, 255, 0.25);
      border-color: rgba(255, 255, 255, 0.5);
      transform: translate(-50%, -50%) scale(1.05);
    }

    .start-button:active {
      transform: translate(-50%, -50%) scale(0.98);
    }

    .start-button:focus-visible {
      outline: 2px solid rgba(255, 255, 255, 0.5);
      outline-offset: 4px;
    }
  `;

  constructor() {
    super();
    this.initClient();
  }

  protected firstUpdated() {
    this.videoElement = this.shadowRoot!.querySelector('#video-background');
    this.canvasElement = this.shadowRoot!.querySelector('#frame-canvas');
    this.debug(
      `Boot: secure=${window.isSecureContext} ua=${navigator.userAgent}`,
    );
    this.checkPermissions();
    this.addEventListener('click', async () => {
      try {
        if (this.outputAudioContext) {
          await this.outputAudioContext.resume();
        }
        if (this.inputAudioContext) {
          await this.inputAudioContext.resume();
        }
      } catch {}
    });
  }

  private async checkPermissions() {
    const parts: string[] = [];
    const anyNav: any = navigator as any;
    if (anyNav.permissions?.query) {
      try {
        const mic = await anyNav.permissions.query({name: 'microphone'});
        parts.push(`mic=${mic.state}`);
      } catch {}
      try {
        const cam = await anyNav.permissions.query({name: 'camera'});
        parts.push(`cam=${cam.state}`);
      } catch {}
    }
    if (parts.length) this.updateStatus(`Permissions: ${parts.join(' ')}`);
  }

  private debug(_msg: string, _data?: unknown) {
    // Debug logging disabled for production UI clarity.
  }

  private initAudio() {
    if (this.outputAudioContext && this.inputNode) {
      this.nextStartTime = this.outputAudioContext.currentTime;
      this.inputNode.gain.value = 0.0; // Start with mic muted
    }
  }

  private async initClient() {
    if (!process.env.API_KEY || process.env.API_KEY === 'undefined') {
      this.updateError('Missing GEMINI_API_KEY');
    }

    this.client = new GoogleGenAI({apiKey: process.env.API_KEY});

    // Audio contexts and nodes will be created on user gesture
    // Session will be initialized when user starts session
  }

  private initSession(): Promise<Session> {
    const model = 'gemini-2.5-flash-native-audio-preview-09-2025';

    try {
      return this.client.live.connect({
        model: model,
        callbacks: {
          onopen: () => {
            this.debug('Live session opened');
          },
          onmessage: async (message: LiveServerMessage) => {
            this.debug('Live message received');
            const audio =
              message.serverContent?.modelTurn?.parts[0]?.inlineData;

            if (audio && this.outputAudioContext && this.outputNode) {
              this.nextStartTime = Math.max(
                this.nextStartTime,
                this.outputAudioContext.currentTime,
              );

              const audioBuffer = await decodeAudioData(
                decode(audio.data),
                this.outputAudioContext,
                24000,
                1,
              );
              const source = this.outputAudioContext.createBufferSource();
              source.buffer = audioBuffer;
              source.connect(this.outputNode);
              source.addEventListener('ended', () => {
                this.sources.delete(source);
              });

              source.start(this.nextStartTime);
              this.nextStartTime = this.nextStartTime + audioBuffer.duration;
              this.sources.add(source);
            }

            const interrupted = message.serverContent?.interrupted;
            if (interrupted) {
              for (const source of this.sources.values()) {
                source.stop();
                this.sources.delete(source);
              }
              this.nextStartTime = 0;
            }
          },
          onerror: (e: ErrorEvent) => {
            this.updateError('Live error: ' + e.message);
            console.error('Live error event', e);
          },
          onclose: (e: CloseEvent) => {
            this.updateStatus('Live close: ' + e.reason + ' (' + e.code + ')');
            console.warn('Live close', e);
          },
        },
        config: {
          responseModalities: [Modality.AUDIO],
          systemInstruction: {
            parts: [
              {
                text: `You are an AI assistant integrated into an application where the main interface is a live camera view. The user interacts with you by pressing and holding a large button at the bottom of the screen. When the button is active, you listen to the user's voice input and provide an answer based on what is visible through the camera.

Your task is to analyze the live camera feed and interpret the user’s spoken request in combination. Always respond with precise, spatially grounded instructions that reference items or locations visible on the screen.

When giving instructions:

Be specific and concise.

Refer to locations using clear spatial language (e.g., “third shelf from the top,” “top right corner,” “behind the red box,” etc.).

Only describe what is visible on the camera or can be reasonably inferred from it.

Do not invent objects or locations that are not present in the scene.

Keep answers straightforward and helpful.

Example:
If the user asks, “Where is the gluten-free bread?” and it is visible through the camera, respond with a description such as:
“The gluten-free bread is on the third shelf from the top, in the top right corner.”

Your goal is to help the user locate objects in the environment accurately and efficiently based on real-time visual input.`,
              },
            ],
          },
          speechConfig: {
            voiceConfig: {prebuiltVoiceConfig: {voiceName: 'Zephyr'}},
            // languageCode: 'en-GB'
          },
        },
      });
    } catch (e) {
      console.error(e);
      return Promise.reject(e);
    }
  }

  private updateStatus(msg: string) {
    this.status = msg;
  }

  private updateError(msg: string) {
    this.error = msg;
  }

  private formatErrorDescriptor(error: unknown): { name: string; message: string } {
    if (error instanceof DOMException) {
      return { name: error.name, message: error.message };
    }
    if (error instanceof Error) {
      return { name: error.name || 'Error', message: error.message };
    }
    return { name: 'Error', message: String(error) };
  }

  private async tryResumeAudioContext(
    context: AudioContext,
  ): Promise<boolean> {
    if (!context || context.state === 'running') {
      return true;
    }
    try {
      await context.resume();
      return true;
    } catch (error) {
      const {name} = this.formatErrorDescriptor(error);
      if (name === 'InvalidStateError') {
        this.debug('AudioContext resume blocked (InvalidStateError)', error);
        return false;
      }
      throw error;
    }
  }

  private async acquireMediaStream(): Promise<MediaStream> {
    const constraints: MediaStreamConstraints = {
      audio: {
        channelCount: 1,
        echoCancellation: true,
        noiseSuppression: true,
        autoGainControl: true,
      },
      video: {
        facingMode: { ideal: 'environment' },
      },
    };

    const stream = await navigator.mediaDevices.getUserMedia(constraints);
    const primaryVideoTrack = stream.getVideoTracks()[0];
    if (!primaryVideoTrack) {
      return stream;
    }

    const settings = (primaryVideoTrack.getSettings?.() ??
      {}) as MediaTrackSettings;
    const facingMode = settings.facingMode;

    if (
      facingMode === 'environment' ||
      typeof navigator.mediaDevices.enumerateDevices !== 'function'
    ) {
      return stream;
    }

    try {
      const devices = await navigator.mediaDevices.enumerateDevices();
      const backCamera = devices.find(
        (device) =>
          device.kind === 'videoinput' &&
          /back|rear|environment/i.test(device.label),
      );

      if (!backCamera) {
        return stream;
      }

      const videoOnlyStream = await navigator.mediaDevices.getUserMedia({
        video: {deviceId: {exact: backCamera.deviceId}},
        audio: false,
      });
      const replacementTrack = videoOnlyStream.getVideoTracks()[0];
      if (!replacementTrack) {
        videoOnlyStream.getTracks().forEach((track) => track.stop());
        return stream;
      }

      stream.addTrack(replacementTrack);
      stream.removeTrack(primaryVideoTrack);
      primaryVideoTrack.stop();

      return stream;
    } catch (fallbackError) {
      this.debug('Back camera fallback failed', fallbackError);
      return stream;
    }
  }

  private blobToBase64(blob: Blob): Promise<string> {
    return new Promise((resolve, reject) => {
      const reader = new FileReader();
      reader.onloadend = () => {
        if (typeof reader.result === 'string') {
          resolve(reader.result.split(',')[1]);
        } else {
          reject(new Error('Failed to read blob as base64 string.'));
        }
      };
      reader.onerror = reject;
      reader.readAsDataURL(blob);
    });
  }

  private async startRecordingWithStream(stream: MediaStream) {
    if (this.isRecording) {
      stream.getTracks().forEach((t) => t.stop());
      return;
    }

    if (!this.inputAudioContext || !this.outputAudioContext || !this.inputNode || !this.outputNode) {
      throw new Error('Audio contexts not initialized');
    }

    if (!this.sessionPromise) {
      this.sessionPromise = this.initSession();
    }

    this.appStopped = false;
    this.framesSent = 0;

    this.mediaStream = stream;

    this.videoElement.srcObject = this.mediaStream;
    try {
      await this.videoElement.play();
    } catch (e) {
      console.warn('video.play() failed', e);
    }

    const vt = this.mediaStream.getVideoTracks()[0];
    const at = this.mediaStream.getAudioTracks()[0];
    this.updateStatus('Media access granted. v=' + (!!vt) + ' a=' + (!!at));

    // Ensure we have an audio track before proceeding
    if (!at) {
      throw new Error('No audio track available in media stream');
    }

    // Ensure audio track is enabled and ready
    if (!at.enabled) {
      at.enabled = true;
    }

    // Ensure audio track is ready - wait a bit if needed
    let trackReadyState = at.readyState;
    if (trackReadyState !== 'live') {
      // Wait for track to become live (max 1 second)
      let attempts = 0;
      while (trackReadyState !== 'live' && attempts < 10) {
        await new Promise(resolve => setTimeout(resolve, 100));
        trackReadyState = at.readyState;
        attempts++;
      }
      if (trackReadyState !== 'live') {
        throw new Error('Audio track is not live. State: ' + trackReadyState);
      }
    }

    await this.tryResumeAudioContext(this.inputAudioContext);
    await this.tryResumeAudioContext(this.outputAudioContext);

    try {
      this.sourceNode = this.inputAudioContext.createMediaStreamSource(
        this.mediaStream,
      );
      this.sourceNode.connect(this.inputNode);
    } catch (e) {
      throw new Error(
        'Failed to create media stream source: ' + (e as Error).message,
      );
    }

    const bufferSize = 4096;
    this.scriptProcessorNode = this.inputAudioContext.createScriptProcessor(
      bufferSize,
      1,
      1,
    );

    this.scriptProcessorNode.onaudioprocess = (audioProcessingEvent) => {
      if (!this.isRecording) return;
      if (!this.isHolding) return;

      const inputBuffer = audioProcessingEvent.inputBuffer;
      const pcmData = inputBuffer.getChannelData(0);
      const srcRate = this.inputAudioContext.sampleRate;
      const downsampleTo16k = (float32: Float32Array, inRate: number) => {
        if (inRate === 16000) return float32;
        const ratio = inRate / 16000;
        const newLen = Math.floor(float32.length / ratio);
        const result = new Float32Array(newLen);
        let idx = 0, pos = 0;
        while (idx < newLen) {
          const nextPos = Math.min(float32.length - 1, (idx + 1) * ratio);
          let sum = 0, count = 0;
          for (let i = pos; i < nextPos; i++) { sum += float32[i]; count++; }
          result[idx++] = sum / Math.max(1, count);
          pos = nextPos;
        }
        return result;
      };
      const mono16k = srcRate === 16000 ? pcmData : downsampleTo16k(pcmData, srcRate);

      const sessionPromise = this.sessionPromise;
      if (!sessionPromise) {
        return;
      }

      sessionPromise
        .then((session) => {
          session.sendRealtimeInput({media: createBlob(mono16k)});
        })
        .catch((e) => this.updateError('Session send failed: ' + e.message));
    };

    this.sourceNode.connect(this.scriptProcessorNode);
    this.scriptProcessorNode.connect(this.inputAudioContext.destination);

    this.frameInterval = window.setInterval(() => {
      if (!this.isRecording) return;
      const context = this.canvasElement.getContext('2d');
      this.canvasElement.width = this.videoElement.videoWidth;
      this.canvasElement.height = this.videoElement.videoHeight;
      context.drawImage(
        this.videoElement,
        0,
        0,
        this.canvasElement.width,
        this.canvasElement.height,
      );
      this.canvasElement.toBlob(
        async (blob) => {
          if (blob) {
            const base64Data = await this.blobToBase64(blob);
            const sessionPromise = this.sessionPromise;
            if (!sessionPromise) {
              return;
            }

            sessionPromise
              .then((session) => {
                session.sendRealtimeInput({
                  media: {data: base64Data, mimeType: 'image/jpeg'},
                });
                this.framesSent++;
                if (this.framesSent % 10 === 0) {
                  this.debug(`Frames sent: ${this.framesSent}`);
                }
              })
              .catch((e) =>
                this.updateError('Session send (image) failed: ' + e.message),
              );
          }
        },
        'image/jpeg',
        0.8,
      );
    }, 500); // 2 frames per second

    this.isRecording = true;
    this.updateStatus('');
  }

  private async startRecording() {
    if (this.isRecording) {
      return;
    }

    if (!this.inputAudioContext || !this.outputAudioContext) {
      throw new Error('Audio contexts not initialized');
    }

    const inputResumePromise = this.tryResumeAudioContext(
      this.inputAudioContext,
    );
    const outputResumePromise = this.tryResumeAudioContext(
      this.outputAudioContext,
    );

    this.updateStatus('Requesting camera/mic access...');

    try {
      let stream: MediaStream | null = null;
      try {
        stream = await this.acquireMediaStream();
        const [inputRunning, outputRunning] = await Promise.all([
          inputResumePromise,
          outputResumePromise,
        ]);

        if (!inputRunning || !outputRunning) {
          this.updateStatus(
            'Audio is pending activation — touch the screen to continue.',
          );
        }
        await this.startRecordingWithStream(stream);
      } catch (error) {
        if (stream) {
          stream.getTracks().forEach((t) => t.stop());
        }
        throw error;
      }
    } catch (err) {
      console.error('Error starting recording:', err);
      const { name, message } = this.formatErrorDescriptor(err);
      this.updateError(`Camera/mic activation failed: ${name} - ${message}`);
      this.stopRecording();
    }
  }

  private stopRecording() {
    if (!this.isRecording && !this.mediaStream && !this.inputAudioContext)
      return;

    this.updateStatus('');

    this.isRecording = false;

    if (this.frameInterval) {
      clearInterval(this.frameInterval);
      this.frameInterval = null;
    }

    if (this.scriptProcessorNode && this.sourceNode && this.inputAudioContext) {
      this.scriptProcessorNode.disconnect();
      this.sourceNode.disconnect();
    }

    this.scriptProcessorNode = null;
    this.sourceNode = null;

    if (this.mediaStream) {
      this.mediaStream.getTracks().forEach((track) => track.stop());
      this.mediaStream = null;
    }

    if (this.videoElement) {
      this.videoElement.srcObject = null;
    }

    this.framesSent = 0;
    this.updateStatus('');
  }

  private stopApplication() {
    if (this.appStopped) {
      return;
    }

    this.stopRecording();

    for (const source of this.sources.values()) {
      try {
        source.stop();
      } catch {}
      this.sources.delete(source);
    }

    const sessionPromise = this.sessionPromise;
    this.sessionPromise = undefined;

    sessionPromise
      ?.then((session) => session.close())
      .catch((e) => this.updateError('Session close failed: ' + e.message));

    this.videoElement?.pause();
    if (this.videoElement) {
      this.videoElement.srcObject = null;
    }

    this.appStopped = true;
    this.updateStatus('Session ended');
  }

  private async restartApplication() {
    if (this.isRecording) {
      return;
    }

    this.updateStatus('Restarting session...');
    this.sessionPromise = this.initSession();
    this.appStopped = false;

    try {
      await this.startRecording();
    } catch (error) {
      this.updateError('Restart failed: ' + (error as Error).message);
    }
  }

  private reset() {
    this.sessionPromise?.then((session) => session.close());
    this.sessionPromise = this.initSession();
    this.updateStatus('');
  }

  private async handleStartSession() {
    if (this.sessionStarted) {
      return;
    }
    
    this.sessionStarted = true;
    this.updateStatus('Requesting camera/mic access...');
    
    let stream: MediaStream | null = null;
    
    try {
      // Create audio contexts within user gesture for iOS PWA compatibility
      // This MUST be done synchronously from the event handler
      if (!this.inputAudioContext) {
        this.inputAudioContext = new (window.AudioContext ||
          (window as any).webkitAudioContext)({sampleRate: 16000});
      }
      if (!this.outputAudioContext) {
        this.outputAudioContext = new (window.AudioContext ||
          (window as any).webkitAudioContext)({sampleRate: 24000});
      }
      
      // Create gain nodes if they don't exist
      if (!this.inputNode) {
        this.inputNode = this.inputAudioContext.createGain();
        this.inputNode.gain.value = 0.0; // Start with mic muted
      }
      if (!this.outputNode) {
        this.outputNode = this.outputAudioContext.createGain();
        this.outputNode.connect(this.outputAudioContext.destination);
      }
      
      // Initialize session if not already done
      if (!this.sessionPromise) {
        this.sessionPromise = this.initSession();
      }
      
      // Request permissions directly from user gesture handler
      // This MUST be called synchronously from the event handler on iOS
      const inputResumePromise = this.tryResumeAudioContext(
        this.inputAudioContext,
      );
      const outputResumePromise = this.tryResumeAudioContext(
        this.outputAudioContext,
      );
      
      // Call getUserMedia immediately to preserve user gesture context
      stream = await this.acquireMediaStream();
      
      const [inputRunning, outputRunning] = await Promise.all([
        inputResumePromise,
        outputResumePromise,
      ]);

      if (!inputRunning || !outputRunning) {
        this.updateStatus(
          'Audio is pending activation — touch the screen to continue.',
        );
      }
      
      // Small delay to ensure audio contexts are fully ready
      await new Promise(resolve => setTimeout(resolve, 100));
      
      // Initialize audio timing
      this.initAudio();
      
      // Now start recording with the stream
      await this.startRecordingWithStream(stream);
    } catch (err) {
      console.error('Error starting session:', err);
      const { name, message } = this.formatErrorDescriptor(err);
      this.updateError(`Failed to access camera/microphone: ${name} - ${message}`);
      this.updateStatus('');
      this.sessionStarted = false; // Allow retry on error
      
      // Clean up stream if it was created
      if (stream) {
        stream.getTracks().forEach((t) => t.stop());
      }
    }
  }

  private async handleHoldStart() {
    if (!this.isRecording) {
      if (this.appStopped) {
        await this.restartApplication();
      } else {
        try {
          await this.startRecording();
        } catch (err) {
          console.error('Error starting recording:', err);
          return;
        }
      }
    }
    
    this.isHolding = true;
    if (this.inputNode) {
      this.inputNode.gain.value = 1.0;
    }
  }

  private handleHoldEnd() {
    this.isHolding = false;
    if (this.inputNode) {
      this.inputNode.gain.value = 0.0;
    }
  }

  render() {
    return html`
      <div class="layout">
        <video
          id="video-background"
          autoplay
          muted
          playsinline
          ?hidden=${this.appStopped}></video>
        <canvas id="frame-canvas" style="display:none"></canvas>
        ${this.error
          ? html`<div class="error-banner" role="alert">${this.error}</div>`
          : null}
        ${!this.sessionStarted
          ? html`
              <button
                class="start-button"
                type="button"
                aria-label="Start Session"
                @click=${this.handleStartSession}
                @touchstart=${(e: TouchEvent) => {
                  e.preventDefault();
                  this.handleStartSession();
                }}>
                Start Session
              </button>
            `
          : null}
        <div class="speak-area">
          ${this.inputNode && this.outputNode
            ? html`<gdm-live-audio-visuals-3d
                .inputNode=${this.inputNode}
                .outputNode=${this.outputNode}></gdm-live-audio-visuals-3d>`
            : null}
          ${this.appStopped
            ? html`<div class="session-overlay">Session Ended</div>`
            : null}
          <button
            class="speak-button"
            type="button"
            aria-pressed=${this.isHolding}
            aria-label=${this.isHolding ? 'Release to mute' : 'Hold to speak'}
            @touchstart=${(e: TouchEvent) => {
              e.preventDefault();
              this.handleHoldStart();
            }}
            @touchend=${(e: TouchEvent) => {
              e.preventDefault();
              this.handleHoldEnd();
            }}
            @touchcancel=${(e: TouchEvent) => {
              e.preventDefault();
              this.handleHoldEnd();
            }}
            @mousedown=${(e: MouseEvent) => {
              e.preventDefault();
              this.handleHoldStart();
            }}
            @mouseup=${(e: MouseEvent) => {
              e.preventDefault();
              this.handleHoldEnd();
            }}
            @mouseleave=${(e: MouseEvent) => {
              e.preventDefault();
              this.handleHoldEnd();
            }}>
            <span class="speak-label">
              ${this.isHolding ? 'Release to Mute' : 'Hold to Speak'}
            </span>
          </button>
        </div>
      </div>
    `;
  }
}

```

## Summary
- Added `tryResumeAudioContext` so `AudioContext.resume()` failures (especially `InvalidStateError`) now resolve gracefully and simply flag the UI to prompt the user for another tap instead of aborting startup.  
```398:425:index.tsx
  private async tryResumeAudioContext(
    context: AudioContext,
  ): Promise<boolean> {
    if (!context || context.state === 'running') {
      return true;
    }
    try {
      await context.resume();
      return true;
    } catch (error) {
      const {name} = this.formatErrorDescriptor(error);
      if (name === 'InvalidStateError') {
        this.debug('AudioContext resume blocked (InvalidStateError)', error);
        return false;
      }
      throw error;
    }
  }
```
- Updated both `startRecording` and `handleStartSession` to use the helper, surface a gentle “Audio is pending activation” status when contexts stay suspended, and continue into stream setup without throwing so the later tap resumes audio cleanly.  
```674:708:index.tsx
        const [inputRunning, outputRunning] = await Promise.all([
          inputResumePromise,
          outputResumePromise,
        ]);

        if (!inputRunning || !outputRunning) {
          this.updateStatus(
            'Audio is pending activation — touch the screen to continue.',
          );
        }
        await this.startRecordingWithStream(stream);
```
```837:865:index.tsx
      const [inputRunning, outputRunning] = await Promise.all([
        inputResumePromise,
        outputResumePromise,
      ]);

      if (!inputRunning || !outputRunning) {
        this.updateStatus(
          'Audio is pending activation — touch the screen to continue.',
        );
      }
      await this.startRecordingWithStream(stream);
```
- Ensured `startRecordingWithStream` retries `tryResumeAudioContext` for both input and output contexts before wiring up nodes, so delayed activations no longer raise a fatal error.  
```560:569:index.tsx
    await this.tryResumeAudioContext(this.inputAudioContext);
    await this.tryResumeAudioContext(this.outputAudioContext);

    try {
      this.sourceNode = this.inputAudioContext.createMediaStreamSource(
        this.mediaStream,
      );
      this.sourceNode.connect(this.inputNode);
```

## Testing
- Not run (device-only behavior).

## Next Steps
- Reinstall/open the PWA on iPhone, start a session, and when the banner reads “Audio is pending activation,” tap once more anywhere to unblock audio; confirm the stream now starts reliably.  
- If issues persist, attach Safari Web Inspector to capture console logs for any remaining errors (especially ones other than `InvalidStateError`).

```typescriptreact
  private async tryResumeAudioContext(
    context: AudioContext,
  ): Promise<boolean> {
    if (!context || context.state === 'running') {
      return true;
    }
    try {
      await context.resume();
      return true;
    } catch (error) {
      const {name} = this.formatErrorDescriptor(error);
      if (name === 'InvalidStateError') {
        this.debug('AudioContext resume blocked (InvalidStateError)', error);
        return false;
      }
      throw error;
    }
  }
```

```typescriptreact
        const [inputRunning, outputRunning] = await Promise.all([
          inputResumePromise,
          outputResumePromise,
        ]);

        if (!inputRunning || !outputRunning) {
          this.updateStatus(
            'Audio is pending activation — touch the screen to continue.',
          );
        }
        await this.startRecordingWithStream(stream);
```

```typescriptreact
      const [inputRunning, outputRunning] = await Promise.all([
        inputResumePromise,
        outputResumePromise,
      ]);

      if (!inputRunning || !outputRunning) {
        this.updateStatus(
          'Audio is pending activation — touch the screen to continue.',
        );
      }
      await this.startRecordingWithStream(stream);
```

```typescriptreact
    await this.tryResumeAudioContext(this.inputAudioContext);
    await this.tryResumeAudioContext(this.outputAudioContext);

    try {
      this.sourceNode = this.inputAudioContext.createMediaStreamSource(
        this.mediaStream,
      );
      this.sourceNode.connect(this.inputNode);
```

