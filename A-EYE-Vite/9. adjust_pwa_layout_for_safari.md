# Adjust PWA layout for Safari
_Exported on 15/11/2025 at 3:00:24 CET from Cursor (1.7.44)_

---

**User**

I am building a PWA. When I open this app in safari, the menu at the bottom covers a black bar that is present. When I open the PWA from the home screen, the black bar is present because the menu bar used to be there. 

Make it so that the content of the app is pushed all the way down when the PWA is active. Look at the images for reference.

Read @DOCUMENTATION.md for more context about the app.

---

**Cursor**

I'll scan the current code to find where viewport height and safe-area insets are handled, especially in `index.tsx` and styles. Then I'll update the layout to respect iOS safe area insets and detect standalone PWA mode to remove the extra bottom space.

I'll update the component's CSS so the bottom area uses dynamic viewport units and doesn't add extra space in standalone PWA mode. Specifically:
- Remove the extra env(safe-area-inset-bottom) from `.speak-area` height.
- Use dvh for `#video-background` and `.speak-area` when supported to ensure 66.667dvh + 33.333dvh = 100dvh.
- Keep the padding with env(safe-area-inset-bottom) for the button to respect the home indicator.

Now applying the edit.

```typescriptreact
/* tslint:disable */
/**
 * @license
 * SPDX-License-Identifier: Apache-2.0
 */

import {GoogleGenAI, LiveServerMessage, Modality, Session} from '@google/genai';
import {LitElement, css, html} from 'lit';
import {customElement, state} from 'lit/decorators.js';
import {createBlob, decode, decodeAudioData} from './utils';
import './visual-3d';
// PWA: register service worker if available
try {
  // dynamic import optional; vite-plugin-pwa injects virtual module
  // eslint-disable-next-line @typescript-eslint/ban-ts-comment
  // @ts-ignore
  import('virtual:pwa-register').then(({ registerSW }) => {
    registerSW({ immediate: true });
  }).catch(() => {});
} catch {}

@customElement('gdm-live-audio')
export class GdmLiveAudio extends LitElement {
  @state() isRecording = false;
  @state() isHolding = false;
  @state() status = '';
  @state() error = '';
  @state() appStopped = false;
  @state() showPermissionGate = false;

  private client: GoogleGenAI;
  private sessionPromise?: Promise<Session>;
  // FIX: Cast window to `any` to access prefixed `webkitAudioContext` for older browsers.
  private inputAudioContext = new (window.AudioContext ||
    (window as any).webkitAudioContext)({sampleRate: 16000});
  // FIX: Cast window to `any` to access prefixed `webkitAudioContext` for older browsers.
  private outputAudioContext = new (window.AudioContext ||
    (window as any).webkitAudioContext)({sampleRate: 24000});
  @state() inputNode = this.inputAudioContext.createGain();
  @state() outputNode = this.outputAudioContext.createGain();
  private nextStartTime = 0;
  private mediaStream: MediaStream;
  private sourceNode: AudioNode;
  private scriptProcessorNode: ScriptProcessorNode;
  private sources = new Set<AudioBufferSourceNode>();
  private frameInterval: number;
  private videoElement: HTMLVideoElement;
  private canvasElement: HTMLCanvasElement;
  private framesSent = 0;
  private isStandalonePWA = false;

  static styles = css`
    :host {
      display: block;
      position: relative;
      width: 100vw;
      height: 100vh;
      min-height: 100dvh;
      background: #000000;
      color: #ffffff;
      overflow: hidden;
    }

    @supports (height: 100dvh) {
      :host {
        height: 100dvh;
      }
      #video-background {
        height: 66.667dvh;
      }
      .speak-area {
        height: 33.333dvh;
      }
    }

    .layout {
      position: relative;
      width: 100%;
      height: 100%;
    }

    #video-background {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 66.667vh;
      object-fit: cover;
    }

    .speak-area {
      position: absolute;
      bottom: 0;
      left: 0;
      width: 100%;
      height: 33.333vh;
      border-top-left-radius: 32px;
      border-top-right-radius: 32px;
      border: 1px solid rgba(255, 255, 255, 0.08);
      border-bottom: none;
      overflow: hidden;
      backdrop-filter: blur(6px);
    }

    .session-overlay {
      position: absolute;
      inset: 0;
      display: flex;
      align-items: center;
      justify-content: center;
      padding: 24px;
      font-size: 18px;
      letter-spacing: 0.1em;
      text-transform: uppercase;
      color: rgba(255, 255, 255, 0.92);
      background: linear-gradient(
        180deg,
        rgba(17, 24, 39, 0.6) 0%,
        rgba(15, 23, 42, 0.85) 100%
      );
      pointer-events: none;
      text-align: center;
    }

    .permission-gate {
      position: absolute;
      inset: 0;
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: center;
      padding: 24px;
      gap: 16px;
      background: rgba(0, 0, 0, 0.6);
      z-index: 20;
      text-align: center;
      pointer-events: auto;
    }

    .permission-button {
      appearance: none;
      border: 1px solid rgba(255, 255, 255, 0.14);
      background: rgba(255, 255, 255, 0.08);
      color: #ffffff;
      padding: 14px 18px;
      border-radius: 14px;
      font-size: 16px;
      letter-spacing: 0.06em;
      text-transform: uppercase;
      cursor: pointer;
      -webkit-tap-highlight-color: transparent;
      touch-action: manipulation;
    }

    .permission-note {
      font-size: 12px;
      color: rgba(255, 255, 255, 0.7);
      line-height: 1.4;
    }

    gdm-live-audio-visuals-3d {
      display: block;
      width: 100%;
      height: 100%;
      pointer-events: none;
      border-top-left-radius: inherit;
      border-top-right-radius: inherit;
    }

    .speak-button {
      position: absolute;
      inset: 0;
      width: 100%;
      height: 100%;
      border: none;
      background: transparent;
      cursor: pointer;
      display: flex;
      align-items: flex-end;
      justify-content: center;
      padding: 24px 24px calc(24px + env(safe-area-inset-bottom));
      color: inherit;
      transition: background 0.2s ease;
      -webkit-touch-callout: none;
      -webkit-user-select: none;
      -moz-user-select: none;
      -ms-user-select: none;
      user-select: none;
      -webkit-tap-highlight-color: transparent;
      touch-action: manipulation;
    }

    .speak-button:hover,
    .speak-button:focus-visible {
      background: rgba(255, 255, 255, 0.04);
    }

    .speak-button:focus-visible {
      outline: 2px solid rgba(255, 255, 255, 0.25);
      outline-offset: 4px;
    }

    .speak-label {
      font-size: 18px;
      letter-spacing: 0.08em;
      text-transform: uppercase;
      color: rgba(255, 255, 255, 0.92);
      text-shadow: 0 0 6px rgba(0, 0, 0, 0.6);
      -webkit-touch-callout: none;
      -webkit-user-select: none;
      -moz-user-select: none;
      -ms-user-select: none;
      user-select: none;
      pointer-events: none;
    }

    .error-banner {
      position: absolute;
      top: calc(env(safe-area-inset-top) + 12px);
      left: 16px;
      right: 16px;
      z-index: 10;
      padding: 12px 16px;
      background: rgba(220, 38, 38, 0.85);
      color: #ffffff;
      border-radius: 12px;
      text-align: center;
      font-size: 14px;
      line-height: 1.4;
      box-shadow: 0 12px 24px rgba(0, 0, 0, 0.35);
      backdrop-filter: blur(12px);
    }
  `;

  constructor() {
    super();
    this.initClient();
  }

  protected firstUpdated() {
    this.videoElement = this.shadowRoot!.querySelector('#video-background');
    this.canvasElement = this.shadowRoot!.querySelector('#frame-canvas');
    this.debug(
      `Boot: secure=${window.isSecureContext} ua=${navigator.userAgent}`,
    );
    // Detect standalone (PWA) mode and show explicit permission gate for iOS
    const isStandaloneMatch =
      (window.matchMedia && window.matchMedia('(display-mode: standalone)').matches) || false;
    const isStandaloneLegacy = (navigator as any).standalone === true;
    this.isStandalonePWA = !!(isStandaloneMatch || isStandaloneLegacy);
    try {
      const permOk = localStorage.getItem('mediaPermOk') === '1';
      if (this.isStandalonePWA && !permOk) {
        this.showPermissionGate = true;
      }
    } catch {}
    this.checkPermissions();
    this.addEventListener('click', async () => {
      try {
        await this.outputAudioContext.resume();
        await this.inputAudioContext.resume();
      } catch {}
    });
  }

  private async bootstrapPermissions() {
    this.updateStatus('Requesting access...');
    try {
      await this.outputAudioContext.resume();
      await this.inputAudioContext.resume();
      await this.startRecording();
      this.showPermissionGate = false;
      try {
        localStorage.setItem('mediaPermOk', '1');
      } catch {}
      this.updateStatus('');
    } catch (err) {
      console.error('Permission bootstrap failed:', err);
      this.updateError('Permission request failed: ' + (err as Error).message);
    }
  }

  private async checkPermissions() {
    const parts: string[] = [];
    const anyNav: any = navigator as any;
    if (anyNav.permissions?.query) {
      try {
        const mic = await anyNav.permissions.query({name: 'microphone'});
        parts.push(`mic=${mic.state}`);
      } catch {}
      try {
        const cam = await anyNav.permissions.query({name: 'camera'});
        parts.push(`cam=${cam.state}`);
      } catch {}
    }
    if (parts.length) this.updateStatus(`Permissions: ${parts.join(' ')}`);
  }

  private debug(_msg: string, _data?: unknown) {
    // Debug logging disabled for production UI clarity.
  }

  private initAudio() {
    this.nextStartTime = this.outputAudioContext.currentTime;
    this.inputNode.gain.value = 0.0; // Start with mic muted
  }

  private async initClient() {
    this.initAudio();

    if (!process.env.API_KEY || process.env.API_KEY === 'undefined') {
      this.updateError('Missing GEMINI_API_KEY');
    }

    this.client = new GoogleGenAI({apiKey: process.env.API_KEY});

    this.outputNode.connect(this.outputAudioContext.destination);

    this.sessionPromise = this.initSession();
  }

  private initSession(): Promise<Session> {
    const model = 'gemini-2.5-flash-native-audio-preview-09-2025';

    try {
      return this.client.live.connect({
        model: model,
        callbacks: {
          onopen: () => {
            this.debug('Live session opened');
          },
          onmessage: async (message: LiveServerMessage) => {
            this.debug('Live message received');
            const audio =
              message.serverContent?.modelTurn?.parts[0]?.inlineData;

            if (audio) {
              this.nextStartTime = Math.max(
                this.nextStartTime,
                this.outputAudioContext.currentTime,
              );

              const audioBuffer = await decodeAudioData(
                decode(audio.data),
                this.outputAudioContext,
                24000,
                1,
              );
              const source = this.outputAudioContext.createBufferSource();
              source.buffer = audioBuffer;
              source.connect(this.outputNode);
              source.addEventListener('ended', () => {
                this.sources.delete(source);
              });

              source.start(this.nextStartTime);
              this.nextStartTime = this.nextStartTime + audioBuffer.duration;
              this.sources.add(source);
            }

            const interrupted = message.serverContent?.interrupted;
            if (interrupted) {
              for (const source of this.sources.values()) {
                source.stop();
                this.sources.delete(source);
              }
              this.nextStartTime = 0;
            }
          },
          onerror: (e: ErrorEvent) => {
            this.updateError('Live error: ' + e.message);
            console.error('Live error event', e);
          },
          onclose: (e: CloseEvent) => {
            this.updateStatus('Live close: ' + e.reason + ' (' + e.code + ')');
            console.warn('Live close', e);
          },
        },
        config: {
          responseModalities: [Modality.AUDIO],
          systemInstruction: {
            parts: [
              {
                text: `You are an AI assistant integrated into an application where the main interface is a live camera view. The user interacts with you by pressing and holding a large button at the bottom of the screen. When the button is active, you listen to the user's voice input and provide an answer based on what is visible through the camera.

Your task is to analyze the live camera feed and interpret the user’s spoken request in combination. Always respond with precise, spatially grounded instructions that reference items or locations visible on the screen.

When giving instructions:

Be specific and concise.

Refer to locations using clear spatial language (e.g., “third shelf from the top,” “top right corner,” “behind the red box,” etc.).

Only describe what is visible on the camera or can be reasonably inferred from it.

Do not invent objects or locations that are not present in the scene.

Keep answers straightforward and helpful.

Example:
If the user asks, “Where is the gluten-free bread?” and it is visible through the camera, respond with a description such as:
“The gluten-free bread is on the third shelf from the top, in the top right corner.”

Your goal is to help the user locate objects in the environment accurately and efficiently based on real-time visual input.`,
              },
            ],
          },
          speechConfig: {
            voiceConfig: {prebuiltVoiceConfig: {voiceName: 'Zephyr'}},
            // languageCode: 'en-GB'
          },
        },
      });
    } catch (e) {
      console.error(e);
      return Promise.reject(e);
    }
  }

  private updateStatus(msg: string) {
    this.status = msg;
  }

  private updateError(msg: string) {
    this.error = msg;
  }

  private blobToBase64(blob: Blob): Promise<string> {
    return new Promise((resolve, reject) => {
      const reader = new FileReader();
      reader.onloadend = () => {
        if (typeof reader.result === 'string') {
          resolve(reader.result.split(',')[1]);
        } else {
          reject(new Error('Failed to read blob as base64 string.'));
        }
      };
      reader.onerror = reject;
      reader.readAsDataURL(blob);
    });
  }

  private async startRecording() {
    if (this.isRecording) {
      return;
    }

    if (!this.sessionPromise) {
      this.sessionPromise = this.initSession();
    }

    this.appStopped = false;
    this.framesSent = 0;

    await this.inputAudioContext.resume();
    await this.outputAudioContext.resume();

    this.updateStatus('Requesting camera/mic access...');

    try {
      const preferBackCamera = async (): Promise<MediaStream> => {
        const constraints: MediaStreamConstraints = {
          audio: {
            channelCount: 1,
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true,
          },
          video: {
            facingMode: { ideal: 'environment' },
          },
        };
        this.debug('getUserMedia with constraints', constraints);
        try {
          const stream = await navigator.mediaDevices.getUserMedia(constraints);
          const track = stream.getVideoTracks()[0];
          const settings = track?.getSettings?.() || {} as MediaTrackSettings;
          this.debug('Acquired stream video settings', settings);
          if (settings.facingMode !== 'environment') {
            // Try enumerate fallback to enforce back camera
            const devices = await navigator.mediaDevices.enumerateDevices();
            const back = devices.find(
              (d) => d.kind === 'videoinput' && /back|rear|environment/i.test(d.label),
            );
            if (back) {
              try {
                const enforced = await navigator.mediaDevices.getUserMedia({
                  audio: constraints.audio,
                  video: { deviceId: { exact: back.deviceId } },
                });
                this.debug('Enforced back camera via deviceId', back);
                stream.getTracks().forEach((t) => t.stop());
                return enforced;
              } catch {
                // fall through to original stream
              }
            }
          }
          return stream;
        } catch (e) {
          this.updateError('getUserMedia failed: ' + (e as Error).message);
          const devices = await navigator.mediaDevices.enumerateDevices();
          const back = devices.find(
            (d) => d.kind === 'videoinput' && /back|rear|environment/i.test(d.label),
          );
          if (back) {
            return await navigator.mediaDevices.getUserMedia({
              audio: constraints.audio,
              video: { deviceId: { exact: back.deviceId } },
            });
          }
          throw e;
        }
      };

      this.mediaStream = await preferBackCamera();

      this.videoElement.srcObject = this.mediaStream;
      try {
        await this.videoElement.play();
      } catch (e) {
        console.warn('video.play() failed', e);
      }

      const vt = this.mediaStream.getVideoTracks()[0];
      const at = this.mediaStream.getAudioTracks()[0];
      this.updateStatus('Media access granted. v=' + (!!vt) + ' a=' + (!!at));

      this.sourceNode = this.inputAudioContext.createMediaStreamSource(
        this.mediaStream,
      );
      this.sourceNode.connect(this.inputNode);

      const bufferSize = 4096;
      this.scriptProcessorNode = this.inputAudioContext.createScriptProcessor(
        bufferSize,
        1,
        1,
      );

      this.scriptProcessorNode.onaudioprocess = (audioProcessingEvent) => {
        if (!this.isRecording) return;
        if (!this.isHolding) return;

        const inputBuffer = audioProcessingEvent.inputBuffer;
        const pcmData = inputBuffer.getChannelData(0);
        const srcRate = this.inputAudioContext.sampleRate;
        const downsampleTo16k = (float32: Float32Array, inRate: number) => {
          if (inRate === 16000) return float32;
          const ratio = inRate / 16000;
          const newLen = Math.floor(float32.length / ratio);
          const result = new Float32Array(newLen);
          let idx = 0, pos = 0;
          while (idx < newLen) {
            const nextPos = Math.min(float32.length - 1, (idx + 1) * ratio);
            let sum = 0, count = 0;
            for (let i = pos; i < nextPos; i++) { sum += float32[i]; count++; }
            result[idx++] = sum / Math.max(1, count);
            pos = nextPos;
          }
          return result;
        };
        const mono16k = srcRate === 16000 ? pcmData : downsampleTo16k(pcmData, srcRate);

        const sessionPromise = this.sessionPromise;
        if (!sessionPromise) {
          return;
        }

        sessionPromise
          .then((session) => {
            session.sendRealtimeInput({media: createBlob(mono16k)});
          })
          .catch((e) => this.updateError('Session send failed: ' + e.message));
      };

      this.sourceNode.connect(this.scriptProcessorNode);
      this.scriptProcessorNode.connect(this.inputAudioContext.destination);

      this.frameInterval = window.setInterval(() => {
        if (!this.isRecording) return;
        const context = this.canvasElement.getContext('2d');
        this.canvasElement.width = this.videoElement.videoWidth;
        this.canvasElement.height = this.videoElement.videoHeight;
        context.drawImage(
          this.videoElement,
          0,
          0,
          this.canvasElement.width,
          this.canvasElement.height,
        );
        this.canvasElement.toBlob(
          async (blob) => {
            if (blob) {
              const base64Data = await this.blobToBase64(blob);
              const sessionPromise = this.sessionPromise;
              if (!sessionPromise) {
                return;
              }

              sessionPromise
                .then((session) => {
                  session.sendRealtimeInput({
                    media: {data: base64Data, mimeType: 'image/jpeg'},
                  });
                  this.framesSent++;
                  if (this.framesSent % 10 === 0) {
                    this.debug(`Frames sent: ${this.framesSent}`);
                  }
                })
                .catch((e) =>
                  this.updateError('Session send (image) failed: ' + e.message),
                );
            }
          },
          'image/jpeg',
          0.8,
        );
      }, 500); // 2 frames per second

      this.isRecording = true;
      this.updateStatus('');
    } catch (err) {
      console.error('Error starting recording:', err);
      this.updateStatus(`Error: ${err.message}`);
      this.stopRecording();
    }
  }

  private stopRecording() {
    if (!this.isRecording && !this.mediaStream && !this.inputAudioContext)
      return;

    this.updateStatus('');

    this.isRecording = false;

    if (this.frameInterval) {
      clearInterval(this.frameInterval);
      this.frameInterval = null;
    }

    if (this.scriptProcessorNode && this.sourceNode && this.inputAudioContext) {
      this.scriptProcessorNode.disconnect();
      this.sourceNode.disconnect();
    }

    this.scriptProcessorNode = null;
    this.sourceNode = null;

    if (this.mediaStream) {
      this.mediaStream.getTracks().forEach((track) => track.stop());
      this.mediaStream = null;
    }

    if (this.videoElement) {
      this.videoElement.srcObject = null;
    }

    this.framesSent = 0;
    this.updateStatus('');
  }

  private stopApplication() {
    if (this.appStopped) {
      return;
    }

    this.stopRecording();

    for (const source of this.sources.values()) {
      try {
        source.stop();
      } catch {}
      this.sources.delete(source);
    }

    const sessionPromise = this.sessionPromise;
    this.sessionPromise = undefined;

    sessionPromise
      ?.then((session) => session.close())
      .catch((e) => this.updateError('Session close failed: ' + e.message));

    this.videoElement?.pause();
    if (this.videoElement) {
      this.videoElement.srcObject = null;
    }

    this.appStopped = true;
    this.updateStatus('Session ended');
  }

  private async restartApplication() {
    if (this.isRecording) {
      return;
    }

    this.updateStatus('Restarting session...');
    this.sessionPromise = this.initSession();
    this.appStopped = false;

    try {
      await this.startRecording();
    } catch (error) {
      this.updateError('Restart failed: ' + (error as Error).message);
    }
  }

  private reset() {
    this.sessionPromise?.then((session) => session.close());
    this.sessionPromise = this.initSession();
    this.updateStatus('');
  }

  private async handleHoldStart() {
    if (!this.isRecording) {
      if (this.appStopped) {
        await this.restartApplication();
      } else {
        try {
          await this.startRecording();
        } catch (err) {
          console.error('Error starting recording:', err);
          return;
        }
      }
    }
    
    this.isHolding = true;
    this.inputNode.gain.value = 1.0;
  }

  private handleHoldEnd() {
    this.isHolding = false;
    this.inputNode.gain.value = 0.0;
  }

  render() {
    return html`
      <div class="layout">
        <video
          id="video-background"
          autoplay
          muted
          playsinline
          ?hidden=${this.appStopped}></video>
        <canvas id="frame-canvas" style="display:none"></canvas>
        ${this.error
          ? html`<div class="error-banner" role="alert">${this.error}</div>`
          : null}
        <div class="speak-area">
          <gdm-live-audio-visuals-3d
            .inputNode=${this.inputNode}
            .outputNode=${this.outputNode}></gdm-live-audio-visuals-3d>
          ${this.showPermissionGate
            ? html`
                <div class="permission-gate" role="dialog" aria-modal="true">
                  <button
                    class="permission-button"
                    type="button"
                    @click=${() => this.bootstrapPermissions()}>
                    Enable Camera & Microphone
                  </button>
                  <div class="permission-note">
                    Tap to grant access. Required when opened from the Home Screen.
                  </div>
                </div>
              `
            : null}
          ${this.appStopped
            ? html`<div class="session-overlay">Session Ended</div>`
            : null}
          <button
            class="speak-button"
            type="button"
            aria-pressed=${this.isHolding}
            aria-label=${this.isHolding ? 'Release to mute' : 'Hold to speak'}
            @pointerdown=${(e: PointerEvent) => {
              e.preventDefault();
              this.handleHoldStart();
            }}
            @pointerup=${(e: PointerEvent) => {
              e.preventDefault();
              this.handleHoldEnd();
            }}
            @pointercancel=${(e: PointerEvent) => {
              e.preventDefault();
              this.handleHoldEnd();
            }}
            @click=${async (e: MouseEvent) => {
              if (!this.isRecording && !this.showPermissionGate) {
                try {
                  await this.startRecording();
                } catch {}
              }
            }}
            @touchstart=${(e: TouchEvent) => {
              e.preventDefault();
              this.handleHoldStart();
            }}
            @touchend=${(e: TouchEvent) => {
              e.preventDefault();
              this.handleHoldEnd();
            }}
            @touchcancel=${(e: TouchEvent) => {
              e.preventDefault();
              this.handleHoldEnd();
            }}
            @mousedown=${(e: MouseEvent) => {
              e.preventDefault();
              this.handleHoldStart();
            }}
            @mouseup=${(e: MouseEvent) => {
              e.preventDefault();
              this.handleHoldEnd();
            }}
            @mouseleave=${(e: MouseEvent) => {
              e.preventDefault();
              this.handleHoldEnd();
            }}>
            <span class="speak-label">
              ${this.isHolding ? 'Release to Mute' : 'Hold to Speak'}
            </span>
          </button>
        </div>
      </div>
    `;
  }
}

```

I updated the CSS to use dvh units and removed the extra bottom height so the PWA fills to the bottom while still respecting the home indicator via padding. Let me check for any linter issues in the modified file.

- I adjusted the layout so the app fills the screen and pushes all content all the way down when running as a PWA (no leftover “tab bar” gap).
- Change: the bottom panel no longer adds extra height for the safe area; instead, the button keeps its own safe-area padding. Heights now use dynamic viewport units so 66.667dvh + 33.333dvh = 100dvh.

What I changed
- In `index.tsx`:
  - `.speak-area` height: from `calc(33.333vh + env(safe-area-inset-bottom))` to `33.333vh`, with a dvh override when supported.
  - `#video-background` height: added dvh override to `66.667dvh`.
  - Kept `.speak-button` bottom padding as `calc(24px + env(safe-area-inset-bottom))` so text stays above the home indicator.

How this fixes it
- In standalone PWA, dvh excludes browser UI, so there’s no phantom “Safari tab bar” space. The safe-area padding only affects the button content, not the panel height.

Quick test
- Open in Safari: should look unchanged.
- Add to Home Screen and open: the black bar is gone; the bottom panel sits flush with the edge, while the label sits above the home indicator.

If you still see a gap on very old iOS versions, tell me the version and I’ll add a targeted fallback.

